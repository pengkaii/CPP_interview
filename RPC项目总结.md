# RPC

#### 网址汇总

[(253条消息) 史上最全的Zookeeper原理详解(万字长文)_雷恩Layne的博客-CSDN博客](https://blog.csdn.net/qq_37555071/article/details/114609145)

[ZooKeeper相关概念总结(入门) | JavaGuide(Java面试 + 学习指南)](https://javaguide.cn/distributed-system/distributed-process-coordination/zookeeper/zookeeper-intro.html)

[ZooKeeper的十二连问，你顶得了嘛？ - Jay_huaxiao - 博客园 (cnblogs.com)](https://www.cnblogs.com/jay-huaxiao/p/13599519.html)

[关于RPC的一些问题的八股文总结_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/353159058410643456)

[消息队列和Zookeeper面试题阅读指南-帅地玩编程 (iamshuaidi.com)](https://www.iamshuaidi.com/1633.html)

[腾讯云智二面_牛客网 (nowcoder.com)](https://www.nowcoder.com/discuss/476840885213323264?sourceSSR=users)

[帅地玩编程-校招|面试|学习路线，你都可以在这里找到 (iamshuaidi.com)](https://www.iamshuaidi.com/)

#### 什么是protobuf？

​		protobuf是由google开发的一种语言无关的数据序列化与反序列化格式。

#### 怎么实现一个序列化与反序列化？

序列化就是将对象的状态信息转换为可以存储或传输的形式的一个过程。

反序列化就是将已经存储在磁盘上的的字节序列恢复成对象的过程

​		利用二进制存储；

- 对于**整数**编码：

  规定0~2^8-1使用1表示（用一个字节表示，读取后续的一个字节）；

  在2^8~2^16-1使用2表示；

  在2^16~2^24-1使用3表示；

  在2^24~2^32使用4表示。（反序列化时就使用后面几个字节表示）

- 对于**字符串**编码：

  使用16表示，后续紧跟着使用一个字节表示字符串占有的字节数（如8，表示占8个字节），再读取相应的字节数。

- 对于**数组**编码：使用24表示，后续一个字节表示数组长度，数组中每个对象优势上面的整数编码。



#### protobuf的编码解码规则

**varint是一种可变长编码**，使用1个或多个字节对整数进行编码，可编码任意大的整数，小整数占用的字节少，大整数占用的字节多，如果小整数更频繁出现，则通过varint可实现压缩存储。编码的理念是：**越小的数字花费越少的字节**。

**varint中每个字节的最高位bit称之为most significant bit(MSB)**，每个字节的最高位表示后面还有没有字节，若为0表示后面没有字节，若为1表示后面有字节。而每个字节的低7位就是实际的值，并且使用小端的表示方法。

**例如1**，varint的表示方法就为: 0000 0001，而int本身是4字节的，采用varint的编码方式就省了三个字节。

再例如300，varint表示为:

100101100                          // 300的二进制
10101100 00000010            // 300的varint编码
，而int本身是4字节的，采用varint的编码方式就省了两个字节。



**编码：**数字300，编码后用1010 1100 0000 0010表示，总共两个字节，我们怎么计算出300

**解码规则：**

首先最高位为1，代表后面有更多字节，第二个字节最高位为0，说明这就两个字节了。 1010 1100 0000 0010 → 010 1100 000 0010

1、首先我们丢掉第一个字节的最高位得到： 010 1100

2、丢掉第二个字节的最高位得到： 000 0010

3、**最低有效组在前面**，倒过来组合，得到0000010 0101100（即十进制的300）



protobuf对于正数的编码采用varint，对于负数的编码采用ZigZag编码后的varint

- 用Base 128 Varint编码的最大表示数为2^28，因为每个字节都要少一位，这种算法用来编码最小数字，如果数字较大就不推荐了。
- Base 128 Varint编码存储最大数字为：**2^（总位数-字节数)**

​		如300编码为1010 1100 0000 0010，

​        



#### 使用protobuf的好处？

- protobuf提供比其余格式（如XML和JSON）更**高效的数据序列化方式**；
- **protobuf是和语言无关的，意味着可以在不同类型的系统之间使用protobuf来交换数据；**
- protobuf提供简单且易使用的API来定义和序列化数据。



#### json相对于protobuf的好处

- 可读性强。json是纯文本格式，本身可以被任意文本编辑器打开，具有良好的可读性和可写性。
- 使用简单。其数据格式简单一点，对初学者易于理解和使用
- 具有自描述性。不需要额外的模式文件（如protobuf的.proto文件）就可以解析。



#### 如何在解析 Protocol Buffer 数据时处理错误？

​		通过`ParseFromString()`方法将数据反序列化，并通过bool类型判断是否序列化成功。



#### 为什么使用protobuf？（与json相比）

- **protobuf采用二进制存储，占用空间少；xml与json都是采用文本存储**
- **以key-value格式存储数据，protobuf不需要存储额外的信息；**
  - **如json存储数据是：name：“zhangsan”，pwd：“123456”；**
  - **而protobuf则是“zhangsan”，123456**



#### probobuf的作用？为什么使用protobuf

- 用于数据序列化`SerializeToString()`与反序列化`ParseFromString()`。
- 用于定义RPC服务方法描述（如RPC方法参数类型和返回值类型）
- 支持rpc方法的描述，有利于发布rpc服务



#### Protobuf里的message和service区别？

​		message：用于描述RPC方法的参数（LoginRequest注册请求）和返回值类型（LoginResponse注册响应）的序列化与反序列化；且都是从Message继承而来。

​		service：发布服务对象（service UserServiceRpc）和方法名（UserServiceRpc是服务名，里面的Login和Resgister是方法名），且都是从Service继承而来。



#### 通过protoc运行生成两个类

```c++
//1、服务提供者使用，真正执行RPC方法的一端
class UserServiceRpc : public google::protobuf::Service
//里面包含该RPC方法的纯虚函数和一个GetDescriptor()函数（用来得到服务的名字和方法）。
//    ==》因此需要自己定义类来重写里面的虚函数。且UserServiceRpc构造函数不需要传参数，有默认构造函数。
    
//2、服务消费者使用，用来统一做RPC方法的数据参数序列化和发起远程调用
class UserServiceRpc_stub : public google::protobuf::Service
//构造函数需要传入一个RpcChannel的指针来构造，无默认构造函数
//里面的像Login()和Register()等函数方法里面都是通过RpcChannel指针来调用CallMethod方法
//		===》而class RpcChannel是一个抽象类，有一个纯虚函数CallMethod()，因此需要自己定义一个类继承RpcChannel来重写CallMethod方法
//通过Stub对象调用服务方法最终都时调用CallMethod()方法
```



#### 如何发布一个远程的RPC服务？（用户代码）

![](https://img-blog.csdnimg.cn/e5af2eccf4fc4dad9b2d940fb82dd374.png)

**远端如何发布**

- 通过protobuf定义方法的请求类型（参数）和响应类型（返回值）
- 通过protobuf发布一个该参数与返回值的RPC服务方法。如Login方法和Register方法
- 服务提供者自定义一个类从UserServiceRpc继承而来，重写该方法如Login。四个步骤：
  - 从LoginRequest获取反序列化后的数据；（由网络功能muduo库实现的和protobuf的数据序列化与反序列化，然后上报到服务节点的Login方法上）
  - 做本地业务，并获取返回值
  - 填写响应消息对象（如错误码、错误消息、返回值）
  - 执行Run()回调函数（用于序列化和网络发送给客户端，就是调用SendRpcResponse()），把LoginResponse发送给rpc client



#### 服务器如何调用框架？

- 首先初始化；
- 然后定义一个provider对象（是一个rpc网络服务对象），调用NotifyService将UserService服务发布到rpc节点上；
- 最后调用Run()函数启动服务节点，开始提供RPC远程调用服务。



#### 框架代码：

​		实现Run()回调函数：

- 读取配置文件；（后续是muduo库来完成）
- 创建一个TcpService对象service（moduo库封装好的，使用moduo库的好处：分离了网络代码和业务代码）
- 绑定连接回调函数（service.setConnectionCallback()）和消息读写回调函数（service.setMessageCallback()，该函数最后调用服务对象的CallMethod方法，最终调用服务对象发布的RPC方法，如Login()方法）
- 设置moduo库的线程数量，4个，一个IO线程，仅用于socket用户连接，3个工作线程。典型的基于Epoll的IO多路复用的Reactor服务器。
- 开启muduo网络服务 `server.start()`和`m_eventLoop.loop()`。



#### 接受一个rpc调用请求时，怎么知道调用应用程序的哪个服务对象的哪个rpc方法呢？

​		NotifyService()函数生成一张map表记录服务对象和其发布的所有方法

```
Service类的对象        发布的方法
UserService          Login()和Request()方法
FriendService		AddFriend、DelFriend()和GetFriendList()方法
```



#### RPC服务提供方的流程

- 首先服务的提供方通过rpcprovider对象向RpcProvider注册服务对象和服务方法；
- 会通过protobuf抽象的Service和Method来把服务对象和服务方法保存到一个map表中；
- 服务提供方启动之后，相当于启动了一个Epoll+多线程的Reactor服务器，其中会绑定连接事件回调函数和消息读写事件的回调函数（读写事件回调相当于上面图中绿色的部分）；
- 当远程有新连接过来，muduo库会帮我们调用绑定的连接回调函数，连接回调函数不需要特殊操作，只需当客户端断开连接时，服务端收到后，将连接shutdown()断开就行。
- **当远程有数据到来后，将数据按照框架协商好的格式进行解析（header_size(4个字节) + header_str(包含请求对象、方法和参数长度（解决tcp粘包的问题）) + args_str），最终解析出来service_name，method_name和方法调用参数的数据。协商的具体格式为`header_size + service_name method_name args_size + args_str`**
  - **从抽象层动态生成请求与响应，将请求参数反序列化填到请求里面，响应由业务完成**。
  - 然后生成一个回调（会绑定一个方法）， 通过调用CallMethod()在框架上调用实际请求的方法（如Login()方法）
  - 实际请求方法从请求里面拿数据，做本地业务，填写相应的响应消息，执行上一步生成的回调，执行回调绑定的方法，即将响应消息序列化，通过muduo发送给RPC方法调用发起方，且服务提供者主动关闭连接（模拟http短连接）



#### 我方RPC服务调用方的流程

- 定义一个UserServiceRpc_Stub代理对象stub，传入一个我们重写RpcChannel的channel。
- 封装好RPC请求方法的请求参数（Request，如Login的姓名与密码）和响应。
- 通过stub代理对象以**同步阻塞的方式调用RPC方法**，最终都是调用rpcChannel基类指针调用CallMethod()方法，实现动态绑定，最终调用channel重写的CallMethod()方法。
  - CallMethod()需要完成的操作包括：rpc请求的数据组装，数据序列化；发送rpc请求，同步阻塞wait；接收rpc响应；响应的反序列化；
- 等待RPC请求的响应结果返回，读取调用的结果



#### 粘包问题

**假如有多个包，前面一个包发送过去了，接下来有一个包由于某些原因没有发送过去，导致后面的包黏在一起，这就是粘包**

**当远程有数据到来后，将数据按照框架协商好的格式进行解析（header_size(4个字节) + header_str(包含请求对象、方法和参数长度（解决tcp粘包的问题）) + args_str），最终解析出来service_name，method_name和方法调用参数的数据。协商的具体格式为`header_size + service_name method_name args_size + args_str`**

遇到的问题

​		当客户端收到response时，使用parseFromString()**反序列化失败**。原因是，通过socket进行通信时，接收到的数据使用char数组存储的，而parseFromString()函数接收的时string类型的字符串，通过string的构造函数将char数组变为string类型，但是该构造函数遇到\0（结束符）就认为结束了；解决方法不使用parseFromString()，使用ParseFromArray()，直接将char数组反序列化。

​		原因是：我们是使用char数组来存储rpc的响应数据的，当使用string的一个构造函数，接收字符数组的一部分并创建一个string对象时，碰到字符串'\0'会直接结束，导致序列化失败。



#### RpcController的作用？

​		当服务发起方发起远程方法调用时，远程方法调用可能不成功，需要自己实现一个rpccontroller类继承RPcController重写错误信息的方法，当远程调用失败时，往rpccontroller写入错误信息，服务发起方收到后判断是否调用成功，以及失败的错误信息是什么。



#### TCP和RPC的区别

TCP（传输控制协议）和RPC（远程过程调用）是两个不同的概念，它们分别涉及到网络通信和分布式系统中的远程调用。下面是TCP和RPC的区别：

**TCP（传输控制协议）：**

1. **定位：** TCP是一种网络协议，用于在网络中建立可靠的、面向连接的通信链路。它工作在传输层，负责数据的可靠传输和流控制。
2. **功能：** TCP提供了可靠的、全双工的、面向连接的通信，确保数据的有序性、完整性和可靠性。它处理的是网络通信的底层传输问题，如数据分段、重传、拥塞控制等。
3. **通信模式：** TCP采用客户端-服务器模式，一个TCP连接由一个客户端和一个服务器端组成。
4. **数据格式：** TCP并不关心传输的数据内容，它将数据视为一连串的字节流。

**RPC（远程过程调用）：**

1. **定位：** RPC是一种分布式系统的通信机制，用于在不同的计算机上调用远程的函数或方法。它涉及到应用层的通信，用于实现分布式系统中的函数调用。
2. **功能：** RPC允许像调用本地函数一样调用远程服务器上的函数，隐藏了底层通信的细节。它用于实现分布式系统中的模块间通信，让远程的过程调用看起来像本地调用。
3. **通信模式：** RPC采用类似于本地函数调用的模式，调用方发起调用请求，远程服务器执行相应的操作，然后返回结果。
4. **数据格式：** 在RPC中，参数和返回值的传输通常需要进行序列化和反序列化，以确保数据能够在网络上传输。



#### 为什么有HTTP协议还需要有RPC

1. **设计目标：**
   - **HTTP协议：** HTTP协议是一种应用层协议，最初设计用于在客户端和服务器之间传输超文本（例如网页）。它以文本为基础，支持请求-响应模型，具有灵活性和可扩展性。
   - **RPC：** RPC是一种远程通信机制，旨在让远程计算机上的函数调用看起来像本地函数调用。它着重于让分布式系统的不同模块之间进行函数调用和通信。
2. **通信方式：**
   - **HTTP协议：** HTTP采用请求-响应模式，客户端发送请求，服务器返回响应。它的数据格式通常是文本（如HTML、JSON）。
   - **RPC：** RPC提供更接近本地函数调用的模式，客户端直接调用远程服务器上的函数，并返回结果。
3. **数据格式：**
   - **HTTP协议：** HTTP的数据格式可以是各种文本类型，如HTML、XML、JSON等。
   - **RPC：** RPC在数据传输时通常需要进行序列化和反序列化，将数据转换为字节流以在网络上传输。
4. **性能和效率：**
   - **RPC：** 由于RPC专注于函数调用的效率和性能，它通常会采用更紧凑的序列化格式（如Protocol Buffers、MessagePack），并使用二进制传输，以提高通信效率。
   - **HTTP协议：** HTTP协议由于设计时的灵活性，可能会引入一些额外的开销，导致在某些情况下通信效率较低。



#### RPC可以用http协议吗

一些RPC框架允许你在HTTP协议上构建远程调用，常见的方法包括：

1. **JSON-RPC：** JSON-RPC是一种基于JSON格式的RPC协议，它可以在HTTP协议上进行通信。客户端将调用信息和参数封装为JSON格式，通过HTTP POST请求发送给服务器，服务器返回JSON格式的响应。
2. **RESTful API：** RESTful API是一种基于HTTP协议的通信方式，虽然它不是传统的RPC，但也可以被视为一种远程调用机制。通过HTTP方法（如GET、POST、PUT、DELETE）来调用不同的操作，参数可以通过URL参数或请求体传递，通常使用JSON格式来传输数据。
3. **gRPC over HTTP/2：** gRPC是Google开发的一种高性能的RPC框架，它支持在HTTP/2协议上进行通信。虽然HTTP/2协议不同于传统的HTTP/1.1，但它仍然是HTTP协议的一种，而gRPC则通过HTTP/2进行了更高效的通信。







### ZooKeeper

​		用作分布式协调服务：充当服务注册中心 、实现分布式锁；ls罗列路径下的节点，get查询某个节点的详细信息（关注数据和是否是永久性节点），creat创建节点，set修改节点的数据，delete删除节点（当前节点有子节点时，不能直接删除该节点，需要先将子节点全部删除才能删除该节点）

##### zk的数据是怎么组织的（目录树的结构）？

​		znode节点，与Linux系统很像，以/表示根路径，根路径下面可以有很多znode节点（可以**携带1M数据**，且可以创建znode子节点）



#### 为什么使用zookeeper作为服务注册中心？

​		在分布式环境中，需要用到一个服务注册中心，来将我们发布的服务注册到上面，客户端就可以在注册中心上找到对应的节点的IP地址和端口号，同时服务注册中心还需要监听注册的服务节点的变化，当注册的节点出现了故障，服务注册中心需要知道，然后动态删除该节点，同时也需要通知客户端某个节点发生了变化，而这一切工作Zookeeper都可以实现：通过心跳机制来检测rpc节点是否还存在，每隔一段时间zookeeper服务端就向rpc节点发送心跳消息来判断rpc节点是否存在，不存在就删除节点，同时客户端会注册watcher，当zookeeper有节点发生变化，通过watcher机制通知客户端节点发生了变化，并做出相应改变。

#### 为什么需要zookeeper？解决了什么问题？

​		问题：为支持高并发，一个OrderService被布置了多份，每个客户端需要保存一份服务提供者的URL列表（理解为ip地址与端口号），但这个列表是静态的，在配置文件写死了，当服务的提供者发生变化时，如某个机器宕机了，或者新增了OrderService实例，客户端根本不知道，需要手工更新配置文件才能得到最新的服务提供者的URL列表，很不方便。**问题：客户端和服务提供者的紧耦合**。

​		解决方案：解除耦合，增加一个中间层--注册中心，用来保存能提供服务的名称以及URL（IP地址与端口号）。首先这些服务会在注册中心进行注册，当客户端来查询的时候，只需要给出名称，注册中心就会返回一个URL。所有的客户端在访问服务前，都需要向这个**注册中心**进行询问，以获得最新的地址。

​		注册中心和各个服务实例维护Session会话，要求实例们定期发送心跳，一旦特定时间收不到心跳，就认为实例挂了，删除该实例。（只能删除临时性节点，永久性节点zk不能删除）该会话会维护一个心跳计数，每发送一次，没有收到心跳响应就+1，达到设定值就认为该实例挂了，删掉（临时性节点）



#### zookeeper的Watcher机制？

​		就是一个事件回调机制，通过给客户端添加一个监听器watcher，监听服务配置中心某个节点中子节点的变化，客户端会维护一个map表，键就是子节点的名字，值就是该子节点携带的数据（ip地址和端口号）。当子节点有变化（新增了子节点或某个子节点挂掉了），zk会主动告知客户端，该子节点相应的变化



#### 原生ZkClient API存在的问题？

- 设置**监听wachter只能是一次性的，每次触发后需要重新设置**。
- znode节点**只储存简单的byte字节数组**，如果存储对象，需要自己转换对象生成字节数组。
- **在网络IO线程，会在1/3的Timeout超时时间发送ping心跳消息**



#### 为什么使用zookeeper_mt多线程版本？

​		zookeeper的API客户端程序提供三个线程：

- API调用线程，调用zookeeper_init()（该函数只是用来创建句柄）函数的线程，返回后只是代表创建句柄成功了，如内存开辟初始化成功了，并不是连接zk_server成功了。zookeeper_init()是异步调用的，只是发起连接请求，并不代表调用这个函数就调用成功，当客户端真正接收到zk_server响应连接成功后，会调用watcher回调函数，给zk客户端发通知说连接成功。
- 网络IO线程， 通过pthread_create()创建一个线程用来和zk服务器发起网络连接，由于客户端不需要高并发，使用poll的IO多路复用。
- watcher回调线程，用来给zk客户端发送消息的（如连接成功的消息，通过信号量来等待直到连接建立成功）



#### Zookeeper分布式锁？

- **保持独占锁**：由于zookeeper中**节点的唯一性**特性，可将zookeeper上的znode看作一把锁，通过create znode的方式来实现。所有客户端都去创建**/distribute_lock**分布式节点，最终成功创建的的客户端拥有这把锁，用完之后删掉自己创建的distribute_lock节点，释放锁。
  - **缺点**：（**产生惊群效应**）当很多客户端在等待获取锁（等待的方式时使用watcher机制来监听distribute_lock节点的删除事件），当成功获取锁的进程释放该节点后，所有处于等待状态的客户端都会被唤醒，于是zookeeper会在短时间发送大量子节点变更事件给所有等待获取锁的客户端，而最终只有一个客户端获取锁。当集群规模较大时，会对zookeeper服务器的性能尝试较大影响。
- **控制时序锁（临时顺序节点）**：**/distribute_lock节点**预先存在，所有客户端在它下面创建临时有序节点，越早创建的节点编号越小，编号最小的获得锁。当自己的节点不是最小的，就没有获取锁。
  - **惊群效应解决**：每个节点只需监听比自己小的节点，当比自己小的节点删除后，客户端会收到watcher事件，此时判断自己的节点是不是所有子节点中最小的，如果是则获取锁，不是则继续等待。**每个客户端只需监听一个节点，不会导致惊群效应**。



#### redis实现分布式锁和zookeeper实现分布式锁的优缺点？

- Redis分布式锁：**强调的是高可用性，不具备强一致性**。（`set key value nx ex + 过期时间`）

  - 优点：完全基于内存，访问速度很快；Redis的处理线程为单线程，使用的是IO多路复用技术，实现一个线程处理多个IO流的效果，减少了创建多个线程的开销，避免不必要的上下文切换以及竞争临界资源导致的锁开销；全程使用hash结构（总结一个字，快，zookeeper是目录树结构）。

  - 缺点：Reids分布式锁获取简单粗暴，需要不断尝试去获取锁，耗性能；redis数据并不是**强一致性**的，可能出现数据不一致的情况；

    - （1）存在问题：

      ① 假如线程A成功得到了锁，并且设置的超时时间是 30 秒。如果某些原因导致线程 A 执行的很慢，过了 30 秒都没执行完，这时候锁过期自动释放，线程 B 得到了锁。

      ② 随后，线程A执行完任务，接着执行del指令来释放锁。但这时候线程 B 还没执行完，线程A实际上删除的是线程B加的锁。

      （2）解决方案：

      可以在 del 释放锁之前做一个判断，验证当前的锁是不是自己加的锁。**在加锁的时候把当前的线程 ID 当做value，并在删除之前验证 key 对应的 value 是不是自己线程的 ID**。但是，这样做其实隐含了一个新的问题，get操作、判断和释放锁是两个独立操作，不是原子性。对于非原子性的问题，我们可以使用Lua脚本来确保操作的原子性。

- zookeeper分布式锁（**目录树结构，都是建立临时节点**）：**强调的是强一致性，不具备高可用性，因此对并发量不能太高**。

  - 优点：设计定位是**分布式协调，具有强一致性**；如果获取不到锁，只需添加一个监听器即可，不需要一直轮询，性能消耗小。
  - 缺点：有较大客户端频繁申请加锁、释放锁（需要动态产生和删除临时节点），对zookeeper集群压力大。



#### zookeeper的leader election策略（选主）

​		每次投票包含了两个最基本的信息，包括服务器的SID（服务器的唯一标识）和ZXID（服务器的事务ID，致使该节点状态发生变化的操作的一个时间戳，全局有序）

- **服务器初始化启动时的选主**（集群节点数为2*N+1，为奇数，最起码为3个，**因为选举主节点时，要求可用节点的数量>总结点数量/2**）：
  - **每个节点发出一个投票**。第一次都会将票投给自己，投票内容包括SID和ZXID，（SID，ZXID）表示，并将各自投票发送给其余节点。
  - **接收来自其余各个节点的服务器的投票**。首先判断该投票的有效性，如检查是否是本轮投票，是否来自状态为LOOKING（该节点认为当前集群没有leader节点）的节点。
  - **处理投票**。将接收到的投票与自己的投票进行对比：
    - 首先比较ZXID。将票投给ZXID大的节点。
    - 如果ZXID相同，将票投给SID大的节点。
  - **统计投票**。每轮票投完后，统计投票信息，判断是否有节点收到过半的票数（**票数超过二分之一**），有则选出了leader。
  - **改变服务器状态**。一旦确认了leader，每个服务器变更自己的状态，如果是Follower，状态变为FOLLOWING，如果是Leader，变更为LEADING。
- **服务器运行时的Leader选举**（leader节点挂了，禁止写请求，进入新一轮选举，与启动时基本一致）



#### **Zookeeper** **是如何保证事务的顺序一致性的呢？**

​		当客户端发送一个事务请求时，zookeeper的leader节点会为该请求分配一个事务id，即ZXID（全局递增），并通过广播将这个事务请求给所有的following节点。ZXID是一个64位的数字，其中高32位是epoch，用来表示leader的变化次数（有新的leader选举出来，epoch自增），低32位是一个计数器，每生成一个新的事务就+1.。所有的following节点按照ZXID的顺序来执行这些事务请求。**由于zookeeper是强一致性的，意味着在处理事务时，zookeeper不能并行处理，事务处理是串行的**。



#### 描述一下项目流程

![](../../%25E6%25A1%258C%25E9%259D%25A2/%25E5%25BE%2585%25E5%2590%2588%25E5%25B9%25B6/%25E5%2588%25AB%25E4%25BA%25BA%25E7%259A%2584%25E9%259D%25A2%25E7%25BB%258F/Typora%25E7%2594%25A8%25E5%2588%25B0%25E5%259B%25BE%25E7%2589%2587/%25E9%25A1%25B9%25E7%259B%25AE%25E4%25BB%25A3%25E7%25A0%2581%25E4%25BA%25A4%25E4%25BA%2592%25E5%259B%25BE-%25E7%2594%25A8%25E7%2594%25BB%25E5%259B%25BE%25E6%259D%25BF%25E6%2589%2593%25E5%25BC%2580.png)

### muduo库



#### muduo的实现

使用“One Loop per thread”的设计模式，可以很好的处理网络IO和数据发送，不会导致我的框架在网络IO部分出现问题

One Loop per thread:一个线程里面还有一个EventLoop，一个Reactor模型包含4个部分，Event事件，主从Recator反应堆，事件分发器，事件处理器；在muduo库设计中，我们主线程的EventLoop只负责新用户的连接，每次找到连接的用户之后就会把他交给子线程进行处理，而每个子线程里面也维护者一个EventLoop，它负责监听已连接用户的读写事件，如果一个文件描述符上发生了对应文件描述符感兴趣的事件，就调用对应的事件处理器EventHandler进行处理

**muduo改多Reactor模型**：核心思想是，主反应堆线程只负责分发Acceptor连接建立，已连接套接字上的I/O事件交给sub-reactor负责分发。其中 sub-reactor的数量，可以根据CPU的核数来灵活设置。主要就是为了分担单Reactor模型下Reactor模型的压力（**一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**）



#### 为什么用muduo

muduo基于C++11进行实现，很好的支持了现代C++语法，使用bind进行回调时间的绑定，更有利于现代C++的使用，同时它的设计也非常棒，基于one Loop per thread，效率非常好，同时他对于C++新手非常友好，代码通俗易懂，而且一般的网络库线程之间都是通过加锁来实现线程间通信的，而**muduo网络库采取无锁的方**式，利用threadlocal来判断当前Chanel是否有该EventLoop执行，更加提升了效率，对于学习C++高性能服务器开发非常有帮助



#### muduo库的基本使用

```c++
void onMessage(const muduo::net::TcpConnectionPtr& conn, muduo::net::Buffer* buf, muduo::Timestamp time) {
	conn->send(buf);
}
int main(int argc, char* argv[])
{
	EventLoop loop;		// 创建事件循环
	InetAddress listenAddr(2000, false, ipv6);	// 创建Server端的地址结构
	EchoServer server(&loop, listenAddr);		// server绑定所属的EventLoop并指定地址
	server.setMessageCallback(onMessage);		// 绑定消息到来时的回调函数
	server.start();								// 启动线程
	loop.loop();								// 开启事件循环
}
```

​		首先创建EventLoop类对象（事件循环），然后使用InetAdderss类构建对应的地址结构（IP+port），然后传入loop以及地址结构初始化TcpServer对象，且绑定连接回调函数与消息收发回调函数，接着调用server对象的start方法完成三件事情：

1. 启动线程池；
2. listen()监听创建的监听socket文件描述符；
3. 将监听的socket的可读事件注册到EventLoop中进行关注；

最后调用loop.loop()函数来开启整个事件循环，来不断监听外来连接请求与读写请求。



#### EventLoop类

​		EventLoop事件循环，里面使用Epoll进行IO多路复用，用来监听是否有新连接的到来。



#### 项目中是如何实现高并发的？

​		muduo库基于Reactor模型实现了**高效的IO复用机制**，同时可以结合**线程池**来实现高并发处理。

- 基于Reactor模型的IO多路复用，来处理事件驱动的IO操作。主要组成部分包括：
  - Channel类：事件通道，封装文件描述符、注册各类事件处理函数（如读、写事件处理函数）、注册需要监听的文件描述符或删除。
  - poller类：（封装了和事件监听有关的方法与成员，调用一次Poller::poll方法它就能给你返回事件监听器的监听结果）**负责监听**文件描述符是否有事件发生以及**返回发生事件的文件描述符及具体事件**（通过poll函数实现，相当于epoll_wait()）。默认使用epoll。
  - EventLoop类：事件循环，负责监听和分发IO事件，一个eventLoop对应一个poller和多个Channel
  - EventLoopThread：事件循环线程，负责运行EventLoop，每个EventLoop对应一个EventLoopThread。
- 使用线程池来管理线程的机制，进行线程复用。
- 总的来说，（One Loop per thread的设计模式）muduo库使用一个或多个EventLoopThread来运行事件循环，每个EventLoopThread线程对应一个EventLoop。 



#### Linux上的五种IO模型？

​		阻塞IO、非阻塞IO、IO多路复用、信号驱动、异步IO



#### 介绍一下Reactor模型？

​		四个重要组件：**Event事件、Reactor反应堆、Demultiplex事件分发器、Eventhandler事件处理器**。



#### Muduo库为什么使用的是LT模式？

- **不会丢失数据或者消息**：应用没有读取完数据，内核会不断上报。
- **低延迟处理**：**每次读数据只需一次系统调用，照顾了多个连接的公平性，不会因为某个连接上的数据量过大而影响其余连接处理消息**。
- **跨平台**处理：像select一样可以跨平台使用，基本所有系统都支持LT模式，但ET模式不一定。





# Zookeeper=================

### 项目

#### 项目说说

利用C++11实现的简易RPC网络通信框架，实现了服务的发布、注册、远程过程调用等功能，在这个项目中我使用zookeeper的watch机制和znode节点实现服务的的分布式部署，利用protobuf进行数据的序列化和反序列化，利用主从Reactor模型（muduo）进行高性能网络发送，*利用生产者消费者模型编写异步日志模块*

#### 为什么需要zookeeper？解决了什么问题？

​		问题：为支持高并发，一个OrderService被布置了多份，每个客户端需要保存一份服务提供者的URL列表（理解为ip地址与端口号），但这个列表是静态的，在配置文件写死了，当服务的提供者发生变化时，如某个机器宕机了，或者新增了OrderService实例，客户端根本不知道，需要手工更新配置文件才能得到最新的服务提供者的URL列表，很不方便。**问题：客户端和服务提供者的紧耦合**。

​		解决方案：解除耦合，增加一个中间层--注册中心，用来保存能提供服务的名称以及URL（IP地址与端口号）。首先这些服务会在注册中心进行注册，当客户端来查询的时候，只需要给出名称，注册中心就会返回一个URL。所有的客户端在访问服务前，都需要向这个**注册中心**进行询问，以获得最新的地址。

​		注册中心和各个服务实例维护Session会话，要求实例们定期发送心跳，一旦特定时间收不到心跳，就认为实例挂了，删除该实例。（只能删除临时性节点，永久性节点zk不能删除）该会话会维护一个心跳计数，每发送一次，没有收到心跳响应就+1，达到设定值就认为该实例挂了，删掉（临时性节点）



#### zookeeper的Watcher机制？

​		就是一个**事件回调机制**，通过给客户端添加一个监听器watcher，监听服务配置中心某个节点中子节点的变化，客户端会维护一个map表，键就是子节点的名字，值就是该子节点携带的数据（ip地址和端口号）。当子节点有变化（新增了子节点或某个子节点挂掉了），zk会主动告知客户端，该子节点相应的变化



#### 原生ZkClient API存在的问题？

- 设置**监听wachter只能是一次性的，每次触发后需要重新设置**。
- znode节点**只储存简单的byte字节数组**，如果存储对象，需要自己转换对象生成字节数组。
- **在网络IO线程，会在1/3的Timeout超时时间发送ping心跳消息**



#### 为什么使用zookeeper_mt多线程版本？

​		zookeeper的API客户端程序提供三个线程：

- API调用线程，调用zookeeper_init()（该函数只是用来创建句柄）函数的线程，返回后只是代表创建句柄成功了，如**内存开辟初始化成功**了，并不是连接zk_server成功了。**zookeeper_init()是异步调用的，发起连接请求，并不代表调用这个函数就调用成功，当客户端真正接收到zk_server响应连接成功后，会调用watcher回调函数，给zk客户端发通知说连接成功**。
- 网络IO线程， 通过pthread_create()创建一个线程用来和zk服务器发起网络连接，由于客户端不需要高并发，使用poll的IO多路复用。
- watcher回调线程，用来给zk客户端发送消息的（如连接成功的消息，通过信号量来等待直到连接建立成功）



#### Zookeeper分布式锁？

- **保持独占锁**：由于zookeeper中**节点的唯一性**特性，可将zookeeper上的znode看作一把锁，通过create znode的方式来实现。所有客户端都去创建**/distribute_lock**节点，最终成功创建的的客户端拥有这把锁，用完之后删掉自己创建的distribute_lock节点，释放锁。
  - **缺点**：（**产生惊群效应**）当很多客户端在等待获取锁（等待的方式时使用watcher机制来监听distribute_lock节点的删除事件），当成功获取锁的进程释放该节点后，所有处于等待状态的客户端都会被唤醒，于是zookeeper会在短时间发送大量子节点变更事件给所有等待获取锁的客户端，而最终只有一个客户端获取锁。当集群规模较大时，会对zookeeper服务器的性能尝试较大影响。
- **控制时序锁（临时顺序节点）**：**/distribute_lock节点**预先存在，所有客户端在它下面创建临时有序节点，越早创建的节点编号越小，编号最小的获得锁。当自己的节点不是最小的，就没有获取锁。
  - **优点**：**每个节点只需监听比自己小的节点**，当比自己小的节点删除后，客户端会收到watcher事件，此时判断自己的节点是不是所有子节点中虽小的，如果是则获取锁，不是则继续等待。**每个客户端只需监听一个节点，不会导致惊群效应**。



#### redis实现分布式锁和zookeeper实现分布式锁的优缺点？

- Redis分布式锁：**强调的是高可用性，不具备强一致性**。（`set key value nx ex + 过期时间`）

  - 优点：完全基于内存，访问速度很快；Redis的处理线程为单线程，使用的是IO多路复用技术，实现一个线程处理多个IO流的效果，减少了创建多个线程的开销，避免不必要的上下文切换以及竞争临界资源导致的锁开销；全程使用hash结构（总结一个字，快，zookeeper是目录树结构）。

  - 缺点：Reids分布式锁获取简单粗暴，需要不断尝试去获取锁，耗性能；redis数据并不是**强一致性**的，可能出现数据不一致的情况；

    - （1）存在问题：

      ① 假如线程A成功得到了锁，并且设置的超时时间是 30 秒。如果某些原因导致线程 A 执行的很慢，过了 30 秒都没执行完，这时候锁过期自动释放，线程 B 得到了锁。

      ② 随后，线程A执行完任务，接着执行del指令来释放锁。但这时候线程 B 还没执行完，线程A实际上删除的是线程B加的锁。

      （2）解决方案：

      可以在 del 释放锁之前做一个判断，验证当前的锁是不是自己加的锁。**在加锁的时候把当前的线程 ID 当做value，并在删除之前验证 key 对应的 value 是不是自己线程的 ID**。但是，这样做其实隐含了一个新的问题，get操作、判断和释放锁是两个独立操作，不是原子性。对于非原子性的问题，我们可以使用Lua脚本来确保操作的原子性。

- zookeeper分布式锁（**目录树结构，都是建立临时节点**）：**强调的是强一致性，不具备高可用性，因此对并发量不能太高**。

  - 优点：设计定位是**分布式协调，具有强一致性**；如果获取不到锁，只需添加一个监听器即可，不需要一直轮询，性能消耗小。
  - 缺点：有较大客户端频繁申请加锁、释放锁（需要动态产生和删除临时节点），对zookeeper集群压力大。



#### zookeeper的leader election策略（选主）

​		每次投票包含了两个最基本的信息，包括服务器的SID（服务器的唯一标识）和ZXID（服务器的事务ID，致使该节点状态发生变化的操作的一个时间戳，全局有序）

- **服务器初始化启动时的选主**（集群节点数为2*N+1，为奇数，最起码为3个，**因为选举主节点时，要求可用节点的数量>总结点数量/2**）：
  - **每个节点发出一个投票**。第一次都会将票投给自己，投票内容包括SID和ZXID，（SID，ZXID）表示，并将各自投票发送给其余节点。
  - **接收来自其余各个节点的服务器的投票**。首先判断该投票的有效性，如检查是否是本轮投票，是否来自状态为LOOKING（该节点认为当前集群没有leader节点）的节点。
  - **处理投票**。将接收到的投票与自己的投票进行对比：
    - 首先比较ZXID。将票投给ZXID大的节点。
    - 如果ZXID相同，将票投给SID大的节点。
  - **统计投票**。每轮票投完后，统计投票信息，判断是否有节点收到过半的票数（**票数超过二分之一**），有则选出了leader。
  - **改变服务器状态**。一旦确认了leader，每个服务器变更自己的状态，如果是Follower，状态变为FOLLOWING，如果是Leader，变更为LEADING。
- **服务器运行时的Leader选举**（leader节点挂了，禁止写请求，进入新一轮选举，与启动时基本一致）



#### **Zookeeper** **是如何保证事务的顺序一致性的呢？**

​		当客户端发送一个事务请求时，zookeeper的leader节点会为该请求分配一个事务id，即ZXID（全局递增），并通过广播将这个事务请求给所有的following节点。ZXID是一个64位的数字，其中高32位是epoch，用来表示leader的变化次数（有新的leader选举出来，epoch自增），低32位是一个计数器，每生成一个新的事务就+1.。所有的following节点按照ZXID的顺序来执行这些事务请求。**由于zookeeper是强一致性的，意味着在处理事务时，zookeeper不能并行处理，事务处理是串行的**。



#### 为什么选择使用zookeeper作为服务注册中心

在分布式环境中，我们需要用到一个服务注册中心，来将我们发布的服务注册到上面取，来保证客户端在这个注册中心找到我们对应的节点，同时服务注册中心还要能够监听我们注册服务结点的变化，如何节点出现故障了我们就需要动态删除节点，同时也需要通知客户端节点节点发生了变化，需要做哪些处理，而Zookeeper正好实现了这么一个机制，它通过心跳机制来检测rpc节点是否还存在，每个一段事件zookeeper服务端就像rpc节点发送心跳包来判断rpc节点是否存在，如果不存在就删除节点，同时通过watch机制告诉客户端节点出现了变化，需要做出相应的变化。



注册中心里面会记录服务对象方法以及提供者的IP端口。这个获取服务对象方法提供者地址信息的过程就叫服务发现！

 服务发现机制主要提供两个功能！
 1. 服务注册：服务提供方在正式提供服务之前要先把自己的服务对象注册到注册中心上，注册中心会把这个服务提供者的地址信息以及提供的服务对象名+方法名保存下来。

 2. 服务订阅：在服务调用方启动时，会去注册中心找自己需要的服务对象对应的服务提供者的地址信息，然后缓存到本地，为远程调用做储备。

#### ZooKeeper怎么实现服务发现功能

**1、服务发现概述**

 在一个分布式环境下，服务方法的调用端怎么知道自己调用的服务方法在哪台机器上有提供？怎么获得这个机器的IP地址和端口以联系这个机器并请求服务？所以这里就需要**注册中心集群了，注册中心里面会记录服务对象方法以及提供者的IP端口。这个获取服务对象方法提供者地址信息的过程就叫服务发现！**

 **服务发现机制主要提供两个功能！**
 1. **服务注册**：服务提供方在正式提供服务之前要先把自己的服务对象注册到注册中心上，注册中心会把这个服务提供者的地址信息以及提供的服务对象名+方法名保存下来。

  2. **服务订阅**：在服务调用方启动时，会去注册中心找自己需要的服务对象对应的服务提供者的地址信息，然后缓存到本地，为远程调用做储备。

**2、ZooKeeper是怎么存储数据的？**
 只展示出了服务提供方目录。
![请添加图片描述](https://img-blog.csdnimg.cn/170e10e692a844ff9d4be1edf3bcc9e9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Zyo5Zyw6ZOB56uZ6YeM5ZCD6Ze45py6,size_20,color_FFFFFF,t_70,g_se,x_16)

1. 
   注册中心的管理者会在ZooKeep下创建一个服务根路径，可以根据接口来命名（上面的图就随便给了一个斜杠 “/” 作为根路径）。在这个路径下面可以再创建服务提供方目录与服务调用方目录。

2. **服务提供方注册时，发起注册时，**会在服务提供方目录中创建一个临时节点，这个节点存储服务提供方的注册信息，就比如上图中我的UserServiceRpc/Login节点中存的就是提供这个方法的服务器的IP端口号。
3. 服务调用方发起订阅时，服务调用方目录会创建一个临时节点，节点中存储服务调用方信息。
4. 当服务提供方目录中节点发生了任何变化时（新增节点，移除节点，节点上数据变动等），ZooKeeper就会通知发起订阅的服务调用方。

**3、Watcher机制**

 服务提供方目录中节点发生了任何变化，ZooKeeper通过Watcher机制通知订阅的服务调用方的
 为什么要有Watcher机制的存在，假如客户端保存了若干服务对象方法对应的服务提供者的地址信息，假如某个服务提供者挂了，在ZooKeeper中表现为服务提供方的某节点消失了。这个时候就需要告知客户端，这个节点消失了，无法继续提供服务了，没有资格继续被客户端缓存在本地了。

 Watcher机制简单来讲，就是**客户端告知服务端，如果某个节点或者这些子节点发生任何的变化，都必须通知客户端。不过要想服务端告知客户端，客户端必须先往上面注册一个对某个节点的watch事件。服务端发给客户端watch通知，不过服务端发送的watch通知里面不会包含节点数据，就只是告诉节点发生了什么watch事件。另外watch的触发是一次性触发**，假如客户端收到了这个watch通知之后，如果后面还想再继续收到相同节点的watch通知，那必须再注册一次对这个节点的watch事件。
 **客户端向服务端注册了watch事件，同时还需要向客户端中的WatchManager提供一个某节点watch事件对应的回调函数，当服务端向客户端通知watch时，WatchManager就会调用提前保存的回调函数。**









#### zk服务注册与发现怎么实现的（重复）

ZooKeeper（zk）是一个分布式协调服务，常用于服务发现、配置管理、分布式锁等场景。在ZooKeeper中实现服务注册与发现通常涉及以下步骤：

**服务注册：**

1. **创建ZooKeeper节点：** 为要注册的服务创建一个ZooKeeper节点，节点的路径通常表示服务的名称或类型。
2. **注册服务信息：** 在节点中存储服务的相关信息，如IP地址、端口号、健康状态等。这些信息可以被其他服务消费者查询。

**服务发现：**

1. **查询节点信息：** 服务消费者通过连接到ZooKeeper，查询指定路径的节点信息，获取注册的服务列表。
2. **监听节点变化：** ZooKeeper支持节点的监听机制。服务消费者可以注册一个监听器，当节点的内容发生变化时，会触发监听器回调。通过监听节点变化，可以及时获得新的服务列表。

##### Zookeeper服务注册与发现流程（回答）

[8、Zookeeper服务注册与发现原理浅析_zookeeper 服务发现原理_RonTech的博客-CSDN博客](https://blog.csdn.net/zyhlwzy/article/details/101847565)

Zookeeper的服务注册与发现，主要应用的是Zookeeper的Znode数据模型和Watcher机制，主要分为如下几个步骤：

- 服务注册：服务提供者（Provider）启动时，会向Zookeeper服务端注册服务信息，即会在Zookeeper服务器上创建一个服务节点，并在节点上存储服务的相关数据（如服务提供者的ip地址、端口等），比如注册一个用户注册服务（user/register）:

- 服务发现：服务消费者（Consumer）启动时，会根据本身依赖的服务信息，向Zookeeper服务端获取注册的服务信息并设置Watch，获取到注册的服务信息之后将服务提供者信息缓存在本地，调用服务时直接根据从Zookeeper注册中心获取到的服务注册信息调用服务，比如发现用户注册服务（user/register）并调用。

- 服务通知watch：当服务提供者因为某种原因宕机或不提供服务之后，Zookeeper服务注册中心的对应服务节点会被删除，因为服务消费者在获取服务信息的时候在对应节点上设置了Watch，因此节点删除之后会触发对应的Watcher，Zookeeper注册中心会异步向服务所关联的所有服务消费者发出节点删除的通知，服务消费者根据收到的通知更新缓存的服务列表。




#### zookeeper节点挂掉之后，有什么操作发生



##### Zookeeper 宕机如何处理？

Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；**如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader**。

**Zookeeper 集群的机制是只要超过半数的节点正常，集群就能正常提供服务**。只有在 Zookeeper 节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以：

3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票 > 1.5)

2 个节点的 cluster 就不能挂掉任何1个节点了(leader 可以得到 1 票 <= 1)



##### 集群有 3 服务器，一个节点宕机，Zk还可用吗

可以继续使用，单数服务器只要没超过一半的服务器宕机就可以继续使用。

集群规则为 2N+1 台，N >0，即最少需要 3 台。





##### Leader挂了，进入崩溃恢复，是如何选举Leader

服务器启动的Leader选举

zookeeper集群初始化阶段，服务器（myid=1-3）**「依次」**启动，开始zookeeper选举Leader~![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5791e745136d4da18aa4254e7c116f23~tplv-k3u1fbpfcp-zoom-1.image)

假设我们集群中有3台机器，那也就意味着我们需要2台同意（超过半数）。这里假设服务器1~3的myid分别为1,2,3，初始化Leader选举过程如下：

1. 服务器 1 启动，发起一次选举。它会首先 投票给自己 ，投票内容为(myid, ZXID)，因为初始化所以 ZXID 都为0，此时 server1 发出的投票为(1, 0)，即myid为1， ZXID为0。此时服务器 1 票数一票，不够半数以上，选举无法完成，服务器 1 状态保持为 LOOKING。
2. 服务器 2 启动，再发起一次选举。服务器2首先也会将投票选给自己(2, 0)，并将投票信息广播出去（server1也会，只是它那时没有其他的服务器了），server1 在收到 server2 的投票信息后会将投票信息与自己的作比较。**首先它会比较 ZXID ，ZXID 大的优先为 Leader，如果相同则比较 myid，myid 大的优先作为 Leader。所以，此时server1 发现 server2 更适合做 Leader，它就会将自己的投票信息更改为(2, 0)然后再广播出去，之后server2 收到之后发现和自己的一样无需做更改。**此时，服务器1票数0票，服务器2票数2票，投票已经超过半数，确定 server2 为 Leader。服务器 1更改状态为 FOLLOWING，服务器 2 更改状态为 LEADING。
3. 服务器 3 启动，发起一次选举。此时服务器 1，2已经不是 LOOKING 状态，它会直接以 FOLLOWING 的身份加入集群。



服务器运行期间的Leader选举

![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c974c58e0d4842cf8c7ef59d9fb28af2~tplv-k3u1fbpfcp-zoom-1.image)

运行时候如果Leader节点崩溃了会走崩溃恢复模式，新Leader选出前会暂停对外服务，大致可以分为四个阶段：选举、发现、同步、广播，此时Leader选举流程如下：

- Leader挂掉，剩下的两个 Follower 会将自己的状态 从 Following 变为 Looking 状态 ，每个Server会发出一个投票，第一次都是投自己，其中投票内容为(myid, ZXID)，注意这里的 zxid 可能不是0了
- 收集来自各个服务器的投票
- 处理投票，处理逻辑：优先比较ZXID，然后比较myid
- 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader
- 改变服务器状态Looking变为Following或Leading
- 然后依次进入发现、同步、广播阶段

举个例子来说明，假设集群有三台服务器，Leader (server2)挂掉了，只剩下server1和server3。 server1 给自己投票为(1,99)，然后广播给其他 server，server3 首先也会给自己投票(3,95)，然后也广播给其他 server。server1 和 server3 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（zxid 大的优先，如果相同那么就 myid 大的优先）。这个时候 server1 收到了 server3 的投票发现没自己的合适故不变，server3 收到 server1 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 server1 收到了发现自己的投票已经超过半数就把自己设为 Leader，server3 也随之变为 Follower。







### Zookeeper是什么，有什么用途

- 有使用过的，使用ZooKeeper作为**「RPC项目的注册中心」**，使用ZooKeeper实现**「分布式锁」**。
- ZooKeeper，它是一个开放源码的**「分布式协调服务」**，它是一个集群的管理者，它将简单易用的接口提供给用户。
- 可以基于Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列**「等功能」**。
- Zookeeper的**「用途」**：命名服务、配置管理、集群管理、分布式锁、队列管理

### ZK作为服务中心真的好吗？缺点？

ZooKeeper最大特点就是强一致性，只要ZooKeeper上面有一个节点发生了更新，都会要求其他节点一起更新，保证每个节点的数据都是完全实时同步的，在所有节点上的数据没有完全同步之前不干其他事。
 拿ZooKeeper搞点小项目其实还能应对，但是如果分布式环境中提供服务的和访问服务的机器越来越多，变化越来越频繁时，ZooKeeper为了维持这个强一致性需要付出很多代价，最后ZooKeeper服务注册中心会承受不住压力而崩溃。

### Zk不推荐用的原因

 zookeeper不适合做注册中心主要是因为实际项目中，宕机时候会涉及到master选举，这个选举时间比较长，选举期间zookeeper是不可用的，无法提供rpc服务，所以不适合做注册中心。





### Zookeeper的命名服务、配置管理、集群管理

- **「命名服务就是」**：

> 命名服务是指通过**「指定的名字」**来获取资源或者服务地址。Zookeeper可以创建一个**「全局唯一的路径」**，这个路径就可以作为一个名字。被命名的实体可以是**「集群中的机器，服务的地址，或者是远程的对象」**等。一些分布式服务框架（RPC、RMI）中的服务地址列表，通过使用命名服务，客户端应用能够根据特定的名字来获取资源的实体、服务地址和提供者信息等。

- **「配置管理：」** ：

> 实际项目开发中，我们经常使用.properties或者xml需要配置很多信息，如数据库连接信息、fps地址端口等等。因为你的程序一般是分布式部署在不同的机器上（如果你是单机应用当我没说），如果把程序的这些配置信息**「保存在zk的znode节点」**下，当你要修改配置，即znode会发生变化时，可以通过改变zk中某个目录节点的内容，利用**「watcher通知给各个客户端」**，从而更改配置。

- **「集群管理」**

> 集群管理包括集群监控和集群控制，监控集群机器状态，剔除机器和加入机器。zookeeper可以方便集群机器的管理，它可以实时监控znode节点的变化，一旦发现有机器挂了，该机器就会与zk断开连接，对用的临时目录节点会被删除，其他所有机器都收到通知。新机器加入也是类似酱紫，所有机器收到通知：有新兄弟目录加入啦。

**主节点选举**：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 Zookeeper 可以协助完成这个过程；

**分布式锁**：Zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。Zookeeper 可以对分布式锁进行控制。



### znode节点有几种类型？zk的数据模型是怎样的

#### zookeeper的数据模型

ZooKeeper的视图数据结构，很像Unix文件系统，也是**树状**的，这样可以确定每个路径都是唯一的。zookeeper的节点统一叫做**「znode」**，它是可以通过**「路径来标识」**，结构图如下：![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8d7058978e614f7f8ee4832cb3b9c7cb~tplv-k3u1fbpfcp-zoom-1.image)

#### znode的4种类型

根据节点的生命周期，znode可以分为4种类型，分别是持久节点、持久顺序节点、临时节点、临时顺序节点

- 持久节点（PERSISTENT）

> 这类节点被创建后，就会一直存在于Zk服务器上。直到手动删除。

- 持久顺序节点（PERSISTENT_SEQUENTIAL）

> 它的基本特性同持久节点，不同在于增加了顺序性。父节点会维护一个自增整性数字，用于子节点的创建的先后顺序。

- 临时节点（EPHEMERAL）

> 临时节点的生命周期与客户端的会话绑定，一旦客户端会话失效（非TCP连接断开），那么这个节点就会被自动清理掉。zk规定临时节点只能作为叶子节点。

- 临时顺序节点（EPHEMERAL_SEQUENTIAL）

> 基本特性同临时节点，添加了顺序的特性。



znode节点，与Linux系统很像，以/表示根路径，根路径下面可以有很多znode节点（可以**携带1M数据**，且可以创建znode子节点）





### znode节点存储什么？节点数据最大不超多少

Znode数据节点的代码如下

```typescript
public class DataNode implements Record {
    byte data[];                    
    Long acl;                       
    public StatPersisted stat;       
    private Set<String> children = null; 
}
```

哈哈，Znode包含了**「存储数据、访问权限、子节点引用、节点状态信息」**，如图：![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/91b23e62a6424ae2848a050556ed34df~tplv-k3u1fbpfcp-zoom-1.image)

- **「data:」** znode存储的业务数据信息
- **「ACL:」** 记录客户端对znode节点的访问权限，如IP等。
- **「child:」** 当前节点的子节点引用
- **「stat:」** 包含Znode节点的状态信息，比如**「事务id、版本号、时间戳」**等等。

每个节点的数据最大不能超过多少

为了**保证高吞吐和低延迟**，以及数据的一致性，znode只适合**存储非常小的数据**，**不能超过1M**，最好都小于1K。



### znode节点的监听机制、讲下Zk watch机制

- Watcher机制
- 监听机制的工作原理
- Watcher特性总结

#### Watcher监听机制

Zookeeper 允许客户端向服务端的某个Znode注册一个Watcher监听，当服务端的一些指定事件触发了这个Watcher，服务端会向指定客户端发送一个事件通知来实现分布式的通知功能，然后客户端根据 Watcher通知状态和事件类型做出业务上的改变。

> 可以把Watcher理解成客户端注册在某个Znode上的触发器，当这个Znode节点发生变化时（增删改查），就会触发Znode对应的注册事件，注册的客户端就会收到异步通知，然后做出业务的改变。

#### Watcher监听机制的工作原理

![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9477e0dada834eee9a997313a14a963b~tplv-k3u1fbpfcp-zoom-1.image)

- ZooKeeper的Watcher机制主要包括客户端线程、客户端 WatcherManager、Zookeeper服务器三部分。
- 客户端向ZooKeeper服务器注册Watcher的同时，会将Watcher对象存储在客户端的WatchManager中。
- 当zookeeper服务器触发watcher事件后，会向客户端发送通知， 客户端线程从 WatcherManager 中取出对应的 Watcher 对象来执行回调逻辑。

#### Watcher特性总结

- **「一次性:」** 一个Watch事件是一个一次性的触发器。一次性触发，客户端只会收到一次这样的信息。
- **「异步的：」** Zookeeper服务器发送watcher的通知事件到客户端是异步的，不能期望能够监控到节点每次的变化，Zookeeper只能保证最终的一致性，而无法保证强一致性。
- **「轻量级：」** Watcher 通知非常简单，它只是通知发生了事件，而不会传递事件对象内容。
- **「客户端串行：」** 执行客户端 Watcher 回调的过程是一个串行同步的过程。
- 注册 watcher用getData、exists、getChildren方法
- 触发 watcher用create、delete、setData方法

### Zookeeper的特性

Zookeeper 保证了如下分布式一致性特性：

- **「顺序一致性」**：从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。
- **「原子性」**：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。
- **「单一视图」**：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。
- **「可靠性：」** 一旦服务端成功地应用了一个事务，并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来。
- **「实时性（最终一致性）：」** Zookeeper 仅仅能保证在一定的时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。



### zookeeper是如何保证事务的顺序一致性的

> 需要了解事务ID，即zxid。ZooKeeper的在选举时**通过比较各结点的zxid和机器ID选出新的主结点的**。zxid由Leader节点生成，有新写入事件时，Leader生成新zxid并随提案一起广播，每个结点本地都保存了当前最近一次事务的zxid，zxid是递增的，所以谁的zxid越大，就表示谁的数据是最新的。

ZXID的生成规则如下：![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ea46600cd60d462ca982c60fa854419b~tplv-k3u1fbpfcp-zoom-1.image)

ZXID有两部分组成：

- 任期：完成本次选举后，直到下次选举前，由同一Leader负责协调写入；
- 事务计数器：单调递增，每生效一次写入，计数器加一。

> ZXID的低32位是计数器，所以同一任期内，ZXID是连续的，**每个结点又都保存着自身最新生效的ZXID**，**通过对比新提案的ZXID与自身最新ZXID是否相差“1”，来保证事务严格按照顺序生效的。**

**所以 ZooKeeper 是通过两阶段提交保证数据的最终一致性，并且通过严格按照 ZXID 的顺序生效提案保证其顺序一致性的。**



#### 最终一致性的两阶段过程

数据写入过程如下：

- 第一阶段：每次的数据写入事件作为提案广播给所有 Follower 结点；可以写入的结点返回确认信息 ACK；
- 第二阶段：Leader 收到一半以上的 ACK 信息后确认写入可以生效，向所有结点广播 COMMIT 将提案生效。

根据写入过程的两阶段的描述，可以知道 ZooKeeper 保证的是最终一致性，即 Leader 向客户端返回写入成功后，可能有部分 Follower 还没有写入最新的数据，所以是最终一致性。**ZooKeeper 保证的最终一致性也叫顺序一致性，即每个结点的数据都是严格按事务的发起顺序生效的**。







#### Zab 协议两阶段提交是如何保证事务严格按顺序生效

Leader 在收到半数以上 ACK 后会将提案生效并广播给所有 Follower 结点，Leader 为了保证提案按 ZXID 顺序生效，使用了一个 ConcurrentHashMap，记录所有未提交的提案，命名为 outstandingProposals，key 为 ZXID，Value 为提案的信息。

对 outstandingProposals 的访问逻辑如下：

- 每发起一个提案，会将提案的 ZXID 和内容放到 outstandingProposals 中，作为待提交的提案；
- 收到 Follower 的 ACK 信息后，根据 ACK 中的 ZXID 从 outstandingProposals 中找到对应的提案，对 ACK 计数；
- 执行 tryToCommit 尝试将提案提交，判断流程是，先判断当前 ZXID 之前是否还有未提交提案，如果有，当前提案暂时不能提交；再判断提案是否收到半数以上 ACK，如果达到半数则可以提交；如果可以提交，将当前 ZXID 从 outstandingProposals 中清除并向 Followers 广播提交当前提案；

#### Leader 是如何判断当前 ZXID 之前是否还有未提交提案

由于前提是保证顺序提交的，所以 Leader 只需判断 outstandingProposals 里，当前 ZXID 的前一个 ZXID 是否存在。

所以 ZooKeeper 是通过两阶段提交保证数据的最终一致性，并且通过严格按照 ZXID 的顺序生效提案保证其顺序一致性的。





### Zk服务器几种角色？Zk下Server工作状态有几种

Zookeeper 服务器角色

Zookeeper集群中，有**Leader、Follower和Observer**三种角色

**「Leader」**

> Leader服务器是整个ZooKeeper集群工作机制中的核心，其主要工作：

- 事务请求的唯一调度和处理者，保证集群事务处理的顺序性
- 集群内部各服务的调度者

**「Follower」**

> Follower服务器是ZooKeeper集群状态的跟随者，其主要工作：

- 处理客户端非事务请求，转发事务请求给Leader服务器
- 参与事务请求Proposal的投票
- 参与Leader选举投票

**「Observer」**

> Observer是3.3.0 版本开始引入的一个服务器角色，它充当一个观察者角色——观察ZooKeeper集群的最新状态变化并将这些状态变更同步过来。其工作：

- 处理客户端的非事务请求，转发事务请求给 Leader 服务器
- 不参与任何形式的投票

**Zookeeper下Server工作状态**

> 服务器具有四种状态，分别是 LOOKING、FOLLOWING、LEADING、OBSERVING。

- 1.LOOKING：寻找Leader状态。当服务器处于该状态时，它会认为当前集群中没有 Leader，因此需要进入 Leader 选举状态。
- 2.FOLLOWING：跟随者状态。表明当前服务器角色是Follower。
- 3.LEADING：领导者状态。表明当前服务器角色是Leader。
- 4.OBSERVING：观察者状态。表明当前服务器角色是Observer。



**ZooKeeper集群部署图**

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6cf565bd494b4262baee09cfadfee24d~tplv-k3u1fbpfcp-zoom-1.image)ZooKeeper集群是一主多从的结构：

- 如果是写入数据，先写入主服务器（主节点），再通知从服务器。
- 如果是读取数据，既读主服务器的，也可以读从服务器的。



### ZooKeeper如何保证主从节点数据一致性

集群是主从部署结构，要保证主从节点一致性问题，无非就是两个主要问题：

- **「主服务器挂了，或者重启了」**
- **「主从服务器之间同步数据」**~

Zookeeper是采用ZAB协议（Zookeeper Atomic Broadcast，Zookeeper**原子广播协议**）来保证主从节点数据一致性的，ZAB协议支持**「崩溃恢复和消息广播」**两种模式，很好解决了这两个问题：

- 崩溃恢复：Leader挂了，进入该模式，选一个新的leader出来
- 消息广播： 把更新的数据，从Leader同步到所有Follower

> Leader服务器挂了，所有集群中的服务器进入LOOKING状态，首先，它们会选举产生新的Leader服务器；接着，新的Leader服务器与集群中Follower服务进行数据同步，当集群中超过半数机器与该 Leader服务器完成数据同步之后，退出恢复模式进入消息广播模式。Leader 服务器开始接收客户端的事务请求生成事务Proposal进行事务请求处理。



### Zookeeper 怎么保证主从节点的状态同步？

Zookeeper 的核心是原子广播机制，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式。

- **1. 恢复模式**

当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。

- **2. 广播模式**

一旦 leader 已经和多数的 follower 进行了状态同步后，它就可以开始广播消息了，即进入广播状态。这时候当一个 server 加入 ZooKeeper 服务中，它会在恢复模式下启动，发现 leader，并和 leader 进行状态同步。待到同步结束，它也参与消息广播。ZooKeeper 服务一直维持在 Broadcast 状态，直到 leader 崩溃了或者 leader 失去了大部分的 followers 支持。





###  ZooKeeper是怎么存储数据的？

展示出了服务提供方目录![请添加图片描述](https://img-blog.csdnimg.cn/170e10e692a844ff9d4be1edf3bcc9e9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Zyo5Zyw6ZOB56uZ6YeM5ZCD6Ze45py6,size_20,color_FFFFFF,t_70,g_se,x_16)

注册中心的管理者会在ZooKeep下创建一个服务根路径，可以根据接口来命名（我上面的图就随便给了一个斜杠 “/” 作为根路径）。在这个路径下面可以再创建服务提供方目录与服务调用方目录。
服务提供方注册时，发起注册时，会在服务提供方目录中创建一个临时节点，这个节点存储服务提供方的注册信息，就比如上图中我的UserServiceRpc/Login节点中存的就是提供这个方法的服务器的IP端口号。
服务调用方发起订阅时，服务调用方目录会创建一个临时节点，节点中存储服务调用方信息。
当服务提供方目录中节点发生了任何变化时（新增节点，移除节点，节点上数据变动等），ZooKeeper就会通知发起订阅的服务调用方。



### 谈下你对 ZAB 协议的了解？

ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。ZAB 协议包括两种基本的模式：崩溃恢复和消息广播。

当整个 Zookeeper 集群刚刚启动或者Leader服务器宕机、重启或者网络故障导致不存在过半的服务器与 Leader 服务器保持正常通信时，所有服务器进入崩溃恢复模式，首先选举产生新的 Leader 服务器，然后集群中 Follower 服务器开始与新的 Leader 服务器进行数据同步。当集群中超过半数机器与该 Leader 服务器完成数据同步之后，退出恢复模式进入消息广播模式，Leader 服务器开始接收客户端的事务请求生成事物提案来进行事务请求处理。





### 一致性协议之 ZAB

 ZAB(ZooKeeper Automic Broadcast) 原子广播协议，该协议能够很好地支持 崩溃恢复 。

#### ZAB 中的三个角色

ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。

- Leader ：集群中 唯一的写请求处理者 ，能够发起投票（投票也是为了进行写请求）。

- Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。
- Observer ：就是没有选举权和被选举权的 Follower 。

在 ZAB 协议中对 zkServer(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 消息广播 和 崩溃恢复 。

#### ZXID和myid

ZooKeeper 采用全局递增的事务 id 来标识，所有 proposal(提议)在被提出的时候加上了ZooKeeper Transaction Id 。ZXID是64位的Long类型，这是保证事务的顺序一致性的关键。ZXID中高32位表示纪元epoch，低32位表示事务标识xid。你可以认为zxid越大说明存储数据越新，如下图所示：

1. 每个leader都会具有不同的epoch值，表示一个纪元/朝代，用来标识 leader周期。每个新的选举开启时都会生成一个新的epoch，从1开始，每次选出新的Leader，epoch递增1，并会将该值更新到所有的zkServer的zxid的epoch。

2. xid是一个依次递增的事务编号。数值越大说明数据越新，可以简单理解为递增的事务id。每次epoch变化，都将低32位的序号重置，这样保证了zxid的全局递增性。

每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群唯一的id（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其id与hostname必须一一对应，如下所示。在该配置文件中，server.后面的数据即为myid

server.1=zoo1:2888:3888
server.2=zoo2:2888:3888
server.3=zoo3:2888:3888



#### 历史队列

每一个follower节点都会有一个先进先出（FIFO)的队列用来存放收到的事务请求，保证执行事务的顺序。所以：

- 可靠提交由ZAB的事务一致性协议保证
- 全局有序由TCP协议保证
- 因果有序由follower的历史队列(history queue)保证

####  消息广播模式

ZAB协议两种模式：**消息广播模式和崩溃恢复模式**。

![img](https://img-blog.csdnimg.cn/img_convert/3d3ee303bf8e27860e9956d07661c59c.png)

说白了就是 ZAB 协议是如何处理写请求的，上面我们不是说只有 Leader 能处理写请求嘛？那么我们的 Follower 和 Observer 是不是也需要 同步更新数据 呢？总不能数据只在 Leader 中更新了，其他角色都没有得到更新吧。

第一步肯定需要 Leader 将写请求 广播 出去呀，让 Leader 问问 Followers 是否同意更新，如果超过半数以上的同意那么就进行 Follower 和 Observer 的更新（和 Paxos 一样）。消息广播机制是通过如下图流程

#### **保证事务的顺序一致性的：**

![img](https://img-blog.csdnimg.cn/img_convert/736319f968748755bfaec592d4a76e96.png)

1. leader从客户端收到一个写请求

2. leader生成一个新的事务并为这个事务生成一个唯一的ZXID
3. leader将这个事务发送给所有的follows节点，将带有 zxid 的消息作为一个提案(proposal)分发给所有 follower。
4. follower节点将收到的事务请求加入到历史队列(history queue)中，当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK
5. 当leader**收到大多数follower（超过一半）的ack消息**，leader会向follower发送commit请求（leader自身也要提交这个事务）
6. 当follower收到commit请求时，会**判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小**，如果是则提交事务，如果不是则等待比它更小的事务的commit(保证顺序性)
7. Leader将处理结果返回给客户端

**过半写成功策略：**Leader节点接收到写请求后，这个Leader会将写请求广播给各个Server，各个Server会将该写请求加入历史队列，并向Leader发送ACK信息，当**Leader收到一半以上的ACK消息**后，说明该写操作可以执行。Leader会向各个server发送commit消息，各个server收到消息后执行commit操作。

这里要注意以下几点：

- Leader并不需要得到Observer的ACK，即Observer无投票权

- Leader不需要得到所有Follower的ACK，只要收到过半的ACK即可，同时Leader本身对自己有一个ACK
- Observer虽然无投票权，但仍须**同步Leader的数据从而在处理读请求时可以返回尽可能新的数据**

另外，**Follower/Observer也可以接受写请求**，此时：

- Follower/Observer接受写请求以后，不能直接处理，而需要将写请求转发给Leader处理

- 除了多了一步请求转发，其它流程与直接写Leader无任何区别
- Leader处理写请求是通过上面的消息广播模式，实质上最后所有的zkServer都要执行写操作，这样数据才会一致

而对于读请求，Leader/Follower/Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。由于处理读请求不需要各个服务器之间的交互，因此Follower/Observer越多，整体可处理的读请求量越大，也即读性能越好。

#### 崩溃恢复模式

恢复模式大致可以分为四个阶段：选举、发现、同步、广播。

1. **选举阶段**（Leader election）：当leader崩溃后，集群进入选举阶段，开始选举出潜在的准 leader，然后进入下一个阶段。
2. **发现阶段**（Discovery）：用于在**从节点**中发现最新的ZXID和事务日志。准Leader接收所有Follower发来各自的最新epoch值。Leader从中选出最大的epoch，基于此值加1，生成新的epoch分发给各个Follower。各个Follower收到全新的epoch后，返回ACK给Leader，**带上各自最大的ZXID和历史提议日志**。Leader选出最大的ZXID，并更新自身历史日志，此时Leader就用拥有了最新的提议历史。（注意：每次epoch变化时，ZXID的第32位从0开始计数）。
3. **同步阶段**（Synchronization）：主要是利用 leader 前一阶段获得的最新提议历史，同步给集群中所有的Follower。**只有当超过半数Follower同步成功，这个准Leader才能成为正式的Leader。**这之后，follower 只会接收 zxid 比自己的 lastZxid 大的提议。
4. 广播阶段（Broadcast）：集群恢复到广播模式，开始接受客户端的写请求。

在发现阶段，或许有人会问：既然Leader被选为主节点，已经是集群里数据最新的了，为什么还要从节点中寻找最新事务呢？这是为了防止某些意外情况。所以这一阶段，Leader集思广益，接收所有Follower发来各自的最新epoch值。

这里有两点要注意：

#### **当集群中有机器挂了，我们整个集群如何保证数据一致性？**

如果只是 `Follower` 挂了，而且挂的没超过半数的时候，因为我们一开始讲了在 `Leader` 中会维护队列，所以不用担心后面的数据没接收到导致数据不一致性。

如果 `Leader` 挂了那就麻烦了，我们肯定需要先暂停服务变为 `Looking` 状态然后进行 `Leader` 的重新选举（上面我讲过了），但这个就要分为两种情况了，分别是 **确保已经被 Leader 提交的提案最终能够被所有的 Follower 提交** 和 **跳过那些已经被丢弃的提案** 。



##### 确保已经被Leader提交的提案最终能够被所有的Follower提交

假设 Leader (server2) 发送 commit 请求，他发送给了 server3，然后要发给 server1 的时候突然挂了。这个时候重新选举的时候我们如果把 server1 作为 Leader 的话，那么肯定会产生**数据不一致性**，因为 server3 肯定会提交刚刚 server2 发送的 commit 请求的提案，而`server1` 根本没收到所以会丢弃。

![img](https://img-blog.csdnimg.cn/img_convert/dcc618d6991dc819108cccaad9450bc4.png)

**怎么解决数据不一致问题？**

这个时候 server1 已经不可能成为 Leader 了，因为 server1 和 server3 进行投票选举的时候会比较 ZXID ，而此时 server3 的 ZXID 肯定比 server1 的大了（后面讲到选举机制时就明白了）。同理，**只能由server3当Leader，server3当上Leader之后，在同步阶段，会将最新提议历史同步给集群中所有的Follower，这就保证数据一致性**了。如果server2在某个时刻又重新恢复了，它作为Follower 的身份进入集群中，再向Leader同步当前最新提议和Zxid即可。

##### **确保跳过那些已经被丢弃的提案**

![img](https://img-blog.csdnimg.cn/img_convert/9b932c14ca9a296001b216ba81de9a5e.png)

假设 Leader (server2) 此时同意了提案N1，自身提交了这个事务并且要发送给所有 Follower 要 commit 的请求，却在这个时候挂了，此时肯定要重新进行 Leader 的选举，假如此时选 server1 为 Leader （这无所谓，server1和server2都可以当选）。但是过了一会，这个 挂掉的 Leader 又重新恢复了 ，此时它肯定会作为 Follower 的身份进入集群中，需要注意的是**刚刚 server2 已经同意提交了提案N1**，但其他 server 并没有收到它的 commit 信息，所以**其他 server 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 该提案N1最终需要被抛弃掉** 。

![img](https://img-blog.csdnimg.cn/img_convert/f052e9b28d002ef54949de17b9378177.png)







### Leader挂了，进入崩溃恢复，是如何选举Leader

#### 服务器启动的Leader选举

zookeeper集群初始化阶段，服务器（myid=1-3）**「依次」**启动，开始zookeeper选举Leader~![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5791e745136d4da18aa4254e7c116f23~tplv-k3u1fbpfcp-zoom-1.image)

假设我们集群中有3台机器，那也就意味着我们需要2台同意（超过半数）。这里假设服务器1~3的myid分别为1,2,3，初始化Leader选举过程如下：

1. 服务器 1 启动，发起一次选举。它会首先 投票给自己 ，投票内容为(myid, ZXID)，因为初始化所以 ZXID 都为0，此时 server1 发出的投票为(1, 0)，即myid为1， ZXID为0。此时服务器 1 票数一票，不够半数以上，选举无法完成，服务器 1 状态保持为 LOOKING。
2. 服务器 2 启动，再发起一次选举。服务器2首先也会将投票选给自己(2, 0)，并将投票信息广播出去（server1也会，只是它那时没有其他的服务器了），server1 在收到 server2 的投票信息后会将投票信息与自己的作比较。**首先它会比较 ZXID ，ZXID 大的优先为 Leader，如果相同则比较 myid，myid 大的优先作为 Leader。所以，此时server1 发现 server2 更适合做 Leader，它就会将自己的投票信息更改为(2, 0)然后再广播出去，之后server2 收到之后发现和自己的一样无需做更改。**此时，服务器1票数0票，服务器2票数2票，投票已经超过半数，确定 server2 为 Leader。服务器 1更改状态为 FOLLOWING，服务器 2 更改状态为 LEADING。
3. 服务器 3 启动，发起一次选举。此时服务器 1，2已经不是 LOOKING 状态，它会直接以 FOLLOWING 的身份加入集群。



#### 服务器运行期间的Leader选举

![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c974c58e0d4842cf8c7ef59d9fb28af2~tplv-k3u1fbpfcp-zoom-1.image)

运行时候如果Leader节点崩溃了会走崩溃恢复模式，新Leader选出前会暂停对外服务，大致可以分为四个阶段：选举、发现、同步、广播，此时Leader选举流程如下：

- Leader挂掉，剩下的两个 Follower 会将自己的状态 从 Following 变为 Looking 状态 ，每个Server会发出一个投票，第一次都是投自己，其中投票内容为(myid, ZXID)，注意这里的 zxid 可能不是0了
- 收集来自各个服务器的投票
- 处理投票，处理逻辑：优先比较ZXID，然后比较myid
- 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader
- 改变服务器状态Looking变为Following或Leading
- 然后依次进入发现、同步、广播阶段

举个例子来说明，假设集群有三台服务器，Leader (server2)挂掉了，只剩下server1和server3。 server1 给自己投票为(1,99)，然后广播给其他 server，server3 首先也会给自己投票(3,95)，然后也广播给其他 server。server1 和 server3 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（zxid 大的优先，如果相同那么就 myid 大的优先）。这个时候 server1 收到了 server3 的投票发现没自己的合适故不变，server3 收到 server1 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 server1 收到了发现自己的投票已经超过半数就把自己设为 Leader，server3 也随之变为 Follower。





### zk分布式锁的实现原理

Zookeeper就是使用临时顺序节点特性实现分布式锁的。

- 获取锁过程 （创建临时节点，检查序号最小）
- 释放锁 （删除临时节点，监听通知）

#### 如何基于 ZooKeeper 实现分布式锁？

ZooKeeper 分布式锁是基于 **临时顺序节点** 和 **Watcher（事件监听器）** 实现的。

获取锁：

1. 首先我们要有一个持久节点`/locks`，客户端获取锁就是在`locks`下创建临时顺序节点。
2. 假设客户端 1 创建了`/locks/lock1`节点，创建成功之后，会判断 `lock1`是否是 `/locks` 下最小的子节点。
3. 如果 `lock1`是最小的子节点，则获取锁成功。否则，获取锁失败。
4. 如果获取锁失败，则说明有其他的客户端已经成功获取锁。客户端 1 并不会不停地循环去尝试加锁，而是在前一个节点比如`/locks/lock0`上注册一个事件监听器。这个监听器的作用是当前一个节点释放锁之后通知客户端 1（避免无效自旋），这样客户端 1 就加锁成功了。

释放锁：

1. 成功获取锁的客户端在执行完业务流程之后，会将对应的子节点删除。
2. 成功获取锁的客户端在出现故障之后，对应的子节点由于是临时顺序节点，也会被自动删除，避免了锁无法被释放。
3. 我们前面说的事件监听器其实监听的就是这个子节点删除事件，子节点删除就意味着锁被释放。



#### redis实现分布式锁和zookeeper实现分布式锁的优缺点？

- Redis分布式锁：**强调的是高可用性，不具备强一致性**。（`set key value nx ex + 过期时间`）

  - 优点：完全基于内存，访问速度很快；Redis的处理线程为单线程，使用的是IO多路复用技术，实现一个线程处理多个IO流的效果，减少了创建多个线程的开销，避免不必要的上下文切换以及竞争临界资源导致的锁开销；全程使用hash结构（总结一个字，快，zookeeper是目录树结构）。

  - 缺点：Reids分布式锁获取简单粗暴，需要不断尝试去获取锁，耗性能；redis数据并不是**强一致性**的，可能出现数据不一致的情况；

    - （1）存在问题：

      ① 假如线程A成功得到了锁，并且设置的超时时间是 30 秒。如果某些原因导致线程 A 执行的很慢，过了 30 秒都没执行完，这时候锁过期自动释放，线程 B 得到了锁。

      ② 随后，线程A执行完任务，接着执行del指令来释放锁。但这时候线程 B 还没执行完，线程A实际上删除的是线程B加的锁。

      （2）解决方案：

      可以在 del 释放锁之前做一个判断，验证当前的锁是不是自己加的锁。**在加锁的时候把当前的线程 ID 当做value，并在删除之前验证 key 对应的 value 是不是自己线程的 ID**。但是，这样做其实隐含了一个新的问题，get操作、判断和释放锁是两个独立操作，不是原子性。对于非原子性的问题，我们可以使用Lua脚本来确保操作的原子性。

- zookeeper分布式锁（**目录树结构，都是建立临时节点**）：**强调的是强一致性，不具备高可用性，因此对并发量不能太高**。

  - 优点：设计定位是**分布式协调，具有强一致性**；如果获取不到锁，只需添加一个监听器即可，不需要一直轮询，性能消耗小。
  - 缺点：有较大客户端频繁申请加锁、释放锁（需要动态产生和删除临时节点），对zookeeper集群压力大。



#### redis如何实现分布式锁



Redis可以通过实现分布式锁来协调多个节点之间的并发访问，确保在特定时刻只有一个节点能够获得锁并执行关键操作。分布式锁通常用于避免多个节点同时修改共享资源，从而保证数据的一致性和正确性。以下是一种基于Redis的分布式锁实现方法：

**使用SETNX（SET if Not eXists）指令：**

1. **获取锁：** 当一个节点想要获取锁时，它可以尝试在Redis中设置一个特定的键（锁名）和一个随机值（锁值），同时设置一个合理的超时时间（避免死锁）。

   ```
   pythonCopy code
   SETNX lock_name unique_value
   ```

   - 如果键不存在，说明该节点获得了锁，可以执行关键操作。
   - 如果键已存在，说明锁已被其他节点持有，该节点无法获得锁，需要等待或执行其他策略。

2. **释放锁：** 当节点执行完关键操作后，需要释放锁。为了保证只有锁的持有者才能释放锁，可以使用Lua脚本来执行释放操作：

   ```
   luaCopy codeif redis.call("GET", KEYS[1]) == ARGV[1] then
       return redis.call("DEL", KEYS[1])
   else
       return 0
   end
   ```

   - 在Lua脚本中，首先检查锁值是否匹配，如果匹配则删除锁键，释放锁。
   - 如果锁值不匹配，说明锁可能已被其他节点获取，不应该释放。

这种方法使用了原子性的SETNX指令来确保只有一个节点能够成功设置锁，避免了竞争条件。同时，使用Lua脚本来释放锁也确保了操作的原子性。





##### redis有哪些命令？lpush是用在哪个数据结构中的?

Redis命令以及它们的作用：

1. **SET key value：** 设置指定键的值。
2. **GET key：** 获取指定键的值。
3. **DEL key：** 删除指定键及其对应的值。
4. **EXPIRE key seconds：** 设置指定键的过期时间。
5. **INCR key：** 将指定键的值增加1。
6. **DECR key：** 将指定键的值减少1。
7. **LPUSH key value：** 将一个值插入到列表的左侧（List）。
8. **RPUSH key value：** 将一个值插入到列表的右侧（List）。
9. **LPOP key：** 移除并返回列表的最左侧元素。
10. **RPOP key：** 移除并返回列表的最右侧元素。
11. **HMSET key field value [field value ...]：** 在哈希（Hash）中设置多个字段的值。
12. **HGET key field：** 在哈希中获取指定字段的值。
13. **SADD key member [member ...]：** 向集合（Set）添加一个或多个成员。
14. **SMEMBERS key：** 获取集合中的所有成员。
15. **ZADD key score member [score member ...]：** 向有序集合（Sorted Set）添加一个或多个成员。
16. **ZRANGE key start stop：** 获取有序集合中指定范围的成员。

关于你提到的`LPUSH`命令，它是用在列表（List）数据结构中的。列表是一种有序的数据结构，允许存储多个值，每个值可以重复。`LPUSH`命令将一个或多个值插入到列表的左侧，即列表的头部，这样最新添加的值将成为列表的第一个元素，而之前的元素会依次后移。







#### 获取锁过程

- 当第一个客户端请求过来时，Zookeeper客户端会创建一个持久节点/locks。如果它（Client1）想获得锁，需要在locks节点下创建一个顺序节点lock1.如图![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/39e5ee84f901453fb894600c331693d6~tplv-k3u1fbpfcp-zoom-1.image)
- 接着，客户端Client1会查找locks下面的所有临时顺序子节点，判断自己的节点lock1是不是排序最小的那一个，如果是，则成功获得锁。![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e958178a196742b480a20dff85dfef4f~tplv-k3u1fbpfcp-zoom-1.image)
- 这时候如果又来一个客户端client2前来尝试获得锁，它会在locks下再创建一个临时节点lock2![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/34a3a0f7221b4de79e1ed5595d7c177c~tplv-k3u1fbpfcp-zoom-1.image)
- 客户端client2一样也会查找locks下面的所有临时顺序子节点，判断自己的节点lock2是不是最小的，此时，发现lock1才是最小的，于是获取锁失败。获取锁失败，它是不会甘心的，client2向它排序**靠前的节点lock1注册Watcher事件**，用来监听lock1是否存在，也就是说client2抢锁失败进入等待状态。![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a17880856e1546bba4f2571dbf8b3b38~tplv-k3u1fbpfcp-zoom-1.image)
- 此时，如果再来一个客户端Client3来尝试获取锁，它会在locks下再创建一个临时节点lock3![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/abda17df527b4dbe8faa7e3a3a9e859e~tplv-k3u1fbpfcp-zoom-1.image)
- 同样的，client3一样也会查找locks下面的所有临时顺序子节点，判断自己的节点lock3是不是最小的，发现自己不是最小的，就获取锁失败。它也是不会甘心的，它会向在它前面的节点lock2注册Watcher事件，以监听lock2节点是否存在。![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5a8725119723434daddca761b0d7be83~tplv-k3u1fbpfcp-zoom-1.image)

#### 释放锁

我们再来看看释放锁的流程，zookeeper的**「客户端业务完成或者故障」**，都会删除临时节点，释放锁。如果是任务完成，Client1会显式调用删除lock1的指令![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ecf2bec179f64ef79b180f66a327e634~tplv-k3u1fbpfcp-zoom-1.image)如果是客户端故障了，根据临时节点得特性，lock1是会自动删除的![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e87703be60a3407e9e570c86bd4c180d~tplv-k3u1fbpfcp-zoom-1.image)lock1节点被删除后，Client2可开心了，因为它一直监听着lock1。lock1节点删除，Client2立刻收到通知，也会查找locks下面的所有临时顺序子节点，发下lock2是最小，就获得锁。![img](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be3c7146f2934dd69a598cdf974ab390~tplv-k3u1fbpfcp-zoom-1.image)

同理，Client2获得锁之后，Client3也对它虎视眈眈



#### 为什么要用临时顺序节点？

**临时节点相比持久节点，最主要的是对会话失效的情况处理不一样，临时节点会话消失则对应的节点消失。这样的话，如果客户端发生异常导致没来得及释放锁也没关系，会话失效节点自动被删除，不会发生死锁的问题。**

使用 Redis 实现分布式锁的时候，我们是通过过期时间来避免锁无法被释放导致死锁问题的，而 ZooKeeper 直接利用临时节点的特性即可。

假设不使用顺序节点的话，所有尝试获取锁的客户端都会对持有锁的子节点加监听器。当该锁被释放之后，势必会造成所有尝试获取锁的客户端来争夺锁，这样对性能不友好。使用顺序节点之后，只需要监听前一个节点就好了，对性能更友好



#### 为什么要设置对前一个节点的监听？

> Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。

同一时间段内，可能会有很多客户端同时获取锁，但只有一个可以获取成功。如果获取锁失败，则说明有其他的客户端已经成功获取锁。**获取锁失败的客户端并不会不停地循环去尝试加锁，而是在前一个节点注册一个事件监听器。**

这个事件监听器的作用是：**当前一个节点对应的客户端释放锁之后（也就是前一个节点被删除之后，监听的是删除事件），通知获取锁失败的客户端（唤醒等待的线程，Java 中的 `wait/notifyAll` ），让它尝试去获取锁，然后就成功获取锁了。**



### 会话（Session）

Session 可以看作是 ZooKeeper **服务器与客户端的之间的一个 TCP 长连接**，通过这个连接，客户端能够通过**心跳检测与服务器保持有效的会话**，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watcher 事件通知。

Session 有一个属性叫做：`sessionTimeout` ，`sessionTimeout` 代表**会话的超时时间**。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在`sessionTimeout`规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。

另外，在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 `sessionID`。由于 `sessionID`是 ZooKeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 `sessionID` 的，因此，无论是哪台服务器为客户端分配的 `sessionID`，都务必保证全局唯一。





### zookeeper羊群效应、解决

**缺点**：（**产生惊群效应**）当很多客户端在等待获取锁（等待的方式时使用watcher机制来监听distribute_lock节点的删除事件），当成功获取锁的进程释放该节点后，所有处于等待状态的客户端都会被唤醒，于是zookeeper会在短时间**发送大量子节点变更事件**给所有等待获取锁的客户端，而最终只有一个客户端获取锁。当集群规模较大时，会对zookeeper服务器的性能尝试较大影响。

**问题描述：**

   羊群效应常出现于通过zookeeper实现分布式锁的场景。

   客户端创建节点，序号最小的获取锁

   **其他客户端监控最小节点**，最小节点完成任务，发出通知，并释放

  其他客户端获取通知后，获取所有节点，序号最小的获取锁，依此类推。

**问题原因：**

但是由于通知的客户端很多，所以通知操作会造成zookeeper性能突然下降，这样会影响zookeeper的使用。

**解决方案：**

客户端创建节点，序号最小的获取锁

**客户端只监控比自己小的那个节点**

最小节点完成任务，发出通知，并释放

**客户端获取通知后，获取所有节点，如果自己的序号最小**，则获取锁，如果不是，监控比自己小的那个节点，依此类推。  

**总结：**

 使用zookeeper时，**尽量避免大量节点监控一个节点的行为**。





### ZK脑裂问题

脑裂问题：当你的 cluster 里面有两个节点，它们都知道在这个 cluster 里需要选举出一个 master。那么当它们两之间的通信完全没有问题的时候，就会达成共识，选出其中一个作为 master。但是如果它们之间的**通信出了问题，**那么两个结点都会觉得现在没有 master，所以每个都把自己选举成 master，于是 cluster 里面就会有两个 master。

ZAB为解决脑裂问题，**要求集群内的节点数量为2N+1**, 当网络分裂后，始终有一个集群的节点数量过半数，而另一个集群节点数量小于N+1（即小于半数）, 因为选主需要过半数节点同意，所以任何情况下集群中都不可能出现大于一个leader的情况。

因此，**有了过半机制**，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。


### Zk不推荐用的原因

 zookeeper不适合做注册中心主要是因为实际项目中，宕机时候会涉及到master选举，这个选举时间比较长，选举期间zookeeper是不可用的，无法提供rpc服务，所以不适合做注册中心。



### 为什么选择Zookeeper作为注册中心（dubbo）

dubbo的注册中心可以选Zookeeper，memcached，redis等。为什么选择Zookeeper，因为它的功能特性咯~

- 命名服务，服务提供者向Zookeeper指定节点写入url，完成服务发布。
- 负载均衡，注册中心的承载能力有限，而Zookeeper集群配合web应用很容易达到负载均衡。
- zk支持监听事件，特别适合发布/订阅的场景，dubbo的生产者和消费者就类似这场景。
- 数据模型简单，数据存在内存，可谓高性能



### Zookeeper 有几种部署模式？

ZooKeeper 有三种运行模式：单机模式、伪集群模式和集群模式。

- **单机模式：**一台集群上运行；这种模式一般适用于开发测试环境，一方面我们没有那么多机器资源，另外就是平时的开发调试并不需要极好的稳定性。
- **集群模式**：多台集群运行；一个 ZooKeeper 集群通常由一组机器组成，一般 3 台以上就可以组成一个可用的 ZooKeeper 集群了。组成 ZooKeeper 集群的每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都会互相保持通信。
- **伪集群模式**：一台集群启动多个 Zookeeper 实例运行。即集群的所有服务器都部署在一台机器上。当你手头上有一台比较好的机器，如果作为单机模式进行部署，就会浪费资源，这种情况下，ZooKeeper 允许你在一台机器上通过启动不同的端口来启动多个 ZooKeeper 服务实例，从而以集群的特性来对外服务。







### 集群中为什么要有主节点？

在分布式环境中，**有些业务逻辑只需要集群中的某一台机器进行执行**，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要**进行 leader 选举**。



### 集群有 3 服务器，一个节点宕机，Zk还可用吗

可以继续使用，单数服务器只要没超过一半的服务器宕机就可以继续使用。

集群规则为 2N+1 台，N >0，即最少需要 3 台。



### 两阶段提交和三阶段提交的过程、问题

#### **两阶段提交协议 2PC**

**1. 第一阶段（投票阶段）：**

（1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应；

（2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。

（3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。

**2. 第二阶段（提交执行阶段）：**

当协调者节点从所有参与者节点获得的相应消息都为”同意”时：

（1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求；

（2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源；

（3）参与者节点向协调者节点发送”完成”消息；

（4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。

**两阶段提交存在的问题：**

- **单点故障问题**，如果协调者挂了那么整个系统都处于不可用的状态了。
- **阻塞问题**，即当协调者发送 `prepare` 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。
- **数据不一致问题**，比如当第二阶段，协调者只发送了一部分的 `commit` 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题

#### **三阶段提交协议 3PC**

与两阶段提交不同的是，三阶段提交有两个改动点：

1. 引入**超时机制**。同时在协调者和参与者中都引入超时机制；
2. 在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。

也就是说，除了引入超时机制之外，3PC 把 2PC 的准备阶段再次一分为二，这样三阶段提交就有 CanCommit、PreCommit、DoCommit 三个阶段。

**1. CanCommit 阶段**

3PC 的 CanCommit 阶段其实和 2PC 的准备阶段很像。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。

（1）**事务询问**：协调者向参与者发送 CanCommit 请求。**询问是否可以执行事务提交**操作。然后开始等待参与者的响应。

（2）**响应反馈**：参与者接到 CanCommit 请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回 Yes 响应，并进入预备状态。否则反馈 No。

**2. PreCommit 阶段**

协调者根据参与者的反应情况来决定是否可以继续事务的 PreCommit 操作。根据响应情况，有以下两种可能：

假如协调者从所有的参与者获得的反馈都是 Yes 响应，那么就会执行事务的预执行。

（1）发送预提交请求：协调者向参与者发送 PreCommit 请求，并进入 Prepared 阶段。

（2）事务预提交：参与者接收到 PreCommit 请求后，会执行事务操作，并将 undo 和 redo 信息记录到事务日志中。

（3）响应反馈：如果参与者成功的执行了事务操作，则返回 ACK 响应，同时开始等待最终指令。

假如有任何一个参与者向协调者发送了 No 响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。

（1）发送中断请求：协调者向所有参与者发送 abort 请求。

（2）中断事务：参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。

**3. doCommit 阶段**

该阶段进行真正的事务提交，也可以分为以下两种情况。

3.1 执行提交

（1）发送提交请求：协调接收到参与者发送的 ACK 响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送 doCommit 请求。

（2）事务提交：参与者接收到 doCommit 请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。

（3）响应反馈：事务提交完之后，向协调者发送 ACK 响应。

（4）完成事务：协调者接收到所有参与者的 ACK 响应之后，完成事务。

3.2 中断事务

协调者没有接收到参与者发送的 ACK 响应（可能是接受者发送的不是 ACK 响应，也可能响应超时），那么就会执行中断事务。

（1）发送中断请求：协调者向所有参与者发送 abort 请求。

（2）事务回滚：参与者接收到 abort 请求之后，利用其在阶段二记录的 undo 信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。

（3）反馈结果：参与者完成事务回滚之后，向协调者发送 ACK 消息。

（4）中断事务：协调者接收到参与者反馈的 ACK 消息之后，执行事务的中断。

**三阶段提交的问题：**

**网络分区可能会带来问题**。需要四阶段解决：四阶段直接调用远程服务的数据状态，确定当前数据一致性的情况。

`3PC` 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在 `DoCommit` 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。

所以，要解决一致性问题还需要靠 `Paxos` 算法。



### `Paxos` 算法

`Paxos` 算法是基于**消息传递且具有高度容错特性的一致性算法**，是目前公认的解决分布式一致性问题最有效的算法之一，**其解决的问题就是在分布式系统中如何就某个值（决议）达成一致** 。

在 `Paxos` 中主要有三个角色，分别为 `Proposer提案者`、`Acceptor表决者`、`Learner学习者`。`Paxos` 算法和 `2PC` 一样，也有两个阶段，分别为 `Prepare` 和 `accept` 阶段。

#### prepare 阶段

- `Proposer提案者`：负责提出 `proposal`，每个提案者在提出提案时都会首先获取到一个 **具有全局唯一性的、递增的提案编号 N**，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在**第一阶段是只将提案编号发送给所有的表决者**。
- `Acceptor表决者`：每个表决者在 `accept` 某提案后，会将该提案编号 N 记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个**编号最大的提案**，其编号假设为 `maxN`。每个表决者仅会 `accept` 编号大于自己本地 `maxN` 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 `Proposer` 。

> 下面是 `prepare` 阶段的流程图，你可以对照着参考一下。

![paxos第一阶段](https://oss.javaguide.cn/p3-juejin/cd1e5f78875b4ad6b54013738f570943~tplv-k3u1fbpfcp-zoom-1.jpeg)paxos第一阶段

#### accept 阶段

当一个提案被 `Proposer` 提出后，如果 `Proposer` 收到了超过半数的 `Acceptor` 的批准（`Proposer` 本身同意），那么此时 `Proposer` 会给所有的 `Acceptor` 发送真正的提案（你可以理解为第一阶段为试探），这个时候 `Proposer` 就会发送提案的内容和提案编号。

表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 **大于等于** 已经批准过的最大提案编号，那么就 `accept` 该提案（此时执行提案内容但不提交），随后将情况返回给 `Proposer` 。如果不满足则不回应或者返回 NO 。

![paxos第二阶段1](https://oss.javaguide.cn/p3-juejin/dad7f51d58b24a72b249278502ec04bd~tplv-k3u1fbpfcp-zoom-1.jpeg)paxos第二阶段1

当 `Proposer` 收到超过半数的 `accept` ，那么它这个时候会向所有的 `acceptor` 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 `acceptor` 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要**向未批准的 `acceptor` 发送提案内容和提案编号并让它无条件执行和提交**，而对于前面已经批准过该提案的 `acceptor` 来说 **仅仅需要发送该提案的编号** ，让 `acceptor` 执行提交就行了。

![paxos第二阶段2](https://oss.javaguide.cn/p3-juejin/9359bbabb511472e8de04d0826967996~tplv-k3u1fbpfcp-zoom-1.jpeg)paxos第二阶段2

而如果 `Proposer` 如果没有收到超过半数的 `accept` 那么它将会将 **递增** 该 `Proposal` 的编号，然后 **重新进入 `Prepare` 阶段** 。

> 对于 `Learner` 来说如何去学习 `Acceptor` 批准的提案内容，这有很多方式，读者可以自己去了解一下，这里不做过多解释。

#### paxos 算法的死循环问题

提案者 P1 提出一个方案 M1，完成了 `Prepare` 阶段的工作，这个时候 `acceptor` 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 `Prepare` 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 `acceptor` 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 `Prepare` 阶段，然后 `acceptor` ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 `Prepare` 阶段。

就这样无休无止的永远提案下去，这就是 `paxos` 算法的死循环问题。解决**就允许一个能提案** 就行了，引出 ZAB









### Zookeeper 宕机如何处理？

Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；**如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader**。

**Zookeeper 集群的机制是只要超过半数的节点正常，集群就能正常提供服务**。只有在 Zookeeper 节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以：

3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票 > 1.5)

2 个节点的 cluster 就不能挂掉任何1个节点了(leader 可以得到 1 票 <= 1)



### 如果zookeeper服务注册中心失效如何保证服务的正常运行

当ZooKeeper作为服务注册中心失效时，为了保证服务的正常运行，可以考虑以下几个方法：

1. **多个注册中心备份**：使用多个独立的ZooKeeper集群作为备份，将服务同时注册到多个注册中心上。这样，当一个注册中心失效时，服务可以切换到备份注册中心，确保服务的可用性。
2. **本地缓存**：在服务启动时，将注册中心的服务信息缓存在本地。当注册中心失效时，服务可以继续使用本地缓存的服务信息，虽然无法获取新的服务实例，但至少能保证已有的服务实例继续运行。
3. **熔断机制**：引入熔断机制，当注册中心失效时，服务不会一直等待新的实例注册信息，而是根据预设的熔断策略，暂时关闭或降低对注册中心的依赖，从而保证服务的响应能力。



### Zookeeper 和 Dubbo 的关系？

Dubbo 的将注册中心进行抽象，是得它可以外接不同的存储媒介给注册中心提供服务，有 ZooKeeper，Memcached，Redis 等。

引入了 ZooKeeper 作为存储媒介，也就把 ZooKeeper 的特性引进来。首先是负载均衡，单注册中心的承载能力是有限的，在流量达到一定程度的时 候就需要分流，负载均衡就是为了分流而存在的，一个 ZooKeeper 群配合相应的 Web 应用就可以很容易达到负载均衡；资源同步，单单有负载均衡还不 够，节点之间的数据和资源需要同步，ZooKeeper 集群就天然具备有这样的功能；**命名服务，将树状结构用于维护全局的服务地址列表，服务提供者在启动 的时候，向 ZooKeeper 上的指定节点** /dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。 其他特性还有 Mast 选举，分布式锁等。

![image-20210608164032133](https://image.iamshuaidi.com/picture/image-20210608164032133.png)



### Zookeeper几个应用场景

#### 数据发布/订阅

当某些数据由几个机器共享，且这些信息经常变化数据量还小的时候，这些数据就适合存储到ZK中。

- 数据存储：将数据存储到 Zookeeper 上的一个数据节点。

- 数据获取：应用在启动初始化节点从 Zookeeper 数据节点读取数据，并在该节点上注册一个数据变更 Watcher
- 数据变更：当变更数据时会更新 Zookeeper 对应节点数据，Zookeeper会将数据变更通知发到各客户端，客户端接到通知后重新读取变更后的数据即可。

#### 统一配置管理

本质上，统一配置管理和数据发布/订阅是一样的。

分布式环境下，配置文件的同步可以由Zookeeper来实现。

- 将配置文件写入Zookeeper的一个ZNode
- 各个客户端服务监听这个ZNode
- 一旦ZNode发生改变，Zookeeper将通知各个客户端服务

![img](https://img-blog.csdnimg.cn/img_convert/588b6ad3bad42e27a8a1fbe5d300247b.png)

#### 统一集群管理

利用ZooKeeper有两个特性，就可以**实时另一种集群机器存活性监控系统**：

1. **客户端在某个节点上注册一个Watcher，那么如果该节点的子节点变化了，会通知该客户端。**
2. **创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。**

如下图所示，监控系统在/manage节点上注册一个Watcher，如果/manage子节点列表有变动，监控系统就能够实时知道集群中机器的增减情况，至于后续处理就是监控系统的业务了。

![img](https://img-blog.csdnimg.cn/img_convert/8614ca8b0cf945ad5842e0bd874e9076.png)



#### 负载均衡

![img](https://img-blog.csdnimg.cn/img_convert/b86dad72c0dc653c28c2108b9aa7ef12.png)


多个相同的jar包在不同的服务器上开启相同的服务，可以通过nginx在服务端进行负载均衡的配置。也可以通过ZooKeeper在客户端进行负载均衡配置。

1. 多个服务注册
2. 客户端获取中间件地址集合
3. 从集合中随机选一个服务执行任务

##### ZK与Nginx负载均衡区别：

- ZooKeeper不存在单点问题，zab机制保证单点故障可重新选举一个leader只负责服务的注册与发现，不负责转发，减少一次数据交换（消费方与服务方直接通信），需要自己实现相应的负载均衡算法。
- Nginx存在单点问题，单点负载高数据量大,需要通过 KeepAlived + LVS 备机实现高可用。每次负载，都充当一次中间人转发角色，增加网络负载量（消费方与服务方间接通信），自带负载均衡算法。

#### 命名服务

![img](https://img-blog.csdnimg.cn/img_convert/66ed838e9dc53afbdf0c9237e68580be.png)


命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局唯一的路径，这个路径就可以作为一个名字，指向集群中某个具体的服务器，提供的服务的地址，或者一个远程的对象等等。

阿里巴巴集团开源的分布式服务框架 Dubbo 中使用 ZooKeeper 来作为其命名服务，维护全局的服务地址列表。在 Dubbo 的实现中：

- 服务提供者在启动的时候，向 ZooKeeper 上的指定节点/dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。

- 服务消费者启动的时候，订阅/dubbo/${serviceName} /consumers 目录下写入自己的 URL 地址。

注意：所有向 ZooKeeper 上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。

另外，Dubbo 还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName} 目录下所有提供者和消费者的信息。

#### **分布式锁**

通过创建唯一节点获得分布式锁，当获得锁的一方执行完相关代码或者是挂掉之后就释放锁。分布式锁的实现也需要用到 **Watcher 机制**



### CAP & BASE理论

#### CAP理论

**CAP** 也就是 **Consistency（一致性）**、**Availability（可用性）**、**Partition Tolerance（分区容错性）** 这三个单词首字母组合。

CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：

- **一致性（Consistency）** : 所有节点访问同一份最新的数据副本
- **可用性（Availability）**: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。
- **分区容错性（Partition Tolerance）** : 分布式系统出现网络分区的时候，仍然能够对外提供服务。

`Eureka` 的处理方式，它保证了AP（可用性），后者就 `ZooKeeper` 的处理方式，它保证了CP（数据一致性）。

CAP理论中，`P`（分区容忍性）是必然要满足的，因为毕竟是分布式，不能把所有的应用全放到一个服务器里面，这样服务器是吃不消的。所以，**只能从AP（可用性）和CP（一致性）中找平衡**。

#### BASE理论

怎么个平衡法呢？在这种环境下出现了BASE理论：即使无法做到强一致性，但分布式系统可以根据自己的业务特点，采用适当的方式来使系统达到**最终的一致性**。BASE理论由：Basically Avaliable 基本可用、Soft state 软状态、Eventually consistent 最终一致性组成。

- **基本可用**(Basically Available)：基本可用是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。例如，电商大促时，为了应对访问量激增，部分用户可能会被引导到降级页面，服务层在该页面只提供降级服务。
- **软状态(**Soft State)： 软状态是指允许系统存在中间状态，而该中间状态不会影响系统整体可用性。分布式存储中一般一份数据至少会有多个副本，允许不同节点间副本同步的延时就是软状态的体现。
- **最终一致性**(Eventual Consistency)： 最终一致性是指系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。弱一致性和强一致性相反，最终一致性是弱一致性的一种特殊情况。

一句话概括就是：平时系统要求是基本可用，运行有可容忍的延迟状态，但是，无论如何经过一段时间的延迟后系统最终必须达成数据是一致的。

- ACID 是传统数据库常用的设计理念，追求强一致性模型。BASE 支持的是大型分布式系统，通过牺牲强一致性获得高可用性。


其实可能发现不管是CAP理论，还是BASE理论，他们都是理论，这些理论是需要算法来实现的，这些算法有2PC、3PC、Paxos、Raft、ZAB，它们所解决的问题全部都是：**在分布式环境下，怎么让系统尽可能的高可用，而且数据能最终能达到一致。**







# RPC===================

[全图文分析：如何利用Google的protobuf，来思考、设计、实现自己的RPC框架 (qq.com)](https://mp.weixin.qq.com/s/HJqh0JHIqhabRd5zHmKAiw)

## 1、RPC 是什么?为什么要 RPC ？RPC 的用途？

**RPC** 即远程过程调用，通过名字我们就能看出 RPC 关注的是远程调用而非本地调用。

**RPC 本质上不算是协议，而是一种调用方式**，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，**不一定非得基于 TCP 协议，可改用 UDP 或者 HTTP**。

**为什么要用RPC ？**

​    在微服务架构中，RPC框架充当了连接不同微服务之间通信的桥梁，实现了服务之间的相互调用。

- 1、RPC框架一般使用长链接，不必每次通信都要3次握手，减少网络开销。

- 2、RPC框架一般都有注册中心，有丰富的监控管理。发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作协议私密，安全性较高

- 3、RPC 协议更简单内容更小，效率更高，服务化架构、服务化治理，RPC框架是一个强力的支撑。

  > - http接口是在接口不多、系统与系统交互较少的情况下，解决信息孤岛初期常使用的一种通信手段；
  > - socket只是一个简单的网络通信方式，只是创建通信双方的通信通道，而要实现rpc的功能，还需要对其进行封装，以实现更多的功能。
  > - RPC一般配合netty框架、spring自定义注解来编写轻量级框架，其实netty内部是封装了socket的，较新的jdk的IO一般是NIO，即非阻塞IO，在高并发网站中，RPC的优势会很明显

**RPC 的用途？** 

RPC框架是一种用于不同计算机或进程间通信的技术，允许像调用本地方法一样调用远程方法。在大型互联网公司中，**RPC框架用于构建分布式系统，将各个服务连接在一起，实现服务间的通信与协作**。

通过 RPC 可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。并且！我们不需要了解底层网络编程的具体细节。举个例子：两个不同的服务 A、B 部署在两台不同的机器上，服务 A 如果想要调用服务 B 中的某个方法的话就可以通过 RPC 来做。

**TCP、HTTP、RPC的区别**

**TCP 是传输层的协议** ，而基于 TCP 造出来的 HTTP 和各类 RPC 协议，它们都只是定义了不同消息格式的 **应用层协议** 而已。

**HTTP**协议又叫做 **超文本传输协议** 。在浏览器上敲个网址就能访问网页，用到的就是 HTTP 协议。HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。

**RPC**又叫做 **远程过程调用**，它本身并不是一个具体的协议，而是一种 **调用方式** ，可以**屏蔽掉一些网络细节**。

## 2、RPC 的原理是什么?

整个 RPC 的 核心功能以下 **5 个部分实现**的：

1. **客户端（服务消费端）**：服务调用方。
2. **客户端 Stub（桩）**：代理类，存放服务端地址信息，将客户端的请求参数数据信息打包成网络消息，再通过网络传输发送给服务端。
3. **网络传输**：通道channel，网络底层传输，可以是 TCP 或 HTTP，UDP。
4. **服务端 Stub（桩）**：不是代理类。接收客户端发送过来的请求消息并进行解包，然后再调用本地服务进行处理。
5. **服务端（服务提供端）**：服务的真正提供者。

具体原理图如下，后面我会串起来将整个 RPC 的过程给大家说一下。

![RPC原理图](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/18-12-6/37345851.jpg)RPC原理图

###   RPC 的原理：

1. 服务消费端（client）以本地调用的方式调用远程服务；
2. 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：`RpcRequest`；
3. 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端；
4. 服务端 Stub（桩）收到消息将消息反序列化为 Java 对象: `RpcRequest`；
5. 服务端 Stub（桩）根据`RpcRequest`中的类、方法、方法参数等信息调用本地的方法；
6. 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：`RpcResponse`（序列化）发送至消费方；
7. 客户端 Stub（client stub）接收到消息并将消息反序列化为 Java 对象:`RpcResponse` ，这样也就得到了最终结果。over!

### RPC框架：

1. **注册中心** ：**可靠的寻址方式（主要是提供服务的发现），比如可以使用Zookeeper来注册服务。**注册中心负责服务地址的注册与查找，相当于目录服务。服务端启动的时候将服务名称及其对应的地址(ip+port)注册到注册中心，服务消费端根据**服务名称**找到对应的服务地址。有了服务地址之后，服务消费端就可以通过网络请求服务端了。

2. **网络传输** ：既然要调用远程的方法就要发请求，请求中至少要包含你调用的类名、方法名以及相关参数吧！自定义的Channel通道。选择 TCP 协议、UDP 协议、HTTP 协议。需要有非常高效的网络通信，比如一般选择Muduo作为网络通信框架

3. **通信协议：**RPC框架需要定义一种通信协议，用于客户端和服务器之间的通信。这个协议可以是自定义的二进制格式，也可以是像JSON-RPC、gRPC、Thrift等标准化的协议。

4. **序列化** ：在远程过程调用中，参数和返回值需要在网络上进行传输。因此，数据必须被序列化为通信协议所支持的格式，然后在接收端进行反序列化。这使得客户端和服务器能够将数据转换为字节流以进行传输，并将其还原为数据结构以供处理。比较常用的有 hession2、kyro、protostuff、谷歌的Protobuf序列化框架。

5. **动态代理** ： 客户端需要通过代理来调用远程服务器上的方法。代理隐藏了底层的网络通信细节，使得客户端能够像调用本地方法一样调用远程方法，当你调用远程方法的时候，实际会通过代理对象来传输网络请求。

6. **负载均衡** ：当多个服务提供相同功能时，负载均衡是确保请求在它们之间平衡分配的机制。这有助于提高系统的可伸缩性和性能。

   我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，**当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器**就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。**负载均衡是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题**。

7. **会话功能：**如果是带会话（状态）的RPC调用，还需要有会话和状态保持的功能；

#####  如何实现一个RPC框架？

1. 服务设计：客户端、服务端、**ZK注册中心**，获取订单接口。
   怎么知道服务端的信息？ 如何去调用的？
2. 先启动服务端： **将接口信息注册至ZK**。（ServicePushManager.registerIntoZK方法）
3. 启动客户端： **从ZK拉取服务端接口信息**。（ServicePullManager.pullServiceFromZK方法）
4. Rpc调用处理流程：
   **客户端**->通过**动态代理**调用服务端接口（ProxyHelper.doIntercept）-> 选取**不同的调用策略**-> **异步方式**调用（通过MAP存储记录channel，rpcRequestPool.fetchResponse获取结果）-> **服务端**（根据请求信息调用对应的接口， RpcRequestHandler.channelRead0）-> **客户端监听接收结果**(RpcResponseHandler.channelRead0)-> 关闭连接（RpcRequestManager.destroyChannelHolder关闭连接）





### Protobuf序列化原理

它的序列化原理基于二进制编码，通过定义消息的结构和字段规则，可以将结构化的数据序列化成紧凑的二进制数据，以及将二进制数据反序列化成结构化的数据。

以下是 Protobuf 序列化的主要原理：

1. **定义消息结构：** 首先，您需要使用 Protobuf 的语言定义（.proto 文件）来描述您希望序列化的消息的结构。这个定义包含消息的字段和其类型，例如整数、字符串、嵌套消息等。
2. **编码规则：** Protobuf 使用一种称为 "varint" 编码的方式来序列化整数、枚举和一些其他数据类型。对于重复的数据（如数组），它使用 "packed" 编码来更有效地存储多个值。
3. **序列化过程：** 在将数据序列化为二进制时，Protobuf 将按照您在 .proto 文件中定义的字段顺序，将每个字段的标识号与字段值编码成二进制格式。对于嵌套的消息，它们会递归地序列化。
4. **反序列化过程：** 反序列化过程与序列化过程相反。根据字段的标识号和类型，Protobuf 解析二进制数据并将其还原为原始的结构化数据。在反序列化时，Protobuf 可以根据字段的类型自动解析并还原数据。
5. **标签号：** 每个字段在 .proto 文件中都有一个唯一的标签号，用于标识该字段。这个标签号在编码时被使用，以确保在反序列化时能够正确地识别和解析每个字段。
6. **字段类型和默认值：** 每个字段都有一个类型和一个可能的默认值。在序列化过程中，如果字段的值等于默认值，那么可以选择不在序列化数据中包含该字段，从而节省空间。
7. **嵌套消息：** 如果消息中包含嵌套的消息类型，那么这些嵌套消息也会按照相同的方式被序列化。在编码中，嵌套消息的标识号将会反映其在父消息中的嵌套结构。

总的来说，Protobuf 的序列化原理基于字段定义、编码规则以及标签号等机制，它能够高效地将结构化数据转换为紧凑的二进制格式，适用于网络传输、持久化存储等多种场景。











### protobuf底层实现

1. **编码和解码：** protobuf使用Varint编码来压缩整数值，从而减少数据占用空间。对于其他数据类型，如浮点数和字符串，它们会被按照一定的规则编码成二进制格式。这些编码和解码操作是protobuf的核心实现，确保了数据在传输和存储过程中的高效性。
2. **数据结构定义：** 在.proto文件中，您可以定义数据的结构、字段名称和数据类型。这些定义被用于生成编码和解码时所需的代码。定义还包括字段的标识号，这些标识号在编码过程中被用来识别字段。
3. **代码生成：** protobuf提供了一个编译器（如`protoc`），它会根据您在.proto文件中定义的数据结构，生成相应语言（如C++、Java、Python等）的代码。这些生成的代码实现了消息的序列化、反序列化以及对消息字段的访问。
4. **通信协议：** protobuf不仅可以用于本地数据存储，还可以用于网络通信。它定义了消息的格式和规则，使不同系统之间能够相互通信并正确解析数据。通常，网络通信中使用的protobuf消息会被封装在适当的通信协议中，例如HTTP、gRPC等。































### RPC 使用场景：

（大型的网站，内部子系统较多、接口非常多的情况下适合使用 RPC）：

- **长链接。**不必每次通信都要像 HTTP 一样去 3 次握手，减少了网络开销。
- **注册发布机制。**RPC 框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。
- **安全性，**没有暴露资源操作。
- **微服务支持。**就是最近流行的服务化架构、服务化治理，RPC 框架是一个强力的支撑。

### 保证数据安全性：

保证数据传输过程中的安全性： 为了确保数据传输过程中的安全性，可以采用以下措施：

1. 使用安全的通信协议，如HTTPS，以加密通信内容。
2. 使用身份认证和授权机制，如OAuth，确保只有合法的客户端可以调用服务端的方法。
3. 使用防火墙和网络隔离来限制非授权访问。
4. 对重要的数据进行签名或哈希，以防止数据被篡改。

### 处理网络拥塞、超时等异常情况？

 网络拥塞和超时等异常情况可能会导致RPC调用失败或延迟，为了保证系统的稳定性和可靠性，可以采取以下措施：

1. 设置合理的超时时间：在RPC调用中，设置适当的超时时间，确保在一定时间内可以得到响应，避免无限等待。
2. 使用重试机制：在遇到网络拥塞或调用失败时，可以尝试重新发送请求，多次尝试后仍不能成功则返回错误。
3. 限流和熔断：在系统压力过大时，可以通过限流和熔断机制来保护系统免受过载的影响。
4. 异常处理：合理地处理RPC调用可能抛出的异常，避免程序崩溃或数据丢失。
5. 日志和监控：记录RPC调用的日志和性能指标，以便及时发现和解决问题。

### 网络传输：（基于 TCP 协议、HTTP协议对比）

**基于 TCP 协议的 RPC 调用**

由服务的调用方与服务的提供方建立 Socket 连接，并由服务的调用方通过 Socket 将需要调用的接口名称、方法名称和参数序列化后传递给服务的提供方，服务的提供方反序列化后再利用反射调用相关的方法。最后将结果返回给服务的调用方，整个基于 TCP 协议的 RPC 调用大致如此。

**基于 HTTP 协议的 RPC 调用**

该方法更像是访问网页一样，其大致流程为：由服务的调用者向服务的提供者发送请求，这种请求的方式可能是 GET、POST、PUT、DELETE 等中的一种，服务的提供者可能会根据不同的请求方式做出不同的处理，或者某个方法只允许某种请求方式。

而调用的具体方法则是**根据 URL 进行方法调用**，而方法所需要的参数可能是**对服务调用方传输过去的 XML 数据或者 JSON 数据解析后的结果**，最后**返回 JOSN 或者 XML 的数据结果**。就像做 Web 项目一样。

**两种方式对比**

**基于 TCP 的协议实现的 RPC 调用**，由于 TCP 协议处于协议栈的下层，能够更加**灵活地对协议字段进行定制**，减少网络开销，提高性能，实现更大的吞吐量和并发数。

但是需要更多关注底层复杂的细节，实现的代价更高。同时对不同平台，如安卓，iOS 等，需要重新开发出不同的工具包来进行请求发送和相应解析，工作量大，难以快速响应和满足用户需求。

**基于 HTTP 协议实现的 RPC** 则可以使用 **JSON 和 XML** 格式的请求或响应数据。

而 JSON 和 XML 作为通用的格式标准（使用 HTTP 协议也需要序列化和反序列化，成熟的 Web 程序已经做好了序列化内容），开源的解析工具已经相当成熟，在其上进行二次开发会非常便捷和简单。

但是由于 **HTTP 协议是上层协议**，发送包含同等内容的信息，使用 HTTP 协议传输所占用的字节数会比使用 TCP 协议传输所占用的**字节数更高**。

因此在同等网络下，通过 HTTP 协议传输相同内容，效率会比基于 TCP 协议的数据效率要低，信息传输所占用的时间也会更长，当然压缩数据，能够缩小这一差距。

### 架构：

![图片](https://mmbiz.qpic.cn/mmbiz_png/WC13ibsIvG3bguLOrxcExA8cyW8hkzf1z6sIicibbM9la01YgyLhkhQHwia5WcySJgCyI87Qun8h9QykkhQ1rJRJkg/640?wx_fmt=png&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://pic3.zhimg.com/80/v2-213483fb3001c1d0883dcdce51394a1a_1440w.webp)

###  调用流程：

![img](https://pic3.zhimg.com/80/v2-6b87c85f47ab2cc8d24228df59269e7e_1440w.webp)

#### **具体步骤：**

1. 服务消费者（client客户端）通过本地调用的方式调用服务。
2. 客户端存根（client stub）接收到请求后负责将方法、入参等信息序列化（组装）成能够进行网络传输的消息
   体。
3. 客户端存根（client stub）找到远程的服务地址，并且将消息通过网络发送给服务端。
4. 服务端存根（server stub）收到消息后进行解码（反序列化操作）。
5. 服务端存根（server stub）根据解码结果调用本地的服务进行相关处理。
6. 本地服务执行具体业务逻辑并将处理结果返回给服务端存根（server stub）。
7. 服务端存根（server stub）将返回结果重新打包成消息（序列化）并通过网络发送至消费方。
8. 客户端存根（client stub）接收到消息，并进行解码（反序列化）。
9. 服务消费方得到最终结果。

#### **Wireshark 抓包分析过程**

客户端去往服务端：

- **客户端 IP：**172.171.4.176
- **服务端 IP：**172.171.5.95


通信使用 HTTP 协议，XML 文件传输格式。传输的字段包括：方法名 methodName，两个参数 2，3。

*图 5：Request 抓包*

服务端返回结果，字段返回值 Value，结果是 5：

*图 6：Response 抓包*

在这两次网络传输中使用了 HTTP 协议，建立 HTTP 协议之间有 TCP 三次握手，断开 HTTP 协议时有 TCP 四次挥手。

#### **涉及到的技术**：

1. **动态代理**
   生成Client Stub（客户端存根）和Server Stub（服务端存根）的时候需要用到java**动态代理技术**。

   **JDK动态代理实现原理:**

   ![img](https://pic1.zhimg.com/80/v2-503c48c07866e38018362315d8f74808_1440w.webp)

   

2. **序列化**：所有的数据都将会被转化为字节进行传送，所以为了能够使参数对象在网络中进行传输，需要对这些参数进行序列化和反序列化操作。 

   序列化：把对象转换为字节序列的过程称为对象的序列化，也就是编码的过程。

   反序列化：把字节序列恢复为对象的过程称为对象的反序列化，也就是解码的过程。 目前比较高效的开源序列化框架：如Kryo、FastJson和Protobuf等。

3. NIO通信
   出于并发性能的考虑，传统的阻塞式 IO 显然不太合适，因此我们需要异步的 IO，即 NIO。可以选择Muduo来解决NIO数据传输的问题。

4. **服务注册中心**
   通过注册中心，让客户端**连接调用服务端所发布的服务**。主流的注册中心组件：Redis、**Zookeeper**、Consul、Etcd。一般使用**ZooKeeper提供服务注册与发现功能**，解决单点故障以及分布式部署的问题(注册中心)

5. **负载均衡**
   在**高并发**的场景下，需要**多个节点或集群**来提升整体吞吐能力。

6. 健康检查
   健康检查包括，**客户端心跳**和服务端主动探测两种方式。

## 3、有哪些常见的 RPC 框架？

gRPC和Thrift都是高性能的RPC框架，但gRPC使用HTTP/2和Protocol Buffers来实现通信，而Thrift则使用自己的二进制协议和IDL。

 RPC 框架指的是可以让客户端直接调用服务端方法，就像调用本地方法一样简单的框架，比如我下面介绍的 Dubbo、Motan、gRPC 这些。 如果需要和 HTTP 协议打交道，解析和封装 HTTP 请求和响应。这类框架并不能算是“RPC 框架”，比如 Feign。

**Dubbo：**Apache Dubbo 是一款微服务框架，为大规模微服务实践提供高性能 RPC 通信、流量治理、可观测性等解决方案， 涵盖 Java、Golang 等多种语言 SDK 实现。Dubbo 提供了从服务定义、服务发现、服务通信到流量管控等几乎所有的服务治理能力，支持 Triple 协议（基于 HTTP/2 之上定义的下一代 RPC 通信协议）、应用级服务发现、Dubbo Mesh （Dubbo3 赋予了很多云原生友好的新特性）等特性。

**gRPC：**gRPC 是 Google 开源的一个高性能、通用的开源 RPC 框架。其由主要面向移动应用开发并基于 HTTP/2 协议标准而设计（支持双向流、消息头压缩等功能，更加节省带宽），基于 ProtoBuf 序列化协议开发，并且支持众多开发语言。

**何谓 ProtoBuf？** ProtoBuf是一种更加灵活、高效的数据格式，可用于通讯协议、数据存储等领域，基本支持所有主流编程语言且与平台无关。不过，通过 ProtoBuf 定义接口和数据类型还挺繁琐的，这是一个小问题。

**Thrift：**Apache Thrift 是 Facebook 开源的跨语言的 RPC 通信框架，目前已经捐献给 Apache 基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于 thrift 研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。

## 4、HTTP 和 RPC 有什么区别？

RPC 是一种**设计**，就是为了解决**不同服务之间的调用问题**，完整的 RPC 实现一般会包含有 **传输协议** 和 **序列化协议** 这两个。

而 HTTP 是一种传输协议，RPC 框架完全可以使用 HTTP 作为传输协议，也可以直接使用 TCP，使用不同的协议一般也是为了适应不同的场景。

**服务发现：**

首先要向某个服务器发起请求，先建立连接，而建立连接的前提是，得知道 **IP 地址和端口** 。找到服务对应的 IP 端口的过程，就是 **服务发现**。

在 **HTTP** 中，知道服务的域名，可以通过 **DNS 服务** 去解析得到它背后的 IP 地址，默认 **80 端口**。

而 **RPC** ，有专门的中间服务去保存服务名和 IP 信息，比如 **Consul、Etcd、Nacos、ZooKeeper，甚至是 Redis**。想要访问某个服务，就去这些中间服务去获得 IP 和端口信息。由于 DNS 也是**服务发现**的一种，所以也有基于 DNS 去做服务发现的组件，比如 **CoreDNS**。

**传输效率**：

- TCP，通常自定义上层协议，可以让请求报文体积更小 
- HTTP：如果是基于HTTP 1.1 的协议，请求中会包含很多无用的内容 

**性能消耗**：主要在于序列化和反序列化的耗时

- TCP，可以基于各种序列化框架进行，效率比较高 
- HTTP，大部分是通过 json 来实现的，字节大小和序列化耗时都要更消耗性能

**底层连接形式：**

主流的 **HTTP1.1** 协议：在建立底层 TCP 连接之后会一直保持这个连接（**keep alive**），之后的请求和响应都会复用这条连接。

**RPC** 协议：跟 HTTP 类似，通过建立 TCP 长链接进行数据交互，但不同的地方在于，RPC 协议一般还会再建个 **连接池**，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，用完放回去，下次再复用，可以说非常环保。

**传输的内容：**

基于 TCP 传输的消息是 **消息头 Header （消息体长度）和消息体 Body（传输内容）**

**Body** 放需要传输的内容（二进制 01 串）。TCP 传字符串和数字可以编码再分别变成 01 串，二进制。结构体转为二进制数组的过程就叫 **序列化** ，反过来将二进制数组复原成结构体的过程叫 **反序列化**。

**主流的 HTTP1.1：**超文本协议，支持音频视频，但 HTTP 设计 初是用于做网页文本展示的，所以它传的内容以字符串为主。Header 和 Body 都是如此。在 Body 这块，它使用 **JSON** 来 **序列化** 结构体数据。

**RPC：**因为它定制化程度更高，可以采用**体积更小的 Protobuf** 或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。

## 5、TCP粘包是什么？什么解决？

TCP协议粘包拆包问题是因为TCP协议数据传输是基于字节流的，它不包含消息、数据包等概念，需要应用层协议自己设计消息的边界，即消息帧（Message Framing）。如果应用层协议没有使用基于长度或者基于终结符息边界等方式进行处理，则会导致多个消息的粘包和拆包

**解决方案：**

对于粘包和拆包问题，常见的解决方案有四种：

- 发送端将每个包都封装成固定的长度，比如100字节大小。如果不足100字节可通过补0或空等进行填充到指定长度；
- 以特殊的字符结尾，比如以“\n”结尾，这样我们就知道数据的具体边界了，从而避免了粘包问题（**推荐方案**）。如果发生拆包需等待多个包发送过来之后再找到其中的\r\n进行合并；例如，FTP协议；
- 将消息分为头部和消息体，头部中保存整个消息的长度，只有读取到足够长度的消息之后才算是读到了一个完整的消息；
- 在 TCP 协议的基础上封装一层自定义数据协议，在自定义数据协议中，包含数据头（存储数据的大小）和 数据的具体内容，这样服务端得到数据之后，通过解析数据头就可以知道数据的具体长度了，也就没有粘包的问题了。

### 为什么UDP没有粘包？

粘包拆包问题在数据链路层、网络层以及传输层都有可能发生。日常的网络应用开发大都在传输层进行，由于UDP有消息保护边界，不会发生粘包拆包问题，因此粘包拆包问题只发生在TCP协议中。

粘包拆包发生场景

因为TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小。

如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。

如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。

- 正常的理想情况，两个包恰好满足TCP缓冲区的大小或达到TCP等待时长，分别发送两个包；
- 粘包：两个包较小，间隔时间短，发生粘包，合并成一个包发送；
- 拆包：一个包过大，超过缓存区大小，拆分成两个或多个包发送；
- 拆包和粘包：Packet1过大，进行了拆包处理，而拆出去的一部分又与Packet2进行粘包处理。

### json怎么处理粘包的

在网络通信中，粘包是指发送方发送的数据包与接收方接收的数据包之间没有明确的界限，导致多个数据包粘在一起，形成一个大的数据块，从而使接收方无法正确解析和处理。在处理JSON粘包时，可以采取以下几种方法：

1. 分隔符：在发送JSON数据包之前，可以在数据包之间添加一个特定的分隔符，例如换行符或特殊字符。接收方在接收数据时根据分隔符来切割数据，将多个数据包拆分成独立的JSON数据。
2. 固定长度：可以规定每个JSON数据包的固定长度，发送方在发送数据时按照固定长度进行切割，接收方根据长度来分解数据包。这种方法适用于JSON数据包大小相对固定的情况。
3. 消息头：在发送JSON数据包之前，可以添加一个消息头，包含数据包的长度信息。接收方首先读取消息头，获取数据包长度，然后根据长度来截取数据包。
4. 序列化与反序列化：在发送方，可以将JSON对象序列化为字符串，在接收方，将接收到的数据反序列化为JSON对象。这种方法可以保证数据包之间不会粘连，因为JSON序列化后是一个完整的字符串。
5. 使用专用库：许多编程语言都提供了处理网络通信和数据粘包的专用库。使用这些库可以更方便地处理JSON粘包问题，例如Python中的socket、Node.js中的net等。

无论采取哪种方法，都需要在发送方和接收方保持一致，以确保正确地解析和处理JSON数据包。处理粘包是网络通信中常见的问题，针对具体的应用场景和数据传输需求选择合适的方法很重要。

## 6、 为什么要自定义协议格式？

**TinyPB** 协议是基于 **protobuf** 的一种**自定义协议**，主要是加了一些必要的字段如 错误码、RPC 方法名、起始结束标志位等

既然**用了 Prootobuf 做序列化**，为什么不直接把序列化后的结果直接发送，而要在上面在**自定义一些字段**？

**为了方便分割请求：**因为 protobuf 后的结果是一串无意义的字节流，你无法区分哪里是开始或者结束。 比如说把两个 Message 对象序列化后的结果排在一起，你甚至无法分开这两个请求。在 TCP 传输是按照字节流传输，没有包的概念，因此应用层就更无法去区分了。

**为了定位：**加上 MsgID 等信息，能帮助我们匹配一次 RPC 的请求和响应，不会串包

**错误提升：**加上错误信息，能很容易知道 RPC 失败的原因，方便问题定位



**TinyRPC** 框架目前支持两类协议：

纯 **HTTP** 协议: TinyRPC 实现了简单的很基本的 HTTP(1.1) 协议的编、解码，完全可以使用 HTTP 协议搭建一个 RPC 服务。

TinyPB 协议: 一种基于 **Protobuf** 的自定义协议，属于二进制协议。

### TinyPB 协议报文格式分解

1. **err_code**: err_code 是框架级别的错误码，即代表调用 RPC 过程中发生的错误，如对端关闭、调用超时等。err_code 为0 代表此次 RPC 调用正常，即正常发送数据且接收到回包。非 0 值代表调用失败，此时会设置 err_info 为详细的错误信息。

   **service_full_name** : 是指的调用的完整方法名。即 servicename.methodname。一般来说，一个 **TinyPB**协议的**TinyRPC** 服务需要注册至少一个 **Service** (这里的 Service 指的继承了google::protobuf::Service 的类)，而一个 Service 下包含多个方法。

   **pk_len**: pk_len 代表整个协议包的长度，单位是1字节，且包括 **[strat]** 字符 和 **[end]** 字符。

   **TinyPb** 协议报文中包含了多个 len 字段，这主要是为了用空间换时间，接收方在提前知道长度的情况下，更方便解码各个字段，从而提升了 decode 效率。

   另外，**TinyPb** 协议里面所有的 int 类型的字段在编码时都会先转为**网络字节序**！

## 7、涉及技术：

### 7.1、  序列化技术

#### **Protobuf介绍、应用场景、序列化、序列化协议需要考虑哪些**

Protobuf (Protocol Buffers) 是谷歌开发的一款无关平台，**无关语言，可扩展，轻量级高效的序列化结构**的数据格式，用于将**自定义数据结构序列化成字节流**，和将**字节流反序列化为数据结构**。所以很适合做数据存储和为不同语言，不同应用之间互相通信的数据交换格式，**只要实现相同的协议格式，即后缀为proto文件**被编译成不同的语言版本，加入各自的项目中，这样不同的语言可以解析其它语言通过Protobuf序列化的数据。目前官方提供c++，java，go等语言支持。

**使用场景**：**在某些场景对消息大小很敏感，或者传输的数据量不大，比如说APP登录场景**，那么可以考虑使用Protobuf

- 序列化：将数据结构或者对象转化成二进制串的过程。
- 反序列化：将序列化过程中所产生的二进制串转换成数据结构或对象的过程。

**选择序列化协议需要考虑哪些：**

1. 序列化之后的**数据流的大小（占用网络带宽**），对于高并发的场景下这点很重要。
2. 序列化和反序列化的**性能（占用CPU和内存资源）**。
3. 是否支持多语言。

##### **json、XML、Protobuf对比**

1. JSON （JavaScript Object Notation）：一般用于WEB项目中，因为**浏览器对JSON格式的数据支持**非常好，大部分编程语言有很多**内建函数**支持，而且JSON几乎支持所有编程语言。
2. XML：XML在WebService中的应用比较多，相比于JSON，它的**数据更加冗余**，因为需要成对的闭合标签，而JSON使用了**键值对的方式，不仅压缩了一定的数据空间，同时也有更好的可读性**。
3. Protobuf：谷歌公司新开发的一种数据格式，适合**高性能，对响应速度有要求**的数据传输场景。因为Protobuf是**二进制数据格式**，需要**编码和解码**。**数据本身不具有可读性**，因此只能反序列化得到可读数据。

##### **Protobuf的优点和缺点**

**优点：**

- 序列化后体积比JSON和XML小，适合网络传输。
- 序列化反序列化速度快，比JSON的处理速度快。
- 消息格式升级和兼容性还不错。

1. Protobuf序列化**速度快**，序列化后的**体积比xml更小**，**传输更快**。使用相对也简单，因为Proto编译器能自己序列化和反序列化。
2. 可以**定义自己的数据结构**，然后使用**代码生成器**去生成的代码来读写这个数据结构，甚至可以在不用重新部署的情况下来更新这个数据结构，只需要使用Protobuf对数据结构进行一次描述，就可以利用不同的语言或者从不同的数据流对你的结构化数据轻松的读写。
3. **语义比xml更加清晰，无需类似xml解析器的东西**（因为**Protobuf编译器**会将**.proto文件编译成对应的数据访问**用以对Protobuf数据进行序列化和反序列化操作）。
4. 多个平台只需要维护**一套.proto对象协议文件**。
5. **可扩展性好、更容易引入新的字段**，并且不需要检查数据的中间服务器可以简单地解析并传递数据，而无需了解所有字段。
6. **数据格式更加具有自我描述性**，可以用各种语言来处理(C++, Java 等各种语言)。

**缺点：**

1. **可读性差，缺乏自描述**

   XML，JSON是自描述的，而ProtoBuf则不是。

   ProtoBuf是二进制协议，编码后的数据可读性差，如果没有idl文件，就无法理解二进制数据流，对调试不友好。

##### **不直接使用XML、Json序列化好处**

**同XML相比**，Protobuf的优势在于**高性能**，它以**高效的二进制存储方式**比XML小3到10倍，快20到100倍，原因在于：

1. ProtoBuf序列化后所生成的**二进制消息非常紧凑**。
2. ProtoBuf**封解包过程**非常简单。

protobuf对传输的数据采取一种最简单的key-value形式的存储方式，这钟存储方式极大的节省了空间。除此之外protobuf还采取了varint(变长编码)形式来压缩数据，对体积较小的字段分配较少的空间，由此使得压缩后的文件非常“紧凑”。

**JSON序列化**：一般在HTTP协议的RPC框架通信中，会选择JSON方式。**JSON具有较好的扩展性、可读性和通用性**。**但JSON序列化占用空间开销较大,**没有JAVA的强类型区分，需要通过反射解决，**解析效率和压缩率都较差**。如果对**并发和性能要求较高**，或者是传输数据量较大的场景，不建议采用JSON序列化方式。

##### **protobuf本身用来解决什么问题**、如何序列化？

**数据序列化：** Protobuf 可以将结构化数据（如对象、消息等）序列化成二进制格式，从而便于在不同平台和编程语言之间进行数据交换和传输。这种高效的二进制序列化可以减少数据的大小和传输时间，相比于使用文本格式的数据交换，效率更高。

**性能优化：** 由于 Protobuf 使用二进制格式进行数据序列化，相比于使用文本格式（如 JSON 或 XML），可以减少数据的大小和传输时间，从而提高系统的性能和效率。

**自定义二进制协议** Prootobuf 来实现序列化：

##### xml、json、protobuf对比（处理因素）、常用序列化技术

序列化的处理要素

1. **解析效率：**序列化协议**应该首要考虑的因素，像**xml/json解析起来比较耗时**，需要解析doom树，**二进制自定义协议解析起来效率要快很多。
2. **空间压缩率：**同样一个对象，**xml/json传输起来**有大量的标签**冗余信息**，信息有效性低，**二进制自定义协议**占用的**空间**相对来说会**小**很多。
3. **扩展性与兼容性：**是否能够利于**信息的扩展**，并且**增加字段后**旧版客户端是否需要强制升级，这都是需要考虑的问题，在自定义二进制协议时候，要做好充分考虑设计。
4. **可读性与可调试性**：xml/json的可读性会比二进制协议好很多，并且通过网络抓包是可以直接读取，二进制则需要反序列化才能查看其内容。
5. 跨语言：有些序列化协议是与开发语言紧密相关的，例如dubbo的Hessian序列化协议就只能支持**Java的RPC调用**。
6. **通用性：xml/json非常通用**，都有很好的第三方解析库，各个语言解析起来都十分方便，二进制数据的处理方面也有Protobuf和Hessian等插件，在做设计的时候尽量做到较好的通用性。

**常用序列化技术**

- 1、**JDK原生序列化**，通过实现Serializable接口。通过ObjectOutPutSream和ObjectInputStream对象进行序列化及反序列化.
- 2、**JSON序列化**：一般在HTTP协议的RPC框架通信中，会选择JSON方式。**JSON具有较好的扩展性、可读性和通用性**。**但JSON序列化占用空间开销较大,**没有JAVA的强类型区分，需要通过反射解决，**解析效率和压缩率都较差**。如果对**并发和性能要求较高**，或者是传输数据量较大的场景，不建议采用JSON序列化方式。
- 3、Hessian2序列化。Hessian 是一个动态类型，**二进制序列化**，并且支持跨语言特性的序列化框架。Hessian 性能上要比 JDK、JSON 序列化高效很多，并且**生成的字节数也更小**。有非常好的兼容性和稳定性，所以 Hessian 更加适合作为 RPC 框架远程通信的序列化协议。

**Hessian自身也存在一些缺陷，大家在使用过程中要注意：**

- 1、对Linked系列对象不支持，比如LinkedHashMap、LinkedHashSet 等，但可以通过CollectionSerializer类修复。
- 2、Locale 类不支持，可以通过扩展 ContextSerializerFactory 类修复。
- 3、Byte/Short 在反序列化的时候会转成 Integer。

#### Protobuf语法、Protobuf是如何工作的

要想使用Protobuf就需要先定义proto文件，先熟悉protobuf消息定义的相关语法。

##### **1. 定义消息类型**（实现 Protobuf 文件接口）

```c++
syntax = "proto3";
message SendRequest {
  string query = 1;
  int32 page_number = 2;
  repeated int32 result_per_page = 3;
}
使用 protoc 工具生成对应的 C++ 代码：
protoc --cpp_out=./ test_tinypb_server.proto
```

`.proto`文件的第一行指定了使用`proto3`语法，说明使用的是`proto3`版本。如果省略protocol buffer编译器就默认使用`proto2`语法。他必须是文件中非空非注释行的第一行。

message表示**消息类型**，可以有多个。

SendRequest定义中指定了三个字段(name/value键值对)，每个字段都会有名称和类型。

repeated是字段规则。

**Protobuf各个语言标量类型对应关系**

一个标量消息字段可以含有一个如下类型，下面表格定义了在.proto文件中的类型，以及与之对应在自动生成访问类中定义类型。

**2. 定义字段的规则**

message的字段必须符合以下规则：

singular：一个遵循**singular规则的字段**，在一个结构良好的message消息体(编码后的message)可以有0或1个该字段（但是不可以有多个）。这是proto3语法的默认字段规则。（这个理解起来有些晦涩，举例来说上面例子中三个字段都是singular类型的字段，在编码后的消息体中可以有0或者1个query字段，但不会有多个。）

repeated：遵循repeated规则的字段在消息体重可以有任意多个该字段值，这些值的顺序在消息体重可以保持（就是数组类型的字段）

**3. 添加更多消息类型**

在单个`.proto`文件中可以定义多个message，这在定义多个相关message时非常有用。比如说，我们定义SendRequest对应的响应message`SendResponse`,把它加到之前的`.proto`文件中。

```text
syntax = "proto3";

message SendRequest {
  string query = 1;
  int32 page_number = 2;
  repeated int32 result_per_page = 3;
}

message SendResponse {
 ...
}
```

**4. 添加注释**

`.proto`文件中的注释和C，C++的注释风格相同，使用// 和 /* ... */

```text
syntax = "proto3";

message SendRequest {
  string query = 1;         // 查询
  int32 page_number = 2;    // 页码
  repeated int32 result_per_page = 3;   //分页数
}
```

**5. message支持嵌套使用**

```text
message SendResponse {
    message Result {
        string url = 1;
        string title = 2;
        repeated string snippets = 3;
    }
    repeated Result results = 1;
}
```

**6.使用其它消息类型**

可以将其它消息类型作为字段的类型，如下，有多个message中都包含Person类型，此时就可以在相同的.proto文件定义一个Person类型，然后在需要用到的message中指定一个Person字段。

```text
message Personinfo {
        repeated Person info = 1;
}
message Person {
        string name = 1;
        int32 age = 2;
        repeated int32 weight = 3;
}
```

上面也可以嵌套在一起写

```text
message Personinfo {
        message Person {
            string name = 1;
            int32 age = 2;
            repeated int32 weight = 3;
        }
        repeated Person info = 1;
}
```

如果想在它的父消息类型外部重用这个消息类型，可以使用Personinfo.Person的方式

```text
message PersonMessage {
    Personinfo.Person info = 1
}
```

##### **7. 定义服务（Service）**

如果想将**消息类型**用在RPC系统中，可以在**.proto文件中定义一个RPC服务接口**，protobuf编译器会根据所选择语言生成对应语言的**服务接口代码及存根**。例如**定义一个RPC服务并具有一个方法**，该方法能够接受SearchRequest并返回一个SearchResponse，此时就可以在.proto文件进行如下定义：

```text
service SearchService {
    //rpc（rpc关键字） 服务函数名  （传入参数） 返回（返回参数）
      rpc  Search （SearchRequest） returns （SearchResponse） ;
}
```

　gRPC就是使用Protobuf的一个RPC系统，gRPC在使用Protobuf时候非常有效。

##### 搭建基于 TinyPB 协议的 RPC 服务

**1、实现 Protobuf 文件接口**

TinyPB 协议基于 Protobuf 来序列化的，在搭建基于 TinyPB 协议的 RPC 服务之前，需要先定义接口文档。具体的 Protobuf 文档需要根据业务的实际功能来编写，这里给出一个例子如下:

```c++
// test_tinypb_server.proto
syntax = "proto3";
option cc_generic_services = true;

message queryAgeReq {
  int32 req_no = 1;
  int32 id = 2;
}
message queryAgeRes {
  int32 ret_code = 1;
  string res_info = 2;
  int32 req_no = 3;
  int32 id = 4;
  int32 age = 5;
}
message queryNameReq {
  int32 req_no = 1;
  int32 id = 2;
  int32 type = 3;
}
message queryNameRes {
  int32 ret_code = 1;
  string res_info = 2;
  int32 req_no = 3;
  int32 id = 4;
  string name = 5;
}
service QueryService {
  // rpc method name
  rpc query_name(queryNameReq) returns (queryNameRes);

  // rpc method name
  rpc query_age(queryAgeReq) returns (queryAgeRes);
}
```

使用 protoc 工具生成对应的 C++ 代码：

```
protoc --cpp_out=./ test_tinypb_server.proto
```

**2、准备配置文件**

**TinyRPC** 读取标准的 **xml** 配置文件**完成一些服务初始化设置**，这个配置文件模板如下，一般只需要按需调整参数即可：

```c++
<?xml version="1.0" encoding="UTF-8" ?>
<root>
  <!--log config-->
  <log>
    <!--identify path of log file-->
    <log_path>./</log_path>
    <log_prefix>test_tinypb_server</log_prefix>

    <!--identify max size of single log file, MB-->
    <log_max_file_size>5</log_max_file_size>

    <!--log level: DEBUG < INFO < WARN < ERROR-->
    <rpc_log_level>DEBUG</rpc_log_level>
    <app_log_level>DEBUG</app_log_level>

    <!--inteval that put log info to async logger, ms-->
    <log_sync_inteval>500</log_sync_inteval>
  </log>

  <coroutine>
    <!--coroutine stack size (KB)-->
    <coroutine_stack_size>256</coroutine_stack_size>

    <!--default coroutine pool size-->
    <coroutine_pool_size>1000</coroutine_pool_size>

  </coroutine>

  <msg_req_len>20</msg_req_len>

  <!--max time when call connect, s-->
  <max_connect_timeout>75</max_connect_timeout>

  <!--count of io threads, at least 1-->
  <iothread_num>8</iothread_num>

  <time_wheel>
    <bucket_num>6</bucket_num>

    <!--inteval that destroy bad TcpConnection, s-->
    <inteval>10</inteval>
  </time_wheel>

  <server>
    <ip>127.0.0.1</ip>
    <port>39999</port>
    <!--注意这里选择 TinyPB 协议-->
    <protocal>TinyPB</protocal>
  </server>
</root>
```

**3、实现业务接口**

**protobuf 文件**提供的只是接口说明，而实际的业务逻辑需要自己实现。只需要**继承 QueryService 并重写方法**即可，例如：

```c++
// test_tinypb_server.cc
class QueryServiceImpl : public QueryService {
 public:
  QueryServiceImpl() {}
  ~QueryServiceImpl() {}

  void query_age(google::protobuf::RpcController* controller,
                       const ::queryAgeReq* request,
                       ::queryAgeRes* response,
                       ::google::protobuf::Closure* done) {

    AppInfoLog << "QueryServiceImpl.query_age, req={"<< request->ShortDebugString() << "}";

    response->set_ret_code(0);
    response->set_res_info("OK");
    response->set_req_no(request->req_no());
    response->set_id(request->id());
    response->set_age(100100111);

    if (done) {
      done->Run();
    }

    AppInfoLog << "QueryServiceImpl.query_age, res={"<< response->ShortDebugString() << "}";

  }

};
```

### 4.1.4. 启动 RPC 服务

TinyRPC 服务启动非常简单，只需寥寥几行代码即可：

```c++
int main(int argc, char* argv[]) {
  if (argc != 2) {
    printf("Start TinyRPC server error, input argc is not 2!");
    printf("Start TinyRPC server like this: \n");
    printf("./server a.xml\n");
    return 0;
  }

  // 1. 读取配置文件
  tinyrpc::InitConfig(argv[1]);
  // 2. 注册 service
  REGISTER_SERVICE(QueryServiceImpl);
  // 3. 启动 RPC 服务
  tinyrpc::StartRpcServer();
  
  return 0;
}
```

生成可执行文件 **test_tinypb_server** 后，启动命令如下：

```
nohup ./test_tinypb_server ../conf/test_tinypb_server.xml &
```

如果没什么报错信息，那么恭喜你启动成功了。如果不放心，可以使用 **ps 命令查看进程是否存在**：

```
ps -elf | grep 'test_tinypb_server'
```

或者使用 **netstat 命令查看端口是否被监听**：

```
netstat -tln | grep 39999
```

至此，基于 TinyPB 协议的 RPC 服务已经启动成功，后续我们将调用这个服务。

##### **Protobuf是如何工作的**

.proto文件是protobuf一个重要的文件，它定义了**需要序列化数据的结构**，当**protobuf编译器**（protoc）来运行.proto文件时候，编译器将生成所选择的语言的代码，比如你选择go语言，那么就会将.proto转换成对应的go语言代码，对于go来说，编译器会为**每个消息类型生成一个pd.go文件**，而**C++会生成一个.h文件和一个.cc文件**。

使用protobuf的3个步骤是：

1. 在.proto文件中**定义消息格式**。
2. 用protobuf编译器编译**.proto文件**。
3. 用C++/Java/go等**对应的protobuf API来写或者读消息**。

在`.proto`文件定义消息，**message是`.proto`文件最小的逻辑单元**，由**一系列name-value键值对**构成。下面的`.proto`文件定义了一个"人"的消息：

```text
message Person {
  required string name = 1;
  required int32 id = 2;
  optional string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    required string number = 1;
    optional PhoneType type = 2 [default = HOME];
  }

  repeated PhoneNumber phone = 4;
}
```

message消息包含**一个或多个编号唯一的字段**，每个字段由**字段限制,字段类型,字段名和编号**四部分组成，字段限制分为：optional(可选的)、required(必须的)以及repeated(重复的)。**定义好消息后，使用ProtoBuf编译器生成C++对应的`.h`和`.cc`文件**，**源文件**提供了message消息的**序列化和反序列化等方法**

### 7.2、 服务注册发现流程、如何实现

服务注册与发现问题通常通过服务注册中心来处理。服务提供者启动时将自己注册到注册中心，服务消费者需要调用服务时，先从注册中心获取服务提供者的信息，然后再进行调用。

**（1）注册与发现流程**

服务注册：**服务提供方**将对外暴露的接口**发布到注册中心**内，注册中心为了检测服务的有效状态，一般会建立**双向心跳机制**。
服务订阅：**服务调用方去注册中心**查找并订阅**服务提供方的 IP**，并**缓存到本地**用于后续调用。

**（2）如何实现：基于ZooKeeper**

A. 在 ZooKeeper 中创建一个服务根路径，可以根据接口名命名，在这个路径再创建**服务提供方与调用方目录**（server、client），分别用来存储服务提供方和调用方的节点信息。
B. **服务端发起注册时**，会在服务提供方目录中创建一个临时节点，节点中存储注册信息。
C. **客户端发起订阅时**，会在服务调用方目录中创建一个临时节点，节点中存储调用方的信息，同时**watch** 服务提供方的目录中所有的服务节点数据。当**服务端产生变化时**ZK就会通知给订阅的客户端。

 **(3) ZooKeeper方案的特点：**

强一致性，ZooKeeper **集群的每个节点的数据**每次发生更新操作，都会**通知其它 ZooKeeper 节点**同时执行更新。



### 7.3、 健康监测

**为什么需要做健康监测?**

比如网络中的波动，硬件设施的老化等等。可能造成**集群当中的某个节点存在问题**，无法正常调用。

**健康监测实现分析**

心跳检测的过程总共包含以下状态:健康状态、波动状态、失败状态。

**完善的解决方案**

（1）阈值： **健康监测增加失败阈值记录**。
（2）成功率： 可以再追加调用成功率的记录（成功次数/总次数）。
（3）探针： 对**服务节点有一个主动的存活检测机制**。

### 7.4、 零拷贝

**（1）什么是零拷贝？**

系统内核处理 IO 操作分为两个阶段：等待数据和拷贝数据。

等待数据，就是系统内核在等待网卡接收到数据后，把数据写到内核中。

拷贝数据，就是系统内核在获取到数据后，将数据拷贝到用户进程的空间中

所谓的零拷贝，就是**取消用户空间与内核空间之间的数据拷贝操作**，**应用进程每一次的读写操作**，都可以通过一种方式，让应用进程向用户空间写入或者读取数据，就如同直接向内核空间写入或者读取数据一样，再通过 **DMA 将内核中的数据拷贝到网卡**，或将网卡中的数据 copy 到内核。

**（2）RPC框架的零拷贝应用**

**Netty 框架是否也有零拷贝机制？**
Netty 的零拷贝则有些不一样，他完全站在了用户空间上，也就是基于 JVM 之上。

**Netty当中的零拷贝是如何实现的？**
RPC 并不会把请求参数作为一个整体数据包发送到对端机器上，中间可能会拆分，也可能会合并其他请求，所以消息都需要有边界。接收到消息之后，需要对数据包进行处理，根据边界对数据包进行分割和合并，最终获得完整的消息。

**Netty零拷贝主要体现在三个方面：**

1、Netty的接收和发送ByteBuffer是采用DIRECT BUFFERS，使用堆外的直接内存（内存对象分配在JVM中堆以外的内存）进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果采用传统堆内存（HEAP BUFFERS）进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后写入Socket中。
2、Netty提供了组合Buffer对象，也就是CompositeByteBuf 类，可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝。

3、Netty的文件传输采用了FileRegion 中包装 NIO 的 FileChannel.transferT o() 方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。

零拷贝带来的作用就是避免没必要的 CPU 拷贝，减少了 CPU 在用户空间与内核空间之间的上下文切换，从而提升了网络通信效率与应用程序的整体性能。

### 7.5 RPC为什么要采用异步?

RPC框架中异步调用可以提高系统的并发性能和响应速度。异步调用一般通过以下方式实现：

1. 异步代理：客户端调用服务时，将请求封装为异步代理对象，该代理对象在收到结果之前不会阻塞，客户端可以继续执行其他任务。
2. 异步通信：客户端和服务端使用异步通信机制，如消息队列，来发送和接收请求和响应。

影响到性能和吞吐量的根本原因是什么呢？ 其实就是RPC请求的整体耗时。如果采用同步调用， CPU 大部分的时间都在等待而没有去计算，从而导致 **CPU 的利用率不够**。RPC 请求比较耗时的原因主要是在哪里？在大多数情况下，RPC 本身处理请求的效率是在毫秒级的。RPC 请求的耗时大部分都是业务耗时。比如业务逻辑中有访问数据库执行慢 SQL 的操作，核心是在I/O瓶颈。所以说，在大多数情况下，影响到 RPC 调用的吞吐量的原因也就是业务逻辑处理慢了，CPU 大部分时间都在等待资源。

现在流行的响应式开发，实质上就是通过**异步方式提升业务处理的吞吐量**，所以， 要作为一个高性能的RPC 框架必须要做到异步化，这样可以极大的提升整体吞吐量。

#### 调用端如何实现异步

常用的方式就是Future 方式，它是返回 Future 对象，通过GET方式获取结果；或者采用入参为 Callback 对象的回调方式，处理结果。

基于RPC的DUBBO框架是如何实现异步调用呢？

用户端发送的每条消息都一个**唯一的消息标识**，调用端向服务端发送请求消息之前会**先创建一个 Future**，并会存储这个消息标识与这个 Future 的映射，**动态代理**所获得的返回值最终就是从这个 Future 中获取的；当收到服务端响应的消息时，调用端会根据**响应消息的唯一标识**，通过之前存储的映射找到对应的 Future，将结果注入给那个 Future，再进行一系列的处理逻辑，最后动态代理从 Future 中通过GET方法获得到正确的返回值。

#### 服务端如何实现异步?

RPC 服务端接收到请求的二进制消息之后会根据协议进行**拆包解包**，之后将完整的消息进行**解码并反序列化**，获得到入参参数之后再通过**反射执行业务逻辑**。那这些操作都是由**一个线程负责执行**吗？

**答：**为了提升性能，连接请求与业务处理不会放在一个线程处理， 这个就是服务端的异步化。对二进制消息数据包拆解包的处理是放在处理**网络 IO 的线程**中，而**解码与反序列化**不涉及**业务逻辑**， 一般也是放在 IO 线程中处理，那服务端的业务逻辑呢？业务逻辑是应该交给专门的业务线程池处理，以防止由于业务逻辑处理得过慢而影响到网络 IO 的处理。

**问题：**滥用多线程， 业务线程是很容易就被打满了，吞吐量很不理想，并且这时 CPU 的利用率也很低

**解决**：**服务端业务处理逻辑加入异步处理机制。在RPC 框架提供一种回调方式，让业务逻辑可以异步处理，处理完之后调用 RPC 框架的回调接口**

#### **RPC框架的异步实现：**

RPC 框架的异步策略主要是**调用端异步与服务端异步**。**调用端的异步就是通过 Future 方式实现异步**，调用端发起一次异步请求并且从请求上下文中拿到一个 Future，之后通过 Future 的 get 方法获取结果，如果业务逻辑中同时调用多个其它的服务，则可以通过 Future 的方式减少业务逻辑的耗时，提升吞吐量。

**服务端异步则需要一种回调方式，让业务逻辑可以异步处理**，之后调用 RPC 框架提供的回调接口，将最终结果异步通知给调用端。这样就**实现了RPC调用的全异步**。

#### 同步与异步调用的区别？

同步调用是指客户端发起调用后会一直等待结果返回，而异步调用是指客户端发起调用后可以继续执行其他任务，并在结果返回时得到通知。选择何种调用方式取决于具体的业务需求。

### 7.6  路由和负载均衡

#### **为什么要用路由**

真实的环境中一般是以**集群的方式**提供服务，对于服务调用方来说，一个接口会有**多个服务提供方**同时提供服务，所以 RPC 在每次发起请求的时候，都需要**从多个服务节点里面选取一个**用于处理请求的**服务节点**。这就需要在RPC应用中**增加路由功能**。

#### **如何实现路由**

**服务注册发现方式：**

通过服务发现的方式从逻辑上看是可行，但注册中心是用来**保证数据的一致性**。通过服务发现方式来实现请求隔离并不理想。

##### **RPC路由策略：**

从服务**提供方节点集合**里面选择一个**合适的节点**（负载均衡），把符合我们要求的节点筛选出来。

**这个就是路由策略：接收请求-->请求校验-->路由策略-->负载均衡-->**

有些场景下，可能还需要更**细粒度的路由方式**，比如说根据**SESSIONID要落到相同的服务节点**上以保持会话的有效性;

#### **RPC框架中的负载均衡**

如何从多个实例里挑选一个出来，进行调用，用负载均衡。

##### 什么是负载均衡？

我们的系统中的某个服务的访问量特别大，我们将这个服务部署在了多台服务器上，当客户端发起请求的时候，多台服务器都可以处理这个请求。那么，如何正确选择处理该请求的服务器就很关键。假如，你就要一台服务器来处理该服务的请求，那该服务部署在多台服务器的意义就不复存在了。负载均衡就是为了避免单个服务器响应同一请求，容易造成服务器宕机、崩溃等问题。

##### **自适应的负载均衡策略：**

RPC 的负载均衡是由 RPC 框架自身提供实现，通过所配置的负载均衡组件，**自主选择一个最佳的服务节点**，**发起 RPC 调用请求**。

RPC **负载均衡策略**一般包括**轮询、随机、权重、最少连接**等。Dubbo默认就是使用**随机负载均衡策略**。

##### 具体如何实现？

这就需要判定服务节点的处理能力。

主要步骤：
（1）添加**计分器**和指标采集器。
（2）**指标采集器**收集服务节点 CPU 核数、CPU 负载以及内存占用率等指标。
（3）可以配置开启哪些指标采集器，并设置这些参考指标的**具体权重**。
（4）通过对服务节点的**综合打分**，最终计算出服务节点的实际权重，选择合适的服务节点。

### 7.7  熔断限流

在实际生产环境中，**每个服务节点都可能由于访问量过大**而引起一系列问题，就需要**业务提供方能够进行自我保护**，从而保证在**高访问量、高并发**的场景下，系统依然能够稳定，高效运行。

在Dubbo框架中， 可以通过**Sentinel哨兵**来实现更为完善的**熔断限流**功能

##### **如何实现限流逻辑？**

方法有很多种， 最简单的是**计数器**，还有**平滑限流的滑动窗口**、**漏斗算法**以及**令牌桶算法**等等。

**Sentinel哨兵采用是滑动窗口来实现的限流。**

**调用方的自我保护：**一个服务 A 调用服务 B 时，服务 B 的业务逻辑又调用了服务 C，这时服务 C 响应超时，**服务 B 就可能会因为堆积大量请求而导致服务宕机**，由此产生**服务雪崩**的问题。

**熔断机制：**
熔断器的工作机制主要是**关闭、打开和半打开**这三个状态之间的切换。
**Sentinel 熔断降级组件**它可以支持以下**降级策略**：

**平均响应时间 ：**当 1s 内持续进入 N 个请求，对应时刻的**平均响应时间**（秒级）均**超过阈值**（ count ，以 ms 为单位），那么在接下的**时间窗口**（ 以 s为单位）之内，对这个方法的调用都会自动地熔断（抛出 DegradeException ）。注意 Sentinel 默认统计的 RT 上限是 4900 ms，超出此阈值的都会算作 4900 ms，若需要变更此上限可以通过启动配置项Dcsp.sentinel.statistic.max.rt=xxx 来配置。

**异常比例 ：**当资源的每秒请求量 >= N（可配置），并且**每秒异常总数占通过量的比值****超过阈值**（ DegradeRule 中的 count ）之后，**资源进入降级状态**，即在接下的时间窗口（ DegradeRule 中的 timeWindow ，以 s 为单位）之内，对这个方法的**调用都会自动地返回**。异常比率的阈值范围是 [0.0, 1.0] ，代表 0% - 100%。

**异常数：**当资源近 1 分钟的**异常数目**超过阈值之后会进行熔断。注意由于统计时间窗口是分钟级别的，若 timeWindow 小于 60s，则结束熔断状态后仍可能再进入熔断状态。

### 7.8  优雅启动、优雅关闭

#### **什么是启动预热？**

启动预热就是让**刚启动的服务，不直接承担全部的流量**，而是让它随着时间的移动**慢慢增加调用次数**，最终让**流量缓和**运行一段时间后达到正常水平。

#### **如何实现优雅启动？**

首先要知道**服务提供方的启动时间**，有两种获取方法：
一种是服务提供方在启动的时候，主动将启动的**时间发送给注册中心**；
另一种就是**注册中心来检测**， 将服务提供方的请求注册时间作为启动时间。

调用方**通过服务**发现获取服务提供方的启动时间， 然后进行**降权**，减少被负载均衡选择的概率，从而实现预热过程。

在Dubbo框架中也引入了"warmup"特性，核心源码是
在" com.alibaba.dubbo.rpc.cluster.loadbalance.AbstractLoadBalance.java"中：

```text
protected int getWeight(Invoker<?> invoker, Invocation invocation) {
      // 先得到Provider的权重
      int weight = invoker.getUrl().getMethodParameter(invocation.getMethodName(), 
Constants.WEIGHT_KEY, Constants.DEFAULT_WEIGHT);
      if (weight > 0) {
          // 得到provider的启动时间戳
          long timestamp = invoker.getUrl().getParameter(Constants.REMOTE_TIMESTAMP_KEY, 0L);
          if (timestamp > 0L) {
              // provider已经运行时间
              int uptime = (int) (System.currentTimeMillis() ‐ timestamp);
              // 得到warmup的值，默认为10分钟
              int warmup = invoker.getUrl().getParameter(Constants.WARMUP_KEY, 
Constants.DEFAULT_WARMUP);
              // provider运行时间少于预热时间，那么需要重新计算权重weight（即需要降权）
              if (uptime > 0 && uptime < warmup) {
                  weight = calculateWarmupWeight(uptime, warmup, weight);
              }
          }
      }
      return weight;
  }

  static int calculateWarmupWeight(int uptime, int warmup, int weight) {
      // 随着provider的启动时间越来越长，慢慢提升权重weight
      int ww = (int) ( (float) uptime / ( (float) warmup / (float) weight ) );
      return ww < 1 ? 1 : (ww > weight ? weight : ww);
  }
```

#### 优雅关闭

#### **为什么需要优雅关闭？**

**调用方**会存在以下情况：**目标服务已经下线**;目标服务正在关闭中。

#### **如何实现优雅关闭？**

当**服务提供方**正在关闭，可以直接返回一个特定的**异常给调用方**。然后调用方把这个**节点从健康列表挪出**，并把其
他请求自动重试到其他节点。如需更为完善， 可以再加上主动通知机制。

在Dubbo框架中， 在以下场景中会触发优雅关闭：
JVM主动关闭( System.exit(int) ； JVM由于资源问题退出( OOM )； 应用程序接受到进程正常结束信号：**SIGTERM 或 SIGINT 信号。**
**优雅停机是默认开启的，停机等待时间为10秒**。可以通过配置 dubbo.service.shutdown.wait 来修改等待时间。Dubbo 推出了多段关闭的方式来保证服务完全无损。

### 7.9 RPC结果缓存、多版本控制

**结果缓存：** 在RPC框架中，可以考虑支持结果缓存来提高性能和减轻服务器的负担。结果缓存可以将服务端返回的结果缓存在客户端或者在中间层（如代理服务器、CDN等）上，下次相同的请求到达时，直接返回缓存的结果，而无需真的去服务端查询。结果缓存可以在一定程度上减少对服务端的请求压力，提高响应速度和系统的吞吐量。但是需要注意缓存的合理性和时效性，避免缓存过期或数据不一致的问题。

**多版本控制：** 当服务端接口发生修改时，为了保证兼容性和平滑升级，可以考虑支持多版本控制。多版本控制允许新的接口与旧的接口共存，不同版本的客户端可以根据需要选择调用相应版本的接口。在RPC框架中，可以通过在请求中指定接口的版本号，或者通过在URL或HTTP头中携带版本信息，来实现多版本控制。这样可以避免因为接口的变更导致的不兼容性问题，使得系统可以平稳地进行升级和迭代。

## 8、其他问题（测试）

#### HTTP echo 测试 QPS

测试机配置信息：Centos**虚拟机**，内存**6G**，CPU为**4核**

测试工具：**wrk**: https://github.com/wg/wrk.git

部署信息：wrk 与 TinyRPC 服务部署在同一台虚拟机上, 关闭 TinyRPC 日志

测试命令：

```
// -c 为并发连接数，按照表格数据依次修改
wrk -c 1000 -t 8 -d 30 --latency 'http://127.0.0.1:19999/qps?id=1'
```

测试结果：

| **QPS**           | **WRK 并发连接 1000** | **WRK 并发连接 2000** | **WRK 并发连接 5000** | **WRK 并发连接 10000** |
| ----------------- | --------------------- | --------------------- | --------------------- | ---------------------- |
| IO线程数为 **1**  | **27000 QPS**         | **26000 QPS**         | **20000 QPS**         | **20000 QPS**          |
| IO线程数为 **4**  | **140000 QPS**        | **130000 QPS**        | **123000 QPS**        | **118000 QPS**         |
| IO线程数为 **8**  | **135000 QPS**        | **120000 QPS**        | **100000 QPS**        | **100000 QPS**         |
| IO线程数为 **16** | **125000 QPS**        | **127000 QPS**        | **123000 QPS**        | **118000 QPS**         |

```
// IO 线程为 4, 并发连接 1000 的测试结果
[ikerli@localhost bin]$ wrk -c 1000 -t 8 -d 30 --latency 'http://127.0.0.1:19999/qps?id=1'
Running 30s test @ http://127.0.0.1:19999/qps?id=1
  8 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.79ms   63.83ms   1.68s    99.24%
    Req/Sec    17.12k     8.83k   97.54k    72.61%
  Latency Distribution
     50%    4.37ms
     75%    7.99ms
     90%   11.65ms
     99%   27.13ms
  4042451 requests in 30.07s, 801.88MB read
  Socket errors: connect 0, read 0, write 0, timeout 205
Requests/sec: 134442.12
Transfer/sec:     26.67MB
```

由以上测试结果，**TinyRPC 框架的 QPS 可达到 14W 左右**。



#### **服务端如何处理请求？有哪些方式？**

**参考答案**：服务端接收到客户端的请求后，常见的处理方式有三种，分别是BIO、NIO和AIO。

- **同步阻塞方式**（BIO）：客户端发一次请求，服务端生成一个对应线程去处理。当客户端同时发起的请求很多时，服务端需要创建多个线程去处理每一个请求，当达到了系统最大的线程数时，新来的请求就无法处理了。 
- **同步非阻塞方式** (NIO)：客户端发一次请求，服务端并不是每次都创建一个新线程来处理，而是通过 I/O 多路复用技术进行处理。就是把多个 I/O 的阻塞复用到同一个 select 的阻塞上，从而使系统在单线程的情况下可以同时处理多个客户端请求。这种方式的优势是开销小，不用为每个请求创建一个线程，可以节省系统开销。 
- **异步非阻塞方式**（AIO）：客户端发起一个 I/O 操作然后立即返回，等 I/O 操作真正完成以后，客户端会得到 I/O 操作完成的通知，此时客户端只需要对数据进行处理就好了，不需要进行实际的 I/O 读写操作，因为真正的 I/O 读取或者写入操作已经由内核完成了。这种方式的优势是客户端无需等待，不存在阻塞等待问题。  

>  **使用场景**
>  BIO 适用于连接数比较小的业务场景，这样的话不至于系统中没有可用线程去处理请求。这种方式写的程序也比较简单直观，易于理解。
>
>  NIO 适用于连接数比较多并且请求消耗比较轻的业务场景，比如聊天服务器。这种方式相比 BIO，相对来说编程比较复杂。
>
>  AIO 适用于连接数比较多而且请求消耗比较重的业务场景，比如涉及 I/O 操作的相册服务器。这种方式相比另外两种，编程难度最大，程序也不易于理解。 



#### **RPC 和 Restful API 对比**

面对对象不同：

- RPC 更侧重于动作。
- REST 的主体是资源。
  RESTful 是面向资源的设计架构，但在系统中有很多对象不能抽象成资源，比如登录，修改密码等而 RPC 可以通过动作去操作资源。所以在操作的全面性上 RPC 大于 RESTful。

传输效率：

- RPC 效率更高。RPC，使用自定义的 TCP 协议，可以让请求报文体积更小，或者使用 HTTP2 协议，也可以很好的减少报文的体积，提高传输效率。
  复杂度：

- RPC 实现复杂，流程繁琐。
- REST 调用及测试都很方便。
  RPC 实现（参见第一节）需要实现编码，序列化，网络传输等。而 RESTful 不要关注这些，RESTful 实现更简单。

灵活性：

- HTTP 相对更规范，更标准，更通用，无论哪种语言都支持 HTTP 协议。
- RPC 可以实现跨语言调用，但整体灵活性不如 RESTful。

#### RPC的实现原理架构图

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/04e5b3e5f9e14cd689323d3a833e7c12~tplv-k3u1fbpfcp-zoom-in-crop-mark:4536:0:0:0.awebp?)

> 也就是说两台服务器A，B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。 比如说，A服务器想调用B服务器上的一个方法：
>
> - **1、建立通信**
>
>   首先要解决通讯的问题：即A机器想要调用B机器，首先得建立起通信连接。主要是通过在客户端和服务器之间**建立TCP连接**，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是**按需连接**，调用结束后就断掉，也可以是**长连接**，多个远程过程调用共享同一个连接。
>   通常这个连接可以是按需连接（需要调用的时候就先建立连接，调用结束后就立马断掉），也可以是长连接（客户端和服务器建立起连接之后保持长期持有，不管此时有无数据包的发送，**可以配合心跳检测机制定期检测建立的连接是否存活有效**），多个远程过程调用共享同一个连接。
>
> - **2、服务寻址** 
>
>   要解决寻址的问题，也就是说，A服务器上的应用怎么告诉底层的RPC框架，如何**连接到B服务器（如主机或IP地址）**以及特定的**端口，方法的名称名称是什么**。通常情况下我们需要提供B机器（主机名或IP地址）以及特定的端口，然后指定调用的方法或者函数的名称以及入参出参等信息，这样才能完成服务的一个调用。**可靠的寻址方式**（主要是提供服务的发现）是RPC的实现基石，比如可以**采用Redis或者Zookeeper来注册服务**等等。
>
> - **2.1、从服务提供者的角度看：** 
>
>   当服务提供者启动的时候，需要将自己提供的服务**注册到指定的注册中心**，以便服务消费者能够通过服务注册中心进行查找；当服务提供者由于各种原因致使提供的服务停止时，需要向注册中心注销停止的服务；服务的提供者需要**定期向服务注册中心发送心跳检测**，服务注册中心如果**一段时间未收到来自服务提供者的心跳**后，认为该服务提供者已经**停止服务**，则将该服务从注册中心上去掉。
>
> - **2.2、从调用者的角度看：** 
>
>   服务的调用者启动的时候根据自己订阅的服务**向服务注册中心查找**服务提供者的地址等信息；当服务调用者消费的服务**上线或者下线**的时候，**注册中心会告知该服务的调用者**；服务调用者下线的时候，则取消订阅。
>
> - **3、网络传输**
>
> - **3.1、序列化** 
>
>   当A机器上的应用发起一个RPC调用时，调用方法和其入参等信息需要通过底层的网络协议如TCP传输到B机器，由于网络协议是基于二进制的，所有我们传输的参数数据都需要先进行序列化（Serialize）或者编组（marshal）成二进制的形式才能在网络中进行传输。然后通过寻址操作和网络传输将序列化或者编组之后的二进制数据发送给B机器。
>
> - **3.2、反序列化** 
>
>   当B机器接收到A机器的应用发来的请求之后，又需要对接收到的参数等信息进行反序列化操作（序列化的逆操作），即将二进制信息恢复为内存中的表达方式，然后再找到对应的方法（寻址的一部分）进行本地调用（一般是通过生成代理Proxy去调用, 通常会有JDK动态代理、CGLIB动态代理、Javassist生成字节码技术等），之后得到调用的返回值。
>
> - **4、服务调用** 
>
>   B机器进行本地调用（通过代理Proxy和反射调用）之后得到了返回值，此时还需要再把返回值发送回A机器，同样也需要经过序列化操作，然后再经过网络传输将二进制数据发送回A机器，而当A机器接收到这些返回值之后，则再次进行反序列化操作，恢复为内存中的表达方式，最后再交给A机器上的应用进行相关处理（一般是业务逻辑处理操作）。 通常，经过以上四个步骤之后，一次完整的RPC调用算是完成了，另外可能因为网络抖动等原因需要重试等。

#### 如何更高效地进行网络通信？

- 使用长连接：建立一次连接后，多次请求可以复用同一个连接，减少建立连接的开销。
- 使用连接池：为了避免频繁地创建和销毁连接，可以使用连接池来管理连接，提高连接复用率。
- 使用异步通信：使用异步通信可以提高并发性能，客户端可以在请求发送后继续执行其他任务，而不需要等待响应。

#### 服务端提供的服务如何暴露给客户端：客户端如何发现这些暴露的服务？

- 使用服务发现注册中心（如ZooKeeper）是一种常见的方法来实现服务的暴露和发现
- 服务端提供服务：
  - 服务端在启动时，将自己提供的服务信息注册到ZooKeeper。这包括服务名称、版本号、IP地址和端口号等。
  - ZooKeeper提供了API和客户端库，可以通过这些API来在ZooKeeper上创建节点，每个节点对应一个服务提供者。
- 客户端发现服务：
  - 客户端在需要调用服务时，连接到ZooKeeper，并查询相关服务的信息。客户端可以使用ZooKeeper提供的API来查询指定节点或者根据服务名称和版本号等进行服务发现。
  - 客户端可以通过监听ZooKeeper节点的变化来实时获取可用服务的变化。
- 负载均衡和故障转移：
  - 客户端可以根据自己的策略选择一个可用的服务提供者进行调用。可以根据服务提供者的负载情况、网络延迟等信息来实现负载均衡，确保请求分布在各个服务提供者上，避免单点过载。
  - 当服务提供者出现故障或不可用时，客户端可以通过监听ZooKeeper节点的变化，实时获取不可用服务的变化，然后根据负载均衡策略选择其他可用的服务提供者进行调用，实现故障转移。

#### 序列化和反序列化优化操作

- 选择高效的序列化协议：选择序列化协议时需要考虑性能、数据大小和可读性，常见的序列化协议有JSON、XML、Protocol Buffers、Thrift等。
- 优化数据结构：合理设计数据结构，避免冗余数据，可以减小序列化后的数据大小，提高性能。
- 使用二进制序列化：二进制序列化通常比文本序列化更高效，可以显著减小数据大小。
- 序列化缓存：对于频繁调用的对象，可以将其序列化结果进行缓存，避免重复序列化，提高性能。













# 别人面经：=========================

### Zookeeper服务注册中心怎么做的？

（1）服务发现：

- 服务注册/反注册：保存服务提供者和服务调用者的信息 
- 服务订阅/取消订阅：服务调用者订阅服务提供者的信息，最好有实时推送的功能 
- 服务路由（可选）：具有筛选整合服务提供者的能力。 

（2）服务配置（不包括其它无关配置）：

- 配置订阅：服务提供者和服务调用者订阅微服务相关的配置 
- 配置下发（可选）：主动将配置推送给服务提供者和服务调用者 

（3）服务健康检测

- 检测服务提供者的健康情况



### 一个注册中心,至少需要具备哪些条件?

（项目中RPC服务注册中心需要注意什么?）

（如果让你设计一个服务注册中心，怎么设计？）

**服务注册接口**：服务提供者通过调用服务注册接口来完成服务注册。

**服务反注册接口**：服务提供者通过调用服务反注册接口来完成服务注销。

**心跳汇报接口**：服务提供者通过调用心跳汇报接口完成节点存活状态上报。

**服务订阅接口**：服务消费者通过调用服务订阅接口完成服务订阅，获取可用的服务提供者节点列表。

**服务变更查询接口**：服务消费者通过调用服务变更查询接口，获取最新的可用服务节点列表。

**服务查询接口**：查询注册中心当前注册了哪些服务信息。

**服务修改接口**：修改注册中心中某一服务的信息。



### 注册中心单机还是分布式的，其中一个挂了怎么办？

1. **单机还是分布式**：

   - **单机注册中心**：如果你的注册中心是单点的，一旦这个注册中心挂了，所有的服务都无法找到对应的服务实例，会导致整个系统的故障。这就是单点故障问题。
   - **分布式注册中心**：为了避免单点故障问题，通常会部署分布式注册中心。多个注册中心节点共同提供服务注册和发现功能，一致性通过一些分布式一致性算法（例如Raft、Zookeeper的Zab等）来保证。

2. **其中一个挂了怎么办**：

   如果是单机注册中心挂了，会导致整个系统无法正常工作。而在分布式环境中，如果一个节点挂了，其他节点仍可以正常工作，客户端通常会有重试逻辑尝试连接其他可用节点。



### 一致性，可靠性怎么保证的？

- 一致性通常是通过分布式一致性算法来保证的，可以确保在注册中心的各个节点之间的数据保持一致。

- 可靠性可以通过副本和数据备份等手段来增强，确保注册中心的高可用性和容灾能力。

- **超时控制**：

  在RPC调用中，超时控制是很重要的。通过设定合理的超时时间，可以避免服务的长时间等待和阻塞，从而提高系统的响应能力。

- **加锁和管道支持并发**：

  - **加锁**：在并发控制中，可以使用锁来确保数据的一致性和完整性。
  - **管道**：在RPC调用中，可以使用管道来支持并发，让多个请求可以同时在传输通道中进行。



### 常用服务注册中心, 与Zookeeper注册中心的差异

1. **Zookeeper**：
   - 由Apache基金会开发的一个分布式协调服务。
   - 支持分布式锁、配置管理、集群管理等。
   - 通过Zab协议确保分布式一致性。
2. **Eureka**：
   - Netflix开发的服务注册与发现解决方案。
   - CAP理论中更倾向于AP（可用性和分区容错性）。
   - 客户端缓存服务注册表，降低对Eureka服务器的依赖。
3. **Nacos**：
   - 阿里巴巴开源的一个更加动态、易于管理的平台。
   - 支持服务发现、配置管理和服务管理。
   - 提供强一致性和最终一致性两种模式。

### 为什么用Zookeeper做注册中心？(优点，与其他选型对比下)

（使用zookeeper有什么好处？）

（说一下zookeeper，为什么使用zookeeper，不选其他注册中心？）

（了解Nacos和Zookeeper的区别吗？）

（为什么不选择Redis作为注册中心？（zookeeper临时节点自动宕机自动清除））

（为什么要用Zookeeper（服务注册、发现））

（Zookeeper和Eureka分别是满足CAP中的哪些）



在分布式环境中，我们需要用到一个服务注册中心，来将我们发布的服务注册到上面取，来保证客户端在这个注册中心找到我们对应的节点，同时服务注册中心还要能够监听我们注册服务结点的变化，如何节点出现故障了我们就需要动态删除节点，同时也需要通知客户端节点节点发生了变化，需要做哪些处理，而Zookeeper正好实现了这么一个机制，它通过心跳机制来检测rpc节点是否还存在，每个一段事件zookeeper服务端就像rpc节点发送心跳包来判断rpc节点是否存在，如果不存在就删除节点，同时通过watch机制告诉客户端节点出现了变化，需要做出相应的变化。



#### redis实现分布式锁和zookeeper实现分布式锁的优缺点？

- Redis分布式锁：**强调的是高可用性，不具备强一致性**。（`set key value nx ex + 过期时间`）

  - 优点：完全基于内存，访问速度很快；Redis的处理线程为单线程，使用的是IO多路复用技术，实现一个线程处理多个IO流的效果，减少了创建多个线程的开销，避免不必要的上下文切换以及竞争临界资源导致的锁开销；全程使用hash结构（总结一个字，快，zookeeper是目录树结构）。

  - 缺点：Reids分布式锁获取简单粗暴，需要不断尝试去获取锁，耗性能；redis数据并不是**强一致性**的，可能出现数据不一致的情况；

    - （1）存在问题：

      ① 假如线程A成功得到了锁，并且设置的超时时间是 30 秒。如果某些原因导致线程 A 执行的很慢，过了 30 秒都没执行完，这时候锁过期自动释放，线程 B 得到了锁。

      ② 随后，线程A执行完任务，接着执行del指令来释放锁。但这时候线程 B 还没执行完，线程A实际上删除的是线程B加的锁。

      （2）解决方案：

      可以在 del 释放锁之前做一个判断，验证当前的锁是不是自己加的锁。**在加锁的时候把当前的线程 ID 当做value，并在删除之前验证 key 对应的 value 是不是自己线程的 ID**。但是，这样做其实隐含了一个新的问题，get操作、判断和释放锁是两个独立操作，不是原子性。对于非原子性的问题，我们可以使用Lua脚本来确保操作的原子性。

- zookeeper分布式锁（**目录树结构，都是建立临时节点**）：**强调的是强一致性，不具备高可用性，因此对并发量不能太高**。

  - 优点：设计定位是**分布式协调，具有强一致性**；如果获取不到锁，只需添加一个监听器即可，不需要一直轮询，性能消耗小。
  - 缺点：有较大客户端频繁申请加锁、释放锁（需要动态产生和删除临时节点），对zookeeper集群压力大。



`Eureka` 的处理方式，它保证了AP（可用性），后者就 `ZooKeeper` 的处理方式，它保证了CP（数据一致性）



### 集群一般有几个节点，为什么？

5个，宕机后选举要大于一半成为leader。



### socket过程中发生的系统调用

多个系统调用以完成数据的发送、接收和管理。以下是在socket过程中可能发生的一些关键系统调用：

1. **socket()：** 创建一个套接字，返回一个套接字描述符，该描述符用于后续的通信。
2. **bind()：** 将一个本地地址绑定到套接字，以便指定本地端口和IP地址。通常在服务器端使用。
3. **listen()：** 开始监听连接请求，使得套接字可以接受传入的连接请求。
4. **accept()：** 接受一个传入的连接请求，创建一个新的套接字来处理与客户端的通信，返回新的套接字描述符。
5. **connect()：** 用于客户端，连接到远程服务器的套接字。它需要指定远程服务器的IP地址和端口。
6. **send() 和 sendto()：** 将数据从应用程序发送到连接的套接字。前者用于已连接的套接字，后者用于未连接的套接字，并指定目标地址。
7. **recv() 和 recvfrom()：** 从套接字接收数据到应用程序。前者用于已连接的套接字，后者用于未连接的套接字，并返回发送方的地址。
8. **close()：** 关闭套接字连接，释放相关资源。



### zookeeper如何实现服务发现

1. **注册服务：** 当一个服务启动时，它会在Zookeeper上注册自己的信息，如服务名称、IP地址和端口号等。这通常通过在Zookeeper上创建一个临时节点来实现，临时节点的生命周期与服务的生命周期绑定。
2. **发现服务：** 当其他服务或客户端需要与某个服务进行交互时，它们可以查询Zookeeper来获取所需服务的信息。查询可以是通过监听特定节点的变化，或者通过定期轮询来实现。
3. **监听变化：** 使用Zookeeper的监听功能，可以让服务在服务注册信息发生变化时得到通知。这样，当服务发生故障或者有新的服务加入时，能够及时更新服务信息。
4. **负载均衡：** 基于服务注册信息，可以实现简单的负载均衡。例如，一个客户端可以从可用的服务列表中选择一个服务进行请求，以分摊负载。
5. **动态扩缩容：** 通过监控Zookeeper中服务注册信息的变化，可以实现动态的服务扩缩容。当有新的服务加入或服务退出时，其他服务能够相应地调整自己的行为。



### zookeeper服务容灾？zookeeper服务节点挂掉之后，怎么删除它？

容灾：在集群若干台故障后，整个集群仍然可以对外提供可用的服务。

​            一般配置奇数台去构成集群，以避免资源的浪费。

​            三机房部署是最常见的、容灾性最好的部署方案。

删除：使用临时节点，会话失效，节点自动清除。





### Zookeeper集群节点宕机了怎么发现剔除的？

发现：watcher机制

剔除：临时节点？

当Zookeeper集群中的节点宕机或不可达时，其他节点可以通过一定的机制来检测并剔除故障节点，以保持集群的正常运行。以下是一种常见的处理故障节点的流程：

1. **心跳机制：** Zookeeper集群中的每个节点会定期发送心跳信号给其他节点，以表示它的存活状态。如果一个节点停止发送心跳信号，其他节点就会认为该节点可能宕机了。
2. **选举机制：** Zookeeper集群使用Paxos或Zab等算法来选举新的领导节点（Leader）。当发现当前Leader不可达时，集群中的其他节点会启动选举过程，选举出一个新的Leader节点来继续处理请求。如果宕机的节点是Leader节点，集群会在选举过程中重新选择一个新的Leader。
3. **会话超时：** 客户端与Zookeeper集群建立会话后，会定期发送心跳信号以维持连接。如果一个节点宕机，无法响应客户端的心跳请求，客户端会认为与服务器的会话超时，然后重新连接到其他可用节点。
4. **观察机制：** 客户端可以在Zookeeper上设置观察点（watch），以便在特定节点的数据发生变化时得到通知。如果一个节点宕机，客户端的观察点会失效，然后客户端可以重新查询其他节点来获取最新的数据。

**剔除：**

在Zookeeper集群中，一般不会直接“剔除”宕机节点，因为Zookeeper的设计目标之一是保持集群中的节点数量相对稳定。当一个节点宕机时，集群会通过选举等机制重新选择新的Leader节点和跟随者节点，以保持集群的正常运行。

但是，在某些情况下，你可能需要手动操作来处理宕机节点的情况，例如：

1. **节点恢复：** 如果一个节点宕机后恢复了，你可以将该节点重新添加到集群中，并确保它能够与其他节点正常通信。这可能涉及一些配置和节点管理操作。
2. **数据修复：** 如果一个节点宕机后，一些数据可能会丢失或不一致，你可能需要进行数据修复或恢复操作，以确保集群的一致性。

总的来说，Zookeeper集群通常会自动适应节点的宕机情况，并通过内部的选举和节点管理机制来保持高可用性和容错性。手动操作通常是在处理一些特殊情况或复杂问题时才会涉及的。



### 服务熔断和服务降级有什么区别？

**服务熔断：**如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。

**熔断设计**
三个模块：熔断请求判断算法、熔断恢复机制、熔断报警

（1）熔断请求判断机制算法：使用无锁循环队列计数，每个熔断器默认维护10个bucket，每1秒一个bucket，每个blucket记录请求的成功、失败、超时、拒绝的状态，默认错误超过50%且10秒内超过20个请求进行中断拦截。

（2）熔断恢复：对于被熔断的请求，每隔5s允许部分请求通过，若请求都是健康的（RT<250ms）则对请求健康恢复。

（3）熔断报警：对于熔断的请求打日志，异常请求超过某些设定则报警



**服务降级：**当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。

- 服务接口拒绝服务：页面能访问，但是添加删除提示服务器繁忙。页面内容也可在Varnish或CDN内获取。
- 页面拒绝服务：页面提示由于服务繁忙此服务暂停。跳转到varnish或nginx的一个静态页面。
- 延迟持久化：页面访问照常，但是涉及记录变更，会提示稍晚能看到结果，将数据记录到异步队列或log，服务恢复后执行。
- 随机拒绝服务：服务接口随机拒绝服务，让用户重试，目前较少有人采用。因为用户体验不佳。



**服务限流**

限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。



### zookeeper原理？羊群效应，怎么解决，解决之后又有什么问题，怎么解决

“Zookeeper羊群效应”是指在分布式系统中，当大量客户端同时查询一个不存在的节点时，这些查询会同时涌入到Zookeeper集群，导致集群负载急剧增加，甚至可能影响集群的正常运行。通过合适的解决方法，如设置连接超时、使用缓存等，可以缓解或避免羊群效应。然而，即使解决了羊群效应，仍然可能出现其他问题，如：

1. **性能问题：** 尽管羊群效应可能会被缓解，但分布式系统中的性能问题仍然可能出现。例如，集群过载、网络延迟、资源竞争等都可能导致性能下降。
2. **数据一致性：** 尽管Zookeeper致力于提供强一致性，但在某些情况下，数据一致性可能受到影响。特别是在集群节点故障、网络分区等情况下，可能需要更多的处理来确保数据的一致性和正确性。

### ZAB算法讲一下

（讲了ZAB是paxos的改版，Mysql是paxos、redis sentinel是raft、zookeeper是ZAB、ZAB的具体实现）

### zk的分布式算法zab，如果选举的时候zxid都相同呢？

（比较SID）

### dubbo 怎么注册到zookeeper以及 dubbo 协议，zookeeper协议

Dubbo是一个高性能的分布式服务框架，它可以通过将服务注册到Zookeeper来实现服务的发现和管理。同时，Dubbo还支持多种协议，其中一种常用的协议是Dubbo协议。下面是关于如何将Dubbo服务注册到Zookeeper并使用Dubbo协议的简要步骤：

**将Dubbo服务注册到Zookeeper：**

1. 配置Zookeeper地址：在Dubbo的配置文件中，配置Zookeeper的地址和端口，以便Dubbo能够连接到Zookeeper集群。通常在`dubbo.properties`或`dubbo.xml`中进行配置。

   ```
   propertiesCopy code
   dubbo.registry.address=zookeeper://localhost:2181
   ```

2. 配置服务提供者：在Dubbo的服务提供者配置中，指定要发布的服务接口、实现类等信息。

   ```
   xmlCopy code
   <dubbo:service interface="com.example.UserService" ref="userService" />
   ```

3. 启动服务：在应用程序中启动Dubbo服务提供者，它会自动将服务注册到Zookeeper。

**使用Dubbo协议：**

Dubbo协议是Dubbo框架支持的一种远程调用协议，它包括了服务暴露和引用两部分。

1. **服务暴露：** 在Dubbo服务提供者的配置中，通过`dubbo`协议来暴露服务。

   ```
   xmlCopy code
   <dubbo:protocol name="dubbo" port="20880" />
   ```

2. **服务引用：** 在Dubbo服务消费者的配置中，通过`dubbo`协议来引用远程服务。

   ```
   xmlCopy code
   <dubbo:reference id="userService" interface="com.example.UserService" />
   ```

通过这样的配置，Dubbo服务提供者会将服务暴露在指定的协议和端口上，Dubbo服务消费者则会通过Dubbo协议引用远程服务。

总的来说，通过将Dubbo服务注册到Zookeeper，并使用Dubbo协议，你可以实现在分布式系统中进行高性能的服务发布和调用。具体的配置和使用取决于你的应用需求和Dubbo版本。







### zookeeper的节点类型?

（持久，临时，顺序）

分布式数据一致性协议都知道哪些（2PC 3PC Paxos）

### Raft协议

Raft协议是一种用于分布式系统中实现一致性的一致性算法，旨在解决分布式系统中的数据一致性和故障容忍问题。Raft的设计目标是更易理解和实现，相对于Paxos等算法，它提供了更清晰的概念和状态转换，因此更容易被工程师理解和应用。

以下是Raft协议的一些关键概念：

1. **Leader Election（领导者选举）：** 在Raft中，每个节点可以处于三种状态：Leader、Follower和Candidate。Leader负责处理客户端请求，Follower和Candidate等待Leader发送心跳信号或发起选举。当没有Leader时，节点会进入选举阶段，通过投票选出新的Leader。
2. **Log Replication（日志复制）：** 一致性算法的核心是确保多个节点的数据一致性。在Raft中，日志被用于记录状态变化。Leader负责向其他节点广播它的日志条目，以确保每个节点上的日志都保持一致。
3. **Safety Properties（安全性属性）：** Raft保证在正常操作情况下数据一致性和正确性。它通过保证只有被大多数节点确认的日志才能被提交，从而防止数据丢失或错误。
4. **Term（任期）：** Raft将时间分割成任期，每个任期开始时都会发生一次领导者选举。每个节点会维护一个当前任期和一个在此任期内可能的Leader。

Raft协议的流程大致如下：

1. 选举阶段：节点开始一个新的任期，变成Candidate状态，发起选举请求。其他节点投票给候选者，候选者如果收到足够多的票数，就会成为Leader。
2. 日志复制阶段：Leader负责接收客户端请求，将操作以日志形式追加到自己的日志中，并将日志广播给其他节点。其他节点在收到Leader的日志后进行确认，并将日志应用到自己的状态机中。
3. 提交阶段：一旦Leader的日志被大多数节点确认，该日志就会被提交，状态机执行对应的操作，数据变化得到一致性。

Raft协议的主要优点是它的可理解性和易于实现性，使得开发人员更容易在分布式系统中实现一致性。然而，需要注意的是，虽然Raft提供了很多好处，但在实际应用中，根据系统需求和特点，仍然需要权衡和选择适合的一致性算法。

### 分布式事务的几种解决方案

（2PC，3PC，TCC，基于消息，然后顺带讲了一下优缺点）  分布式事务的几种方式吧（2pc、3pc、tcc、基于消息）以及区别

### Zookeeper 是如何保证一致性的？

Zookeeper通过使用ZAB（Zookeeper Atomic Broadcast）协议来保证分布式系统中的数据一致性和可靠性。ZAB协议是一种基于Paxos算法的变体，专门设计用于Zookeeper。

以下是Zookeeper如何通过ZAB协议来保证一致性的简要过程：

1. **Leader选举：** 在一个Zookeeper集群中，每个时刻只有一个节点被选为Leader，负责处理客户端请求的写操作。当Leader宕机或无法通信时，剩余的节点会发起Leader选举。
2. **事务提案：** 在Leader选举完成后，Leader接受客户端的写请求，并将这些请求转化为一个个提案（Proposal）。这些提案包含操作类型、数据和序列号等信息。
3. **ZAB协议的两个阶段：** ZAB协议由两个阶段组成，分别是广播阶段（Broadcast Phase）和提交阶段（Commit Phase）。
   - **广播阶段：** Leader将提案广播给所有的Follower节点。Follower会记录这些提案并将其复制到自己的日志中。
   - **提交阶段：** 当Leader收到大多数（大于半数）节点的确认，就会将提案标记为已提交。然后，Leader会通知所有节点，告知这个提案已经提交，其他节点可以在自己的状态机中执行相应的操作。
4. **数据一致性：** 由于只有被大多数节点确认的提案才会被提交，这就保证了数据的一致性。即使Leader在提交之前宕机，其他Follower节点也会继续广播和提交提案。
5. **数据持久性：** 在Zookeeper中，已经提交的提案将会被持久化存储，即使节点宕机重启，数据也不会丢失。

总的来说，Zookeeper通过ZAB协议实现了一套可靠的数据一致性机制。这个机制确保了数据在集群中的所有节点之间保持一致，即使在节点故障等情况下也能够恢复和维持数据的一致性。

zookeeper 的一致性，为了防止单机挂掉，zookeeper维护了一个集群，实现自身的高可用。

**重点回答zookeeper的ZAB协议**

**事务的顺序一致性**：全局唯一事务ID,ZXID





### Zookeeper的分布式锁实现方式吗？

（临时节点，如果服务器挂了，锁会自己消失）



### ZooKeeper的作用？

项目答：注册中心。

扩展答：

1.数据发布/订阅 

2.自动化的DNS服务 

3.数据库复制处理 

4.基于zookeeper分布式系统机器间的通信方式 

5.命名服务

6.集群管理（监控、控制） 

7.Master选举 

8.分布式锁 

9.分布式队列



### zookeeper有什么特性

（临时节点、持久节点、ZAB）



### zookeeper 服务下线还有没有别的实现方法



下线Zookeeper服务时，通常需要考虑集群的高可用性和数据一致性。以下是一些可能的实现方法：

1. **临时节点和会话：** 如果节点下线是由于某个会话的关闭，Zookeeper可以自动检测到会话的结束，并删除与该会话相关的临时性节点。这有助于维护数据一致性。
2. **优雅退出：** 在关闭Zookeeper节点之前，首先需要确保节点不再接受新的请求，可以将节点状态标记为“维护中”或类似状态。然后，等待已有的请求和会话结束后，再正式关闭节点。这种方法可以避免中断正在处理的请求，但需要根据具体情况控制下线过程的时机。
3. **集群节点自动检测：** 配置Zookeeper集群节点之间的心跳检测机制，一旦检测到某个节点不可用，其他节点可以自动将其标记为离线，然后进行必要的Leader选举等操作。
4. **数据迁移和备份：** 在下线之前，可以将Zookeeper节点上的数据迁移到其他节点上，以确保数据的备份和持久性。这可能需要一些手动操作和脚本。
5. **备份节点：** 在下线节点之前，可以将某个节点设置为备份节点，然后将其下线。备份节点可以在需要时替代其他节点，以保持集群的正常运行。



### zookeeper宕机与dubbo直连的情况？

zookeeper注册中心宕机-->dubbo直连,可以调服务 

zookeeper宕机了，消费者可以通过本地缓存通信调提供者的服务

现象：zookeeper注册中心宕机，还可以消费dubbo暴露的服务。
原因：健壮性

```
监控中心宕掉不影响使用，只是丢失部分采样数据
数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务
注册中心对等集群，任意一台宕掉后，将自动切换到另一台
注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯
服务提供者无状态，任意一台宕掉后，不影响使用
服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复
```



### 任何一个请求(流量)过来都会打到注册中心么?

（不会，第一次会，有本地缓存）

不是每个请求都会直接打到注册中心（例如Zookeeper）。注册中心在分布式系统中主要用于服务发现和元数据管理，而不是直接处理实际的业务请求。

在使用注册中心的架构中，请求流程通常是这样的：

1. **服务发现：** 在分布式系统中，客户端通常不会直接知道服务提供者的地址和位置。客户端首先会向注册中心查询要请求的服务的地址信息。
2. **客户端请求：** 客户端（例如应用程序、服务消费者）发送请求到服务提供者，希望获得某项服务的结果。
3. **获取服务地址：** 注册中心会返回包含该服务可用实例地址的列表，客户端从中选择一个实例来发送请求。
4. **请求处理：** 客户端将请求发送到所选的服务提供者实例。服务提供者处理请求并返回结果。



### 有一大批流量总是被打到一个实例上面,这个实例的兄弟实例分到的流量很少,怎么办?

(通过合理负载均衡)



### 有一个实例挂了怎么办?

（zookeeper心跳检测更新列表并利用watcher机制发给服务消费者）



### 注册中心怎么进行心跳检测

**Zookeeper 和 Eureka的机制**
Zookeeper 和 Eureka 都实现了一种 TTL 的机制，就是如果客户端在一定时间内没有向注册中心发送心跳，则会将这个客户端摘除。Eureka 做的更好的一点在于它允许在注册服务的时候，自定义检查自身状态的健康检查方法。这在服务实例能够保持心跳上报的场景下，是一种比较好的体验。

**Nacos的机制**
在 Dubbo 和 SpringCloud 这两大体系内，也被培养成用户心智上的默认行为。Nacos 也支持这种 TTL 机制，不过这与 ConfigServer 在阿里巴巴内部的机制又有一些区别。

心跳检测

Nacos 目前支持临时实例使用心跳上报方式维持活性，发送心跳的周期默认是 5 秒，Nacos 服务端会在 15 秒没收到心跳后将实例设置为不健康，在 30 秒没收到心跳时将这个临时实例摘除。

不过正如前文所说，有一些服务无法上报心跳，但是可以提供一个检测接口，由外部去探测，这些服务对服务发现和负载均衡的需求同样强烈。


### 注册中心对于服务端掉线时怎么处理

(移出ip链表，发送给服务消费者，等待服务器上线，重新连接)



### 服务端用的哪个类监听的

（ServerSocket）



### RPC心跳怎么实现的？

是服务端给服务注册中心心跳还是服务端给客户端心跳？

服务调用方怎么知道服务不可用了？

(zookeeper的心跳检测+更新ip列表+watcher发送给服务调用方)：注册中心发送

(利用netty的IdleStateHandler实现心跳服务)：客户端给服务端发送PING消息

在远程过程调用（RPC）中，心跳机制通常用于检测和维护通信双方（客户端和服务器）之间的连接状态。通过发送周期性的心跳信号，可以判断远程服务是否仍然可用，从而及时处理连接中断或服务器故障。

以下是实现RPC心跳的一般方法：

1. **客户端心跳：** 客户端定期发送心跳请求到服务器，以示其仍然处于活动状态。
2. **服务器响应：** 服务器在收到客户端的心跳请求后，返回一个心跳响应，表示服务器仍然处于活动状态。
3. **定时发送：** 心跳通常以固定的时间间隔发送。这可以通过定时器或定时任务来实现。
4. **超时处理：** 客户端在发送心跳后等待服务器响应，如果在一定时间内未收到响应，客户端会认为连接断开或服务器故障。这时，可以触发相应的重连或故障处理机制。
5. **调整频率：** 心跳频率的选择取决于系统的性能和可靠性要求。较高的心跳频率可能会导致更多的网络流量，但能更快地检测连接问题。
6. **适应网络情况：** 在不稳定的网络环境中，可以根据网络延迟等情况调整心跳频率或超时时间，以平衡性能和可靠性。
7. **减少负担：** 为了减少服务器的负担，心跳请求可以是轻量级的，只需包含一些简单的标识信息即可。
8. **容错机制：** 考虑到可能的网络波动，最好是在一定时间内连续几次未收到心跳响应后再判定连接失效，避免误判。



### 怎么实现的类似本地调用？

本地知道类名+服务名，直接调用





### 如果注册中心服务器宕机怎么保证高可用？

高可用：通过设置减少系统不能提供服务的时间。

在zookeeper主要考虑***容灾和扩容\***两方面提高高可用。

1. **灾难恢复：** 针对注册中心宕机引发的灾难情况，需要制定灾难恢复计划，包括备份数据的恢复和重新部署等。
2. **部署多个注册中心实例：** 部署多个注册中心实例，构建一个注册中心集群。每个实例都可以独立地处理服务注册和发现请求，提高整个注册中心的可用性。
3. **使用负载均衡：** 在客户端访问注册中心时，可以通过负载均衡来分发请求到不同的注册中心实例。这样，即使其中一个实例宕机，其他实例仍然可以继续提供服务。
4. **数据备份和复制：** 使用主从或多主复制等技术，将注册中心的数据进行备份和复制到其他实例。当主实例宕机时，备份实例可以接管服务，从而保持服务的可用性。
5. **选举机制：** 如果注册中心集群中有一个主实例和多个备份实例，可以使用选举机制来选择新的主实例。这可以通过Zookeeper等协调服务来实现。
6. **故障转移：** 当注册中心的主实例宕机时，备份实例可以自动接管服务。这需要使用心跳检测和自动故障转移机制。
7. **监控和警报：** 设置监控系统来实时监测注册中心的状态。一旦发现异常，可以发送警报通知管理员，以便及时处理。
8. **地理多样性：** 如果有条件，可以将不同注册中心实例部署在不同的地理位置，以减少单点故障的风险。



### 服务的地址怎么知道？

(注册中心) 



服务注册中心的功能除了放在额外的服务器上实现还能放在哪里？怎么实现?





### RPC服务注册、服务发现、服务注销怎么做的？

在RPC（远程过程调用）架构中，服务的注册、发现和注销是实现分布式系统中服务管理的关键步骤。常见的方法是结合使用注册中心来实现这些功能。以使用Zookeeper作为注册中心为例，以下是RPC服务注册、服务发现和服务注销的一般做法：

**服务注册：**

1. **服务提供者注册：** 当一个服务提供者启动时，它会向注册中心发送服务注册请求，将自己的信息（如服务名、地址、端口等）注册到注册中心上。这可以使用Zookeeper提供的API来实现。
2. **注册信息存储：** 注册中心将服务提供者的注册信息存储在节点中，通常是一个持久的Zookeeper节点。每个服务提供者都有一个唯一的节点路径。

**服务发现：**

1. **服务消费者查询：** 当一个服务消费者需要调用某个服务时，它会向注册中心发起服务发现请求，查询与所需服务相关的注册信息。
2. **注册信息获取：** 注册中心返回所有可用的服务提供者的注册信息。消费者可以根据自己的策略选择一个合适的提供者进行调用。
3. **负载均衡：** 在选择服务提供者时，可以结合负载均衡算法，如轮询、随机、权重等，来分配请求。

**服务注销：**

1. **服务下线：** 当一个服务提供者需要下线时，它会向注册中心发送注销请求，将自己的注册信息从注册中心中移除。
2. **节点删除：** 注册中心收到注销请求后，会删除相应的注册节点，从而标志该服务不再可用。





### 服务注册怎么进行服务注销监听?

在使用Zookeeper实现服务注册和注销时，可以通过注册监听器来监听节点的变化，从而实现服务的注销监听。以下是一般的步骤：

1. **连接到Zookeeper：** 首先，在服务提供者启动时，需要连接到Zookeeper集群。
2. **创建临时节点：** 将服务提供者的信息（如IP地址、端口号等）存储在一个临时节点上。临时节点的特点是，当与该节点关联的会话断开时，节点会自动删除。
3. **监听节点：** 在创建临时节点后，可以为该节点注册一个监听器。这个监听器将会在节点发生变化时被触发。
4. **监听器回调：** 当节点发生变化时，Zookeeper会调用注册的监听器回调函数。在这个回调函数中，你可以实现服务的注销操作。
5. **服务注销：** 在监听器回调函数中，你可以执行服务的注销操作，即将自己的节点从Zookeeper中删除。这会触发服务的注销操作，同时通知服务发现的消费者。







### RPC项目zookeeper怎么实现注册、发现的？

（临时节点存储ip+端口+负载均衡策略）

在一个RPC项目中使用Zookeeper来实现服务注册和服务发现，通常需要以下步骤：

**服务注册：**

1. **连接到Zookeeper：** 在服务提供者启动时，首先要连接到Zookeeper集群。
2. **创建节点：** 为每个服务提供者创建一个Zookeeper节点，节点的路径通常包括服务名、版本等信息。例如，`/services/{service-name}/{version}/{provider-id}`。
3. **节点数据：** 在节点中存储服务提供者的信息，如IP地址、端口号等。
4. **临时节点：** 可以创建临时节点，使得服务提供者断开连接后，临时节点会被自动删除，实现了动态注册和注销。

**服务发现：**

1. **连接到Zookeeper：** 在服务消费者启动时，也要连接到Zookeeper集群。
2. **查询节点：** 通过Zookeeper的API，查询指定路径下的所有服务提供者节点。
3. **监听变化：** 可以注册监听器，监听服务提供者节点的变化，如新增、删除、数据更新等。
4. **负载均衡：** 在发现多个服务提供者时，可以实现负载均衡策略，如轮询、随机等，选择一个提供者进行调用。
5. **动态发现：** 由于使用了临时节点，当一个服务提供者宕机或下线时，节点会被删除，消费者能够自动感知到服务不可用。

需要注意的是，使用Zookeeper实现服务注册和发现时，你需要学习Zookeeper的基本概念和API，以及如何使用它来创建节点、监听节点变化等。此外，还需要考虑一些实际情况，如网络延迟、超时处理、重试机制等，以保证注册和发现的稳定性和可靠性。不同的RPC框架可能会对这些细节有一些封装，所以在实际项目中，你可能会根据所选的框架进行具体的实现。







### 了解过zookeeper的问题吗?

（崩溃恢复无法提供服务、写的性能瓶颈是一个问题、选举过程速度缓慢、无法进行有效的权限控制）





### 负载均衡

#### 项目中负载均衡怎么实现的（看项目代码）

怎么实现负载均衡策略的（我只做了最简单的轮询、加权、随机，通过在zookeeper中配置，然后将引用按照权重将Channel的引用加入到一个List当中）

   先设置一个负载均衡接口LoadBalancer，然后用继承接口得到轮询、随机两个类，然后在NacosServiceDiscovery设置一个loadBalancer属性及它的函数，

   在SocketTestClient的创建client时传入loadBalancer参数到SocketClient类中，serviceDiscovery

   测试类中

   `SocketClient client = new SocketClient(CommonSerializer.KRYO_SERIALIZER, new RoundRobinLoadBalancer());`

   构造函数

   `serviceDiscovery = new NacosServiceDiscovery(loadBalancer);`

   NacosServiceDiscovery中

   `public NacosServiceDiscovery(LoadBalancer loadBalancer){`

   `if (loadBalancer == null){`

   `this.loadBalancer = new RandomLoadBalancer();`

   `}else {this.loadBalancer = loadBalancer;}}`

   lookupService方法调用

   `Instance instance = loadBalancer.select(instances);`

#### 项目中负载均衡算法用到那些

轮询、随机

#### 解释一下什么是负载均衡？

指将负载（工作任务）进行平衡、分摊到多个操作单元上进行运行

之后结合算法回答

#### 负载均衡了解哪些

(1) RandomLoadBalance:随机负载均衡。随机的选择一个。是Dubbo的**默认**负载均衡策略(Dubbo 中的随机负载是按照权重设置随机概率)。

(2) RoundRobinLoadBalance:轮询负载均衡。轮询选择一个(Dubbo中有权重的概念，按公约后的权重设置轮询比率)。

问题：存在慢的提供者请求的问题，比如：第二胎机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上

(3) LeastActiveLoadBalance:最少活跃调用数，相同活跃数的随机。活跃数指调用前后计数差。

好处：使慢的 Provider 收到更少请求，因为越慢的 Provider 的调用前后计数差会越大。

(4) ConsistentHashLoadBalance:一致性哈希负载均衡。一致性hash：添加删除机器前后映射关系一致，当然，不是严格一致。实现的关键是环形Hash空间。将数据和机器都hash到环上，数据映射到顺时针离自己最近的机器中。

好处：当某一台提供者挂时，原本该发往该提供者的请求，基于虚拟节点，平摊到其他提供者，不会引起剧烈变动

#### RPC调用中使用随机算法和轮转算法做负载均衡的优缺点

优点：实现简单，水平扩展方便

缺点：因为相同的请求会被落到不同的机器上，浪费内存啊，内存有限，Cache会被淘汰，频繁淘汰，当然使得命中率低下啊。 

#### dubbo负载均衡算法，一致性哈希的实现？

1.问简单的话，用4.(4)

2.难的话源码，[https://blog.csdn.net/Revivedsun/article/details/71022871](https://hd.nowcoder.com/link.html?target=https://blog.csdn.net/Revivedsun/article/details/71022871)

#### Dubbo为什么推荐基于随机的负载均衡？

1.实现简单，水平扩展方便

2.在一个截面上碰撞的概率高，但调用越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重

#### 负载均衡作用

（1）根据集群中每个节点的负载情况将用户请求转发到合适的节点上, 以避免单点压力过大的问题

（2）负载均衡可实现集群高可用及伸缩性

​        高可用：某个节点故障时，负载均衡器会将用户请求转发到其他节点,从而保证所有服务持续可用.

​        伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。

#### 如何设计负载均衡器

负载均衡器工作原理有两大方法：

1. 接收客户端请求，将请求转发给集群中的各台服务器处理，服务器将处理结果返回给负载均衡器，负载均衡器将处理结果转发给相应的客户端。
2. 接收客户端请求，将请求转发给集群中的各台服务器处理，服务器将处理结果直接返回给相应的客户端。

#### 负载均衡如何保证健壮性？

（采用心跳机制检测宕机节点。）

#### 一个服务可能有多台机器可以调用?

（利用负载均衡算法）



### 序列化协议

***JSON\***

- JSON 进行序列化的额外空间开销比较大，对于大数据量服务这意味着需要巨大的内存和磁盘开销； 
- JSON 没有类型，但像 Java 这种强类型语言，需要通过反射统一解决，所以性能不会太好（比如反序列化时先反序列化为String类，要自己通过反射还原）。 

***Kryo\***：

- 使用变长的int和long保证这种基本数据类型序列化后尽量小 
- 需要传入完整类名或者利用 register() 提前将类注册到Kryo上，其类与一个int型的ID相关联，序列中只存放这个ID，因此序列体积就更小 
- 不是线程安全的，要通过ThreadLocal或者创建Kryo线程池来保证线程安全 
- 不需要实现Serializable接口 
- 字段增、减，序列化和反序列化时无法兼容 
- 必须拥有无参构造函数 

***Hessian\***：

- 使用固定长度存储int和long 
- 将所有类字段信息都放入序列化字节数组中，直接利用字节数组进行反序列化，不需要其他参与，因为存的东西多处理速度就会慢点。 
- 把复杂对象的所有属性存储在一个Map中进行序列化。所以在父类、子类存在同名成员变量的情况下，Hessian序列化时，先序列化子类，然后序列化父类，因此反序列化结果会导致子类同名成员变量被父类的值覆盖 
- 需要实现Serializable接口 
- 兼容字段增、减，序列化和反序列化 
- 必须拥有无参构造函数 
- Java 里面一些常见对象的类型不支持，比如：
  - Linked 系列，LinkedHashMap、LinkedHashSet 等； 
  - Locale 类，可以通过扩展 ContextSerializerFactory 类修复； 
  - Byte/Short 反序列化的时候变成 Integer。 

***Protobuf：\***

- 序列化后体积相比 JSON、Hessian 小很多
- IDL 能清晰地描述语义，所以足以帮助并保证应用程序之间的类型不会丢失，无需类似XML 解析器；
- 序列化反序列化速度很快，不需要通过反射获取类型；
- 打包生成二进制流
- 预编译过程不是必须的

策略：几个序列化协议的区别以及优缺点、Kryo的原理和安全性、两个接口区别。

项目细节：在项目怎么定义序列化协议，怎么定义序列化相关的类以及项目序列化的细节，

#### 序列化和反序列化有什么作用

（1）**实现了数据的持久化**：永久性保存对象，保存对象的字节序列到本地文件或者数据库中；
（2）**序列化实现远程通**：通过序列化以字节流的形式使对象在网络中进行传递和接收；
（3）通过序列化在进程间传递对象；

#### 讲讲Serializable和  Externalizable

1、Serializable序列化时不会调用默认的构造器，而Externalizable序列化时会调用默认构造器的！

2、Serializable：一个对象想要被序列化，它的类就要实现 此接口，这个对象的所有属性都可以被序列化和反序列化来保存、传递。 

​      Externalizable：自定义序列化可以控制序列化的过程和决定哪些属性不被序列化。

3、使用Externalizable时，必须按照写入时的确切顺序读取所有字段状态。否则会产生异常。



















# 其他杂问题======================

如何求一个链表是否有环

> 快慢指针，快指针一次走两步，慢指针一次走1步，判断快慢指针是否会相遇，如果相遇就有环，如果没有相遇就没有环

还有其它方法吗？

> 利用unordered_map记录走过的节点，如果一个节点重复走过那么就是有环，如果没有重复节点就没有环

怎么保证快慢指针一定可以遇到

> 类比操场上跑圈，那跑得快得人到最后可以套那个跑的慢的一圈，两者就相遇了()

假设快指针一次走5步，慢指针一次走2步

> 可能快指针可能刚好超过慢指针，以前都没有考虑过，面试官然我再想想

- 时间复杂度: O(n)，快慢指针相遇前，指针走的次数小于链表长度，快慢指针相遇后，两个index指针走的次数也小于链表长度，总体为走的次数小于 2n
- 空间复杂度: O(1)

#### **判断链表是否有环**

可以使用快慢指针法，分别定义 fast 和 slow 指针，从头结点出发，fast指针每次移动两个节点，slow指针每次移动一个节点，如果 fast 和 slow指针在途中相遇 ，说明这个链表有环。

为什么fast 走两个节点，slow走一个节点，有环的话，一定会在环内相遇呢，而不是永远的错开呢

首先第一点：**fast指针一定先进入环中，如果fast指针和slow指针相遇的话，一定是在环中相遇，这是毋庸置疑的。**

那么来看一下，**为什么fast指针和slow指针一定会相遇呢？**

可以画一个环，然后让 fast指针在任意一个节点开始追赶slow指针。

会发现最终都是这种情况， 如下图：

![142环形链表1](https://code-thinking-1253855093.file.myqcloud.com/pics/20210318162236720.png)

fast和slow各自再走一步， fast和slow就相遇了

这是因为fast是走两步，slow是走一步，**其实相对于slow来说，fast是一个节点一个节点的靠近slow的**，所以fast一定可以和slow重合。



#### **如果有环，如何找到这个环的入口**

**此时已经可以判断链表是否有环了，那么接下来要找这个环的入口了。**

假设从头结点到环形入口节点 的节点数为x。 环形入口节点到 fast指针与slow指针相遇节点 节点数为y。 从相遇节点 再到环形入口节点节点数为 z。 如图所示：

![img](https://code-thinking-1253855093.file.myqcloud.com/pics/20220925103433.png)

那么相遇时： slow指针走过的节点数为: `x + y`， fast指针走过的节点数：`x + y + n (y + z)`，n为fast指针在环内走了n圈才遇到slow指针， （y+z）为 一圈内节点的个数A。

因为fast指针是一步走两个节点，slow指针一步走一个节点， 所以 fast指针走过的节点数 = slow指针走过的节点数 * 2：

```
(x + y) * 2 = x + y + n (y + z)
```

两边消掉一个（x+y）: `x + y = n (y + z)`

因为要找环形的入口，那么要求的是x，因为x表示 头结点到 环形入口节点的的距离。

所以要求x ，将x单独放在左面：`x = n (y + z) - y` ,

再从n(y+z)中提出一个 （y+z）来，整理公式之后为如下公式：`x = (n - 1) (y + z) + z` 注意这里n一定是大于等于1的，因为 fast指针至少要多走一圈才能相遇slow指针。

这个公式说明什么呢？

先拿n为1的情况来举例，意味着fast指针在环形里转了一圈之后，就遇到了 slow指针了。

当 n为1的时候，公式就化解为 `x = z`，

这就意味着，**从头结点出发一个指针，从相遇节点 也出发一个指针，这两个指针每次只走一个节点， 那么当这两个指针相遇的时候就是 环形入口的节点**。

也就是在相遇节点处，定义一个指针index1，在头结点处定一个指针index2。

让index1和index2同时移动，每次移动一个节点， 那么他们相遇的地方就是 环形入口的节点。

那么 n如果大于1是什么情况呢，就是fast指针在环形转n圈之后才遇到 slow指针。

其实这种情况和n为1的时候 效果是一样的，一样可以通过这个方法找到 环形的入口节点，只不过，index1 指针在环里 多转了(n-1)圈，然后再遇到index2，相遇点依然是环形的入口节点。



