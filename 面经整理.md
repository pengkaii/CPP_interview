#### 自我介绍：

您好，我叫xxx，本科就读于xxx，本科期间绩点xx，专业排名前xx%，同时多次获得校级奖学金xx；目前硕士就读于xx，硕士期间必修课平均分xx，专业排名前xx，参加了智能无轨导全位置爬行高效焊接机器人系统项目的研发工作，其中我主要负责xx，主要是xx，目前已经获得了一定的成果，可以实现xx；基于本人对于开发技术的兴趣，也利用课余时间学习了计算机相关基础，如计算机网络、操作系统等相关知识，并尝试xx，进一步熟悉了网络编程等技术；另外，我在xx进行了xx实习，其中我主要负责xx，针对xx，这段经历也让我进一步提高了代码编写能力以及自主开发能力。

## 一、项目相关

#### 项目设计思路，做项目原因，项目内容

- 练手，熟悉网络编程和io多路复用，把学到的知识运用于实际
- 熟悉项目从无到有的一整个流程
- 实现了一个支持用户登录，注册，访问图片，视频资源的服务器。总体上采用了Reactor模型，主线程利用epoll多路复用技术，来监听IO事件。对于连接事件，主线程直接处理。而对于读写任务，采用了线程池的方式，减少了多线程创建回收的花费。利用了C++正则库提供的函数，以解析HTTP报文，对于需要发送的文件，采用了文件映射到内存，写入socket的方式。对于登录的操作，通过mysql的提供的接口，对数据库进行最简单的访问，查询匹配的账号密码，用以用户登入，注册。同时实现了一个异步的日志系统，当写日志时，将日志存放入队列中，通过一个日志写线程来完成写入，减少了日志写入时候的系统调用次数

#### 很多人的简历上都会有这个项目，为什么你还要选择？

项目是在学习计网的过程中逐步搭建的，这个项目综合性比较强，从中既能学习Linux环境下的一些系统调用，也能熟悉网络编程和一些网络框架，其中也根据自己的理解加入了一些性能调优的手段

#### 项目中如何体现你的代码规范？

从模块解耦、命名空间、命名规范等去讲

### IO多路复用相关

#### IO是什么

- 在计算机中，输入/输出（即IO）是指信息处理系统（比如计算机）和外部世界（可以是人或其他信息处理系统）的通信。输入是指系统接收的信号或数据，输出是指从系统发出的数据或信号。（数据从网卡或硬盘读到内核缓冲区）

#### 几种I/O模型

- 阻塞 blocking：调用者调用了某个函数， 等待这个函数返回 ，期间什么也不做，不停的去检查这个函数有没有返回，必须等这个函数返回才能进行下一步动作。
- 非阻塞 non-blocking（NIO）：非阻塞等待，每隔一段时间就去检测 IO 事件是否就绪。 没有就绪就可以做其他事。 非阻塞 I/O 执行系统调用总是立即返回，不管事件是否已经发生，若事件没有发生，则返回 -1 ，此时可以根据 errno 区分这两 种情况，对于 accept,recv 和 send事件未发生时，errno 通常被设置成 EAGAIN 
- IO复用（IO multiplexing）：Linux 用 select/poll/epoll 函数实现 IO 复用模型，这些函数也会使进程阻塞，但是和阻塞 IO 所不同的是这些函数可以同时阻塞多个 IO 操作。而且可以同时对多个读操作、写操作的 IO 函数进行检测。直到有数据可读或可写时，才真正调用 IO 操作函数。
  - 多路IO复用是一种同步IO模型，它可以实现一个线程可以同时监视多个文件描述符，一旦有某个文件描述符准备就绪，就会通知应用程序，对该文件描述符进行操作。在监视的各个文件描述符没有准备就绪的时候，应用程序线程就会阻塞，交出cpu，让cpu去执行其他的任务，以此提高cpu的利用率
- 信号驱动（signal-driven）：信号驱动 IO ，安装一个信号处理函数，进程继续运行并不阻塞，当 IO 事件就绪，进程收到 SIGIO 信号，然后处理 IO 事件。与非阻塞 IO 的区别在于它提供了消息通知机制，不需要用户进程不断的轮询检查，减少了系统 API 的调用次数，提高了效率
- 异步（asynchronous）：Linux 中，可以调用 aio_read 函数告诉内核描述字缓冲区指针和缓冲区的大小、文件偏移及通知的方式，然后立即返回，当内核将数据拷贝到缓冲区后，再通知应用程序

#### I/O多路复用技术介绍下？

- I/O多路复用技术是为了解决原始的阻塞网络IO如果需要支持多个客户端只能使用多进程和多线程的模式，但这样新到一个TCP连接就需要分配一个进程或者线程，对操作系统的负担很大，因此想到使用一个进程来维护多个socket，（一个进程虽然任一时刻只能处理一个请求，但是处理每个请求的事件时，耗时控制在 1 毫秒以内，这样 1 秒内就可以处理上千个请求，把时间拉长来看，多个请求复用了一个进程），这就是IO多路复用技术（这种思想很类似一个 CPU 并发多个进程，所以也叫做时分多路复用）

#### 异步IO与IO多路复用有什么区别

- 多路复用IO，阻塞IO，非阻塞IO都是同步IO，具体表现为，当事件发生时，会通知cpu，需要cpu中断来处理到来的数据，该过程会占用cpu的时间（在 read 调用时，**内核将数据从内核空间拷贝到用户空间的过程都是需要等待的，也就是说这个过程是同步的**）
- 异步IO则是由内核来接收数据的处理过程，完成之后通知cpu，可以直接获取数据，少了处理数据的时间（内核数据准备好和数据从内核态拷贝到用户态这**两个过程都不用等待**，应用程序并不需要主动发起拷贝动作）

#### select,poll,epoll区别

- 首先select,poll,epoll都是IO多路复用机制，主要是为了解决：阻塞网络IO如果需要支持多个客户端只能使用多进程和多线程的模式，操作系统负担大的问题；采用一个进程维护多个socket，进程可以通过一个系统调用函数从内核中获取多个事件
- select 实现多路复用的方式是，将已连接的 Socket 都放到一个**文件描述符集合**，然后调用 select 函数**将文件描述符集合拷贝到内核里**，然后通过遍历文件描述符判断是否有事件发生，有的话还需要**将整个文件描述符拷贝到用户态**中，用户再通过遍历找到对应的Socket
-  poll 和 select 并没有太大的本质区别，**都是使用线性结构存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合来找到可读或可写的 Socket，也需要在用户态与内核态之间拷贝文件描述符集合**，这种方式随着并发数上来，性能的损耗会呈指数级增长
- epoll则解决了select和poll的轮询缺点，**使用了一组函数来完成任务**而不是单个函数
  - epoll 在内核里使用**红黑树**来关注进程所有待检测的 Socket，需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，**减少内核和用户空间大量的数据拷贝和内存分配**(epoll_wait 调用了 `__put_user` 函数，将数据从内核拷贝到用户空间)
  - epoll 使用事件驱动的机制，内核里维护了**一个链表来记录就绪事件**，只**将有事件发生的 Socket 集合传递给应用程序**，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个集合，大大提高了检测的效率
  - 而且，epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高

#### epoll水平模式和边缘模式使用时要注意什么？

- 使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**
- 使用边缘触发模式时，当被监控的 Socket 描述符上有可读事件发生时，**服务器端只会从 epoll_wait 中苏醒一次**，即使进程没有调用 read 函数从内核读取数据，也依然只苏醒一次，缓冲区数据如果不一次性读完，不会再通知，因此要使用while循环读取缓冲区直到返回-1和EAGAIN来判断是否读完所有数据；**每个使用ET模式的文件描述符都应该是非阻塞**的，如果是阻塞的那么读写事件会因为没有后续的事件而一直处于阻塞状态，**使用 I/O 多路复用时，最好搭配非阻塞 I/O 一起使用**
- 一般来说，边缘触发的效率比水平触发的效率要高，因为边缘触发可以减少 epoll_wait 的系统调用次数，系统调用也是有一定的开销的的，毕竟也存在上下文的切换

#### epoll 如何判断数据已经读取完成

- epoll ET(Edge Trigger)模式，才需要关注数据是否读取完毕了。使用select或者epoll的LT模式，不用关注，select/epoll检测到有数据可读去读就OK了

- 针对TCP，调用recv方法，根据recv的返回值。如果返回值小于我们设定的recv buff的大小，那么就认为接收完毕
- TCP、UDP都适用，将socket设为NOBLOCK状态（使用fcntl函数），然后select该socket可读的时候，使用read/recv函数读取数据。当返回值为-1，并且errno是EAGAIN或EWOULDBLOCK的时候，表示数据读取完毕

#### UDP需要LT和ET吗，为什么

-  UDP也可以使用select/epoll，因为select等IO多路复用的操作对象就是文件操作符fd；
- **大多数情况下，TCP服务器是并发的，UDP的服务器是迭代的。**因为 UDP 是非面向连接的，没有一个客户端可以老是占住服务端，不需要对每个应用保持一个线程。只要处理过程不是死循环，或者耗时不是很长，服务器对于每一个客户机的请求在某种程度上来说是能够满足
- 所以说，UDP没有必要使用多路复用。因此也就没有LTET使用之说，UDP使用recvfrom进行接受数据

#### 你说到epoll，那你知道epoll维护了什么样的结构吗

- epoll主要是利用epoll_create(size)（size随意，大于0即可）产生一个epoll实例，创建在内核区（eventpoll），返回一个文件描述符，对该区域进行操作（通过另外两个api）；由于直接创建在内核区，省去了一次拷贝复制操作
- eventpoll内有两个结构体：**rb_root rbr(红黑树集合，存放要检测的文件描述符),** list_head rdlist**(就绪列表，有数据改变的文件描述符**)

#### epoll_ctl是线程安全的吗？

- epoll_wait和epoll_ctl都是线程安全的, 前者带acquire语意, 后者带release语意, 换句话说, 如果epoll_wait后能拿到某个新fd的事件, 那么对应的epoll_ctl前发生的内存修改都可见
- **epoll是通过锁来保证线程安全的, epoll中粒度最小的自旋锁ep->lock(spinlock)用来保护就绪的队列, 互斥锁ep->mtx用来保护epoll的重要数据结构红黑树**

#### 为什么epoll使用红黑树

- 红黑树是个高效的数据结构，增删查一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 select/poll 在每次操作时都传入整个 Socket 集合，**减少了内核和用户空间大量的数据拷贝和内存分配**
- 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来

#### 服务器使用的并发模型？

- 项目中的事件可分为I/O事件、信号及定时事件，该项目采用的是单Reactor多线程进行处理，其主线程(I/O处理单元)只负责监听文件描述符上是否有事件发生，有的话立即通知工作线程(逻辑单元 )，将socket可读写事件放入请求队列，交给工作线程处理，即读写数据、接受新连接及处理客户请求均在工作线程中完成。通常由同步I/O实现（epoll_wait）
- 还有另外一些并发模型，比如说

  - proactor模式中，主线程和内核负责处理读写数据、接受新连接等I/O操作，工作线程仅负责业务逻辑，如处理客户请求。通常由异步I/O实现(aio_read/aio_write)。
  - 主从Reactor模式，也就是多Reactor模型：核心思想是，主反应堆线程只负责分发Acceptor连接建立，已连接套接字上的I/O事件交给sub-reactor负责分发。其中 sub-reactor的数量，可以根据CPU的核数来灵活设置。主要就是为了分担单Reactor模型下Reactor模型的压力（**一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**）

#### 用了epoll一定快吗，什么情况不好

- 看事件的触发率，如果所有的一直都是触发的，那就不存在浪费了，因为处理的都是一定发生的，所以select也不一定不好

### 线程池相关

#### 为什么要采用线程池处理数据+阻塞队列处理数据，好处是什么

- 创建/销毁线程伴随着系统开销（用户内核转换等），过于频繁的创建/销毁线程，会很大程度上影响处理效率；线程并发数量过多，抢占系统资源从而导致阻塞 ；线程池是为了避免创建和销毁线程所产生的开销，避免活动的线程消耗的系统资源：

  - **提高响应速度**，任务到达时，无需等待线程即可立即执行；
  - **提高线程的可管理性**：线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控。

- 阻塞队列会对当前线程产生阻塞，比如一个线程从一个空的阻塞队列中取元素，此时线程会被阻塞直到阻塞队列中有了元素。**当队列中有元素后，被阻塞的线程会自动被唤醒**；使用阻塞队列的原因：

  - 线程池创建线程需要获取locker，影响并发效率，阻塞队列可以很好的缓冲。

  - 另外一方面，如果新任务的到达速率超过了线程池的处理速率，那么新到来的请求将累加起来，这样的话将耗尽资源

#### 线程池核心参数等的设计

一般来说线程池核心参数有:

- 存储线程的数组锁
- 条件变量
- 任务队列(一个function类的队列)

主要通过线程池数组来存储已经创建的线程，为了后续主线程回收时使用。**通过锁和条件变量来实现了生产者消费者模式**。主要通过任务队列实现主线程创建任务，放入任务队列，其余线程为消费者，不断获取任务队列中的任务，然后执行。在该项目中，通过使用分离线程，优化了需要主线程来回收的过程。

#### 线程池中的工作线程是一直等待吗？

在run函数中，为了能够处理高并发的问题，将线程池中的工作线程都设置为阻塞等待在请求队列是否不为空的条件上，因此项目中线程池中的工作线程是处于一直阻塞等待的模式下的

#### 你的线程池工作线程处理完一个任务后的状态是什么？

- 当处理完任务后如果请求队列为空时，则这个线程重新回到阻塞等待的状态
- 当处理完任务后如果请求队列不为空时，那么这个线程将处于与其他线程竞争资源的状态，谁获得锁谁就获得了处理事件的资格

#### 如果同时1000个客户端进行访问请求，线程数不多，怎么能及时响应处理每一个呢？

- 本项目是通过对子线程循环调用来解决高并发的问题的。
- 首先在创建线程的同时就调用了pthread_detach将线程进行分离，不用单独对工作线程进行回收，资源自动回收
- 通过子线程的run调用函数进行while循环，让每一个线程池中的线程永远都不会停止，访问请求被封装到请求队列(list)中，如果没有任务线程就一直阻塞等待，有任务线程就抢占式进行处理，直到请求队列为空，表示任务全部处理完成。

#### 如果一个客户请求需要占用线程很久的时间，会不会影响接下来的客户请求呢，有什么好的策略呢?

- 会，因为线程池内线程的数量时有限的，如果客户请求占用线程时间过久的话会影响到处理请求的效率，当请求处理过慢时会造成后续接受的请求只能在请求队列中等待被处理，从而影响接下来的客户请求
- 应对策略：可以为线程处理请求对象设置处理超时时间, 超过时间先发送信号告知线程处理超时，然后设定一个时间间隔再次检测，若此时这个请求还占用线程则直接将其断开连接

#### 介绍一下几种典型的锁？

线程池的实现还需要依靠锁机制以及信号量机制来实现线程同步，保证操作的原子性 

- 读写锁：
  - 多个读者可以同时进行读
  - 写者必须互斥（只允许一个写者写，也不能读者写者同时进行）
  - 写者优先于读者（一旦有写者，则后续读者必须等待，唤醒时优先考虑写者）

- 互斥锁：
  - 一次只能一个线程拥有互斥锁，其他线程只有等待
  - 互斥锁是在抢锁失败的情况下主动放弃CPU进入睡眠状态直到锁的状态改变时再唤醒，而操作系统负责线程调度，为了实现锁的状态发生改变时唤醒阻塞的线程或者进程，需要把锁交给操作系统管理，所以互斥锁在加锁操作时涉及上下文的切换。
- 条件变量：
  - 互斥锁一个明显的缺点是他只有两种状态：锁定和非锁定。而条件变量通过允许线程阻塞和等待另一个线程发送信号的方法弥补了互斥锁的不足，他常和互斥锁一起使用，以免出现竞态条件。
  - 当条件不满足时，线程往往解开相应的互斥锁并阻塞线程然后等待条件发生变化。一旦其他的某个线程改变了条件变量，他将通知相应的条件变量唤醒一个或多个正被此条件变量阻塞的线程。总的来说互斥锁是线程间互斥的机制，条件变量则是同步机制

- 自旋锁：如果进线程无法取得锁，进线程不会立刻放弃CPU时间片，而是一直循环尝试获取锁，直到获取为止。如果别的线程长时期占有锁，那么自旋就是在浪费CPU做无用功，但是**自旋锁一般应用于加锁时间很短的场景，这个时候效率比较高**

#### 如何销毁线程

- **等待线程结束：**int pthread_join(pthread_t tid, void** retval);

  主线程调用，等待子线程退出并回收其资源，类似于进程中wait/waitpid回收僵尸进程，调用pthread_join的线程会被阻塞。

  - tid：创建线程时通过指针得到tid值。
  - retval：指向返回值的指针。

- **结束线程：**pthread_exit(void *retval);

  子线程执行，用来结束当前线程并通过retval传递返回值，该返回值可通过pthread_join获得。

- **分离线程：**int pthread_detach(pthread_t tid);

  主线程、子线程均可调用。主线程中pthread_detach(tid)，子线程中pthread_detach(pthread_self())，调用后和主线程分离，子线程结束时自己立即回收资源

#### detach和join有什么区别

- 当调用join()，主线程等待子线程执行完之后，主线程才可以继续执行，此时主线程会释放掉执行完后的子线程资源。主线程等待子线程执行完，可能会造成性能损失。
- 当调用detach()，主线程与子线程分离，他们成为了两个独立的线程遵循cpu的时间片调度分配策略。子线程执行完成后会自己释放掉资源。分离后的线程，主线程将对它没有控制权；当你确定程序没有使用共享变量或引用之类的话，可以使用detach函数，分离线程。

#### 线程池中有多少个线程，线程池数量如何设定

- 默认8个,调整线程池中的线程数量的最主要的目的是为了充分并合理地使用 CPU 和内存等资源，从而最大限度地提高程序的性能
- Ncpu 表示 CPU的数量。如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 Ncpu+1；如果是IO密集型任务，参考值可以设置为 2 * Ncpu。因为线程间竞争的不是CPU的计算资源而是IO，IO的处理一般较慢，多于cores数的线程将为CPU争取更多的任务，不至在线程处理IO的过程造成CPU空闲导致资源浪费
  - 最佳线程数量 = （（线程等待时间+线程CPU时间）／ 线程CPU时间）* CPU个数。
  - 由公式可得，线程等待时间所占比例越高，需要越多的线程，线程CPU时间所占比例越高，所需的线程数越少

#### 多线程中线程越多越好吗

- 不是，假设现有8个CPU、8个线程，每个线程占用一个CPU，同一时间段内，若8个线程都运行往前跑，相比较5/6/7个线程，8个线程的效率高；但若此时有9个线程，只有8个CPU，9个线程同时运行，则此时牵扯到线程切换，而线程切换是需要消耗时间的。
- 随着线程数越多，效率越来越高，但到一个峰值，再增加线程数量时，就会出现问题。线程太多要来回的切换，最终可能线程切换所用时间比执行时间业务所用时间还大
- 随着线程数越多,由于线程执行的时序的问题，程序可能会崩溃或产生二义性



### 具体实现相关

#### 用了状态机啊，为什么要用状态机？

- 在逻辑处理模块中，响应HTTP请求采用主从状态机来完成；传统的控制流程都是按照顺序执行的，状态机能处理任意顺序的事件，并能提供有意义的响应—即使这些事件发生的顺序和预计的不同
- 项目中使用主从状态机的模式进行解析，**从状态机（parse_line）负责读取报文的一行，主状态机负责对该行数据进行解析**，主状态机内部调用从状态机，从状态机驱动主状态机。**每解析一部分都会将整个请求的m_check_state状态改变**，状态机也就是根据这个状态来进行不同部分的解析跳转的

#### 状态机的转移图画一下

**从状态机负责读取报文的一行，主状态机负责对该行数据进行解析**，主状态机内部调用从状态机，从状态机驱动主状态机

<img src="https://img-blog.csdnimg.cn/8490e9ceb6c044e0ae259dd41400fef7.png" alt="img" style="zoom: 67%;" />

- 主状态机，三种状态，标识解析位置
  - CHECK_STATE_REQUESTLINE，解析请求行
  - CHECK_STATE_HEADER，解析请求头
  - CHECK_STATE_CONTENT，解析消息体，仅用于解析POST请求
- 从状态机,三种状态，标识解析一行的读取状态。
  - LINE_OK，完整读取一行，该条件涉及解析请求行和请求头部
  - LINE_BAD，报文语法有误
  - LINE_OPEN，读取的行不完整
- 处理结果：

| NO_REQUEST     | 请求不完整，需要继续读取请求报文数据 |
| -------------- | ------------------------------------ |
| GET_REQUEST    | 获得了完整的HTTP请求                 |
| BAD_REQUEST    | HTTP请求报文有语法错误               |
| INTERNAL_ERROR | 服务器内部错误                       |

#### 状态机的缺点

状态机的缺点就是**性能比较低**，一般一个状态做一个事情，性能比较差，在追求高性能的场景下一般不用，**高性能场景一般使用流水线设计**

#### HTTP报文处理流程

- 浏览器端发出http连接请求，主线程创建http对象接收请求并将所有数据读入对应buffer，将该对象插入任务队列，工作线程从任务队列中取出一个任务进行处理。
- 工作线程取出任务后，调用process_read函数，通过主、从状态机对请求报文进行解析。
- 解析完之后，跳转do_request函数生成响应报文，通过process_write写入buffer，返回给浏览器端

#### 你的项目里用到了单例，能说一下单例有哪些实现方式？

- 单例模式就是用来创建独一无二的，只有一个实例对象，是一种最简单的设计模式。特点：1.**一个类只能有一个实例**；2.**自己创建这个实例**；3.**整个系统都要使用这个实例**。

- 对于有一些对象，其实例我们只需要一个，比方说：线程池、缓存（cache）、日志对象等，如果创建多个实例，就会导致许多问题产生，比如资源使用过量、程序行为不可控，或者导致不一致的结果

- 单例模式的基本构造：**私有的**构造方法；**私有的、静态的**实例化变量应用；提供一个**公有的、静态的**获取类实例对象方法

  - **懒汉式**：构造函数声明为private或者protect防止被外部函数实例化，内部保存一个private static的类指针保存唯一的实例，实例的动作有一个public的类方法实现

    ```c++
    class Singleton{
    private:
        static Singleton * instance;
        Singleton(){}
        Singleton(const Singleton& tmp){}
        Singleton& operator=(const Singleton& tmp){}
    public:
        static Singleton* getInstance(){
            if(instance == NULL){
                instance = new Singleton();
            }
            return instance;
        }
    };
    Singleton* Singleton::instance = NULL;
    ```

    这个实现在单线程下是正确的，但在多线程情况下，如果两个线程同时首次调用GetInstance方法且同时检测到Instance是NULL，则两个线程会同时构造一个实例给Instance，这样就会发生错误

  - 改进的懒汉式（双重检查锁）:只有在第一次创建的时候进行加锁，当Instance不为空的时候就不需要进行加锁的操作
  
    ```c++
    class Singleton{
    private:
        static pthread_mutex_t mutex;
        static Singleton * instence;
        Singleton(){
            pthread_mutex_init(&mutex, NULL); 
        }
        Singleton(const Singleton& tmp){}
        Singleton& operator=(const Singleton& tmp){}
    public:
        static Singleton* getInstence(){
            pthread_mutex_lock(&mutex);
            if(instence == NULL){            
                instence = new Singleton();            
            }
            pthread_mutex_unlock(&mutex);
            return instence;
        }
    };
    Singleton* Singleton::instence = NULL;
    pthread_mutex_t Singleton::mutex;
    ```

  - 饿汉式:饿汉式的特点是一开始就加载了，如果说懒汉式是“时间换空间”，那么饿汉式就是“空间换时间”，因为一开始就创建了实例，所以每次用到的之后直接返回就好了。饿汉模式是线程安全的
  
    ```C++
    class Singleton{
    private:
        static Singleton* instence;
        Singleton(const Singleton& temp){}
        Singleton& operator=(const Singleton& temp){}
    protected:
    	 Singleton(){} 
    public:
        static Singleton* getInstence(){ 
            return instence;    
        }
    };
    Singleton* Singleton::instence = new Singleton();
    ```

#### 单例在什么时候初始化？

- 其实就是考察C++中static对象的初始化
- non-local static 对象的初始化发生在main函数执行之前，也即main函数之前的单线程启动阶段，所以不存在线程安全问题。但C++没有规定多个non-local static 对象的初始化顺序，尤其是来自多个编译单元的non-local static对象，他们的初始化顺序是随机的
- 对于local static 对象，其**初始化发生在控制流第一次执行到该对象的初始化语句**时。多个线程的控制流可能同时到达其初始化语句。
  - 在C++11之前，在多线程环境下local  static对象的初始化并不是线程安全的:如果一个线程正在执行local static对象的初始化语句但还没有完成初始化，此时若其它线程也执行到该语句，那么这个线程会认为自己是第一次执行该语句并进入该local static对象的构造函数中。这会造成这个local static对象的重复构造，进而产生**内存泄露**问题
  - 而C++11则在语言规范中解决了这个问题。C++11规定，在一个线程开始local static 对象的初始化后到完成初始化前，其他线程执行到这个local static对象的初始化语句就会等待，直到该local static 对象初始化完成

#### 项目里用到了计时器，你的计时器是什么实现的？计时器是为了实现什么功能？是一个什么堆？

- 定时器主要是为了定期删除非活跃事件，防止连接资源的浪费。非活跃，是指浏览器与服务器端建立连接后，长时间不交换数据，一直占用服务器端的文件描述符，导致连接资源的浪费。定时事件，是指固定一段时间之后触发某段代码，由该段代码处理一个事件，如从内核事件表删除事件，并关闭文件描述符，释放连接资源
- 使用vector基于堆结构进行封装主要是为了快速地删除堆顶也就是过期元素，而采用vector的主要原因是vector支持随机读写，这样子当连接异常或者需要针对某个中间连接增加时长时就可以从中间取出了

#### 说一下定时器的工作原理

- 定时器利用结构体将定时事件进行封装起来。定时事件，即定期检测非活跃连接。

- 服务器主循环为每一个连接创建一个定时器，并对每个连接进行定时。另外，利用基于堆结构设计的vector将所有定时器串联起来，依靠epoll_wait中的timeout来控制epoll_wait的时间，使其能够在下一个时刻来临时(连接断开的时间)能够停止阻塞（函数返回），每次获取timeout时刻的时候就会清除过期的连接

#### 定时任务处理函数的逻辑

- 服务器主循环每次都会循环都会调用timeMS = timer_->GetNextTick()获取到下一个事件节点的时间长度，该函数内部会进行清除过期连接的操作，然后将timeMS通过epoll_wait获取事件并进行处理，针对读写事件都会通过ExtentTime\_延长该事件的时间

#### 单 Reactor 多线程模型介绍一下

-  I/O 多路复用接口写网络程序是面向过程的方式写代码的，这样的开发的效率不高，基于面向对象的思想，对 I/O 多路复用作了一层封装，让使用者不用考虑底层网络 API 的细节，只需要关注应用代码的编写，这就是**Reactor 模式**，**来了一个事件，Reactor 就有相对应的反应/响应**，也即**I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**
- Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

  - Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
  - 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；
- **单 Reactor 多线程方案：**
  - Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
  - 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
  - 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
  - Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
  - 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；
- 单 Reactor 多线程的方案优势在于**能够充分利用多核 CPU 的性能**，带来了多线程竞争资源的问题。例如，子线程完成业务处理后，要把结果传递给主线程的 Handler 进行发送，这里涉及共享数据的竞争。要避免多线程由于竞争共享资源而导致数据错乱的问题，就需要在操作共享资源前加上互斥锁，以保证任意时间里只有一个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据

#### 说下你的日志系统的运行机制？

- 使用单例模式创建日志系统，对服务器运行状态、错误信息和访问数据进行记录，该系统可以实现按天分类，超行分类功能，可以根据实际情况分别使用同步和异步写入两种方式
- 其中异步写入方式，将生产者-消费者模型封装为阻塞队列，创建一个写线程，工作线程将要写的内容push进队列，写线程从队列中取出内容，写入日志文件。
- 超行、按天分文件逻辑，具体的，日志写入前会判断当前day是否为创建日志的时间，行数是否超过最大行限制；若为创建日志时间，写入日志，否则按当前时间创建新log，更新创建时间和行数；若行数超过最大行限制，在当前日志的末尾加count/max_lines为后缀创建新log

#### 日志系统为什么要异步？和同步的区别是什么？

- 同步方式写入日志时会产生比较多的系统调用，若是某条日志信息过大，会阻塞日志系统，造成系统瓶颈。异步方式采用生产者-消费者模型，具有较高的并发能力。

  - 生产者-消费者模型，并发编程中的经典模型。以多线程为例，为了实现线程间数据同步，生产者线程与消费者线程共享一个缓冲区，其中生产者线程往缓冲区中push消息，消费者线程从缓冲区中pop消息。

  - 阻塞队列将生产者消费者模型进行封装，使用循环数组实现队列，作为两者共享的缓冲区

- 异步日志，将所写的日志内容先存入阻塞队列，写线程从阻塞队列中取出内容，写入日志。可以提高系统的并发性能。

- 同步日志，日志写入函数与工作线程串行执行，由于涉及到I/O操作，当单条日志比较大的时候，同步模式会阻塞整个处理流程，服务器所能处理的并发能力将有所下降，尤其是在峰值的时候，写日志可能成为系统的瓶颈。

- 写入方式通过初始化时是否设置队列大小（表示在队列中可以放几条数据）来判断，若队列大小为0，则为同步，否则为异步。若异步,则将日志信息加入阻塞队列,同步则加锁向文件中写

#### （创新点）异步日志系统的改进：生产者消费者模式 => 双缓冲机制

- 生产者消费者模式存在的问题：消费者从消息队列每读取一条日志信息就写入文件系统，但是写文件操作是很耗时的。频繁的从消息队列中获取数据，而且每次都要上锁，一定会对生产者的写日志效率产生影响，因为生产者也要对消息队列上锁才能把日志信息插入队列的头部，如果此时消息队列正好被消费者锁住了，那么生产者就必须等待了，这样就会很大影响到日志系统整体的吞吐率
- 双缓冲机制的基本思路是：准备两块 buffer: 1 和 2; 前端负责往 buffer 1 填数据(日志信息)；后端负责把 buffer 2 的数据写入文件。当 buffer 1 写满之后，交换 1 和 2，让后端将 buffer 1 的数据写入文件，而前端则往 buffer 2 填入新的日志信息，如此反复。
- **缓冲区交换：**直接交换两个缓冲区的地址， 把生产者在写入数据时的指向缓冲区1的指针重新指向缓冲区2， 把消费者读取数据时指向的缓冲区2的指针重新指向缓冲区1，这样就达到了交换缓冲区的目的了。
- 双缓冲区的好处是：在大部分的时间中，前台线程和后台线程不会操作同一个缓冲区，这也就意味着前台线程的操作，不需要等待后台线程缓慢的写文件操作(因为不需要锁定临界区)。通过双缓冲技术，很好地解决了生产者和消费者之间的异步操作和速度不匹配问题，提高了日志系统的整体吞吐率。
- 为什么设计成双缓冲而不用更多的缓冲区？前端缓冲不足时会自动扩展，但是双缓冲足够应付使用场景，因为日志只记录必要的信息，并不会太多

#### 怎样实现同一个账户同一时间只能在一个终端登录

- 在账户表的基础上，新建了一个账户account_session表，用来记录登录账户的account_id和最新一次登录成功用户的session_id，然后首先要修改登录方法：每次登录成功后，要将登录用户信息写入Session的同时还要更新account_session表里相应账户的session_id（当然，如果是第一次登录时，进行的便是插入动作），然后要修改获取当前用户信息的方法，在里面要做两重判断，首先，看当前会话是否存在登录用户信息，如果没有，则肯定是未登录，不再赘述，如果有，还要再进一步要用当前会员里存的account_id去account_session表查询最新的session_id，与当前会员中的session_id作比较，如果是一致的，说明当前会话是最新的会话，登录状态正常，如果不一致，说明在当前登录会话创建后，被新的登录会话覆盖掉了，当前的登录会话已经失效，这时候，服务器应该删除当前的登录会话并返回提示给客户端，至此，限制账户同一时间单终端登录功能便实现了





## 二、操作系统

### 内存管理相关

#### malloc如何分配空间

- malloc函数主要是通过brk、mmap这两个系统调用实现的；
- 当分配小于128k的内存时，使用**brk**分配内存，将堆顶指针向高地址移动，获得新的内存空间，通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**
- 分配大于128k的内存时，使用**mmap**分配内存，利用私有匿名映射的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存；通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**
- 这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系
- malloc是从堆里面申请内存，也就是说函数返回的指针是指向堆里面的一块内存。操作系统中有一个记录空闲内存地址的链表。当操作系统收到程序的申请时，就会遍历该链表，然后就寻找第一个空间大于所申请空间的堆结点，然后就将该结点从空闲结点链表中删除，并将该结点的空间分配给程序

#### 为什么不全部使用 mmap 来分配内存？

- 向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间，如果都用 mmap 来分配内存，等于每次都要执行系统调用；mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断
- **频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大**
- malloc 通过 brk() 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接预分配更大的内存来作为内存池，当内存释放的时候，就缓存在内存池中。**等下次在申请内存的时候，就直接从内存池取出对应的内存块就行了，而且可能这个内存块的虚拟地址与物理地址的映射关系还存在，这样不仅减少了系统调用的次数，也减少了缺页中断的次数，这将大大降低 CPU 的消耗**

#### 为什么不全部使用 brk 来分配？（brk频繁调用有什么问题）

- 通过 brk 从堆空间分配的内存，并不会归还给操作系统，对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”
- malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间

#### free() 函数只传入一个内存地址，为什么能知道要释放多大的内存

-  malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节，保存了该内存块的描述信息，比如有该内存块的大小
- 当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了

#### 知道JeMalloc,tcMalloc,ptMalloc吗

- ptmalloc 是基于 glibc 实现的内存分配器，它是一个标准实现，所以兼容性较好。pt 表示 per thread 的意思。当然 ptmalloc 确实在多线程的性能优化上下了很多功夫。由于**过于考虑性能问题，多线程之间内存无法实现共享**，只能**每个线程都独立使用各自的内存**，所以在**内存开销上是有很大浪费的**。
- tcmalloc 全称是 thread-caching malloc，所以 **tcmalloc 最大的特点是带有线程缓存**，tcmalloc 为**每个线程分配了一个局部缓存**，对于小对象的分配，可以直接由线程局部缓存来完成，**对于大对象的分配场景，tcmalloc 尝试采用自旋锁来减少多线程的锁竞争问题**
- jemalloc 同样都包含 **thread cache 的特性**。但是 jemalloc 在设计上比 ptmalloc 和 tcmalloc 都要复杂，jemalloc 将内存分配粒度划分为 Small、Large二个分类，并记录了很多 meta 数据，所**以在空间占用上要略多于 tcmalloc**，不过**在大内存分配的场景，jemalloc 的内存碎片要少于 tcmalloc**

#### 系统调用的开销

- 当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作，运行态的切换会耗费不少时间
- 如果是在第一次访问虚拟地址还会发生缺页中断，这样会导致 CPU 消耗较大

#### 如何优化brk减少频繁系统调用

- 内存管理的核心目标主要有两点：高效的内存分配和回收，提升单线程或者多线程场景下的性能；减少内存碎片，包括内部碎片和外部碎片，提高内存的有效利用率
- 因此可以从这两方面对brk进行优化，针对内存分配和回收问题，可以采用类似tcmalloc的机制，为每个线程分配一个局部缓存，对于小对象的分配，可以直接由线程局部缓存来完成；对于内存碎片问题，采用类似jemalloc的思想，将内存分配粒度划分为 Small、Large二个分类进行减少碎片

#### 虚拟内存介绍下？

- 为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套**虚拟地址空间**，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。作为程序，也不用关心物理地址的事情。
- 每个进程都有自己的虚拟空间，而物理内存只有一个，所以当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换**技术，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）

#### linux有几级页表

- Linux把计算机分成独立层/依赖层两个层次，对于页面的映射和管理也是；linux从最初的2级页表，到3级页表(x86支持物理地址扩展)，再到4级页表(64cpu)
- 页表大小位4KB,所以页内地址需要12位，虚拟地址64位，需要8B,那一页最多存储4KB/8B=2^9,即一级页表占用9位，intel x86_64平台使用48位物理地址，剩余的48-32=9x4可以构成四级页表，48位物理地址可以表示多大的内存空间，2^48B=256TB,此外，x86系列是向后兼容的，为什么是4级页表？因为linux一路从2级页表、3级页表，再到当前的4级页表，满足当前需要，若未来那天不够用了，当然会扩展出更大的
- 四级目录，分别是：
  - 全局页目录项 PGD（*Page Global Directory*）；
  - 上层页目录项 PUD（*Page Upper Directory*）；
  - 中间页目录项 PMD（*Page Middle Directory*）；
  - 页表项 PTE（*Page Table Entry*）；

#### CPU中控制逻辑地址转换的是什么

- 操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存（完成地址转换和 TLB 的访问与交互）
- 主要有两种方式，分别是**内存分段和内存分页**，来管理虚拟地址与物理地址之间的关系

#### MMU底层如何实现的

- MMU主要包含以下几个功能：

  - **虚实地址翻译**：在用户访问内存时，将用户访问的虚拟地址翻译为实际的物理地址，以便CPU对实际的物理地址进行访问
  - **访问权限控制**：可以对一些虚拟地址进行访问权限控制，以便于对用户程序的访问权限和范围进行管理，如代码段一般设置为只读，如果有用户程序对代码段进行写操作，系统会触发异常。
  - **引申的物理内存管理**：对系统的物理内存资源进行管理，为用户程序提供物理内存的申请、释放等操作接口。

- 使用MMU带来的好处或者优势：

  - **提升物理内存的利用率**：物理内存按需申请，如代码段的内存在执行时进行映射和转换，进程fork后，通过写时复制(Copy-On-Write)进行真正的物理内存分配。解决内存管理碎片化的问题，即在系统运行一段时间后，频繁的内存申请和释放会导致内存碎片化，无法申请到一块足够大的地址连续的内存。

  - **对内存地址的访问进行控制**：代码段只读权限控制，多线程的栈内存之间的空洞页隔离可以防止栈溢出后改写其他线程的栈内存，不同进程之间的地址隔离等等。
  - **将进程的地址空间隔离**：不同进程之间可以使用相同的虚拟内存地址空间，而进程间的物理内存又可以做到隔离，这保证了进程的独立性同时，又简化了地址的访问方式

- MMU实现主要分为几个阶段：

  - 用户进程访问虚存地址。
  - 触发TLB查找过程，该部分通过硬件完成，没有软件参与。
  - TLB miss场景下，查找PTE，该部分在不同CPU上实现不同，像X86都是硬件查找，PowerPC有些处理器使用软件查找，即在内核实现一个TLB miss的异常处理，可以灵活做到TLB查找。
  - Do Page Fault，分为几种情况：
    - 新申请内存第一次读写，触发物理内存分配；
    - 进程fork后子进程写内存触发Copy-On-Write；
    - 非法内存读写，错误处理。





### 进程线程相关

#### 什么场景下用多线程比较好

- 系统接受实现多用户多请求的高并发时，通过多线程来实现。

- 如果程序执行到要花大量时间处理的任务时，那主程序就得等待其执行完才能继续执行下面的。那用户就不得不等待它执行完。这时候可以开线程把花大量时间处理的任务放在线程处理，这样线程在后台处理时，主程序也可以继续执行下去，用户就不需要等待。线程执行完后执行回调函数。

- 大任务处理起来比较耗时，这时候可以起到多个线程并行加快处理

#### 为什么多线程比多进程性能好（线程进程间的区别）

- 线程与进程的比较如下：

  - **进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位**；

  - 进程拥有**一个完整的资源平台**，而线程**只独享必不可少的资源，如寄存器和栈**；

  - 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；

  - **线程能减少并发执行的时间和空间开销；**


- 不管是时间效率，还是空间效率线程比进程都要高，线程相比进程能减少开销，体现在：

  - 线程的**创建时间比进程快**，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；线程的**终止时间比进程快**，因为线程释放的资源相比进程少很多；

  - **同一个进程内的线程切换比进程切换快**，因为**线程具有相同的地址空间（虚拟内存共享）**，这意味着同一个进程的线程都具有同一个页表，那么在切换的时候不需要切换页表。而对于**进程之间的切换，切换的时候要把页表给切换掉**，而页表的切换过程开销是比较大的；

  - 由于同一进程的各线程间共享内存和文件资源，那么在**线程之间数据传递的时候，就不需要经过内核**了，这就使得**线程之间的数据交互效率更高**了；


#### 进程间通信方式

- 无名管道：只存在于内存，用于父子或者兄弟进程，单向，可以用read和write进行读写，生命周期随着进程创建而建立，随着进程终止而消失
- FIFO(有名管道)：可以在无关的进程之间交换数据，是一种特殊的文件系统；不管是匿名管道还是命名管道，进程写入的数据都是**缓存在内核**中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则
- 消息队列 ：克服了管道通信的数据是无格式的字节流的问题，**系统内核维护**， **独立于发送接受进程**。消息队列通信的速度不是最及时的，毕竟**每次数据的写入和读取都需要经过用户态与内核态之间的拷贝过程**
- 共享内存：解决消息队列通信中用户态与内核态之间数据拷贝过程带来的开销，**直接分配一个共享空间，每个进程都可以直接访问**，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，最快的通信方式；**带来新的问题，多进程竞争同个共享资源会造成数据的错乱**
- 信号量：用于进程间的互斥与同步,保护共享资源
- 信号：异常情况下的工作模式，就需要用「信号」的方式来通知进程
- Socket:跨网络与不同主机上的进程之间通信

#### 进程间通信方式都需要进入内核态吗

- 每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核；
- 如果只是对共享内存进行读写操作的话，则是不需要进入内核态的，然而共享内存往往需要信号量来保证互斥和同步，因此通信也是需要进入内核态的

#### 共享内存优缺点

- 优点：直接分配一个共享空间，每个进程都可以直接访问**，就像访问进程自己的空间一样快捷方便，不需要陷入内核态或者系统调用，大大提高了通信的速度，**
- **缺点：多进程竞争同个共享资源会造成数据的错乱**

#### 进程间同步方式

- 进程间同步主要是**通过互斥锁以及信号量进行实现**，互斥锁主要用于实现进程对临界资源的互斥访问，进入临界区加锁，出临界区解锁；信号量根据初始信号量的设置可以实现同步以及互斥，通过PV原子操作的系统调用对信号量控制，P 操作是用在进入临界区之前，V 操作是用在离开临界区之后，这两个操作是必须成对出现的
- 另外还可以实现管程实现进程间的同步，其引入了 **条件变量** 以及相关的操作：**wait()** 和 **signal()** 来实现同步操作。对条件变量执行 wait() 操作会导致调用进程阻塞，把管程让出来给另一个进程持有。signal() 操作用于唤醒被阻塞的进程。

#### 什么是线程安全？

- 线程安全问题是出现在多个线程访问的情况下产生的，也就是要确保在多条线程访问的时候，程序还能按照预期的行为去执行；**当多个线程访问某个方法时，不管通过怎样的调用方式或者说这些线程如何交替的执行，在主程序中不需要去做任何的同步，这个类的结果行为都是设想的正确行为，那么就可以说这个类时线程安全的。**
- 引起线程不安全最根本的原因就是 ：线程对于共享数据的更改会引起程序结果错误。
- 线程安全的解决策略就是：保护共享数据在多线程的情况下，保持正确的取值

#### 乐观锁和悲观锁本质的区别是什么

- **乐观锁**：乐观锁在操作数据时非常乐观，认为别人不会同时修改数据。因此乐观锁不会上锁，只是在执行更新的时候判断一下在此期间别人是否修改了数据：如果别人修改了数据则放弃操作，否则执行操作。
  - **乐观锁适用于多读的应用类型，这样可以提高吞吐量**，像数据库提供的类似于**write_condition机制**
  - **乐观锁的实现方式主要有两种：CAS**（Compare And Swap）**机制和版本号机制**
  - **CAS的缺点**：
    - **ABA问题**，栈顶问题：一个栈的栈顶经过两次(或多次)变化又恢复了原值，但是栈可能已发生了变化，比较有效的方案是引入版本号，内存中的值每发生一次变化
    - **高竞争下的开销问题 在并发冲突概率大的高竞争环境下，如果CAS一直失败，会一直重试，CPU开销较大**
    - 功能限制 CAS的功能是比较受限的，例如**CAS只能保证单个变量**（或者说单个内存值）操作的原子性
- **悲观锁**：悲观锁在操作数据时比较悲观，认为别人会同时修改数据。因此操作数据时直接把数据锁住，直到操作完成后才会释放锁；上锁期间其他人不能修改数据。（**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**）
  - 悲观锁的实现方式是加锁，加锁既可以是对代码块加锁，也可以是对数据加锁
- **乐观锁适用于写比较少的情况下（多读场景）**，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以**一般多写的场景下用悲观锁就比较合适**

#### 如果进程A将自己的一个指针通过共享内存共享给进程B，会发生什么？

共享内存映射到每个进程中的地址空间都是不同的，二者的地址空间不同会造成访问越界或者段错误

#### 孤儿进程是什么，僵尸进程是什么

- 孤儿进程是没有父进程的进程。当然创建的时候肯定是要先创建父进程了，当父进程退出时，它的子进程们（一个或者多个）就成了孤儿进程了
- 子进程先退出，而父进程又没有去处理回收释放子进程的资源，这个时候子进程就成了僵尸进程。有大量的僵尸进程占用进程号，导致新的进程无法创建
  - 处理：干掉父进程；父进程调用wait或waitpid；fork两次；signal函数

#### 了解守护进程吗，如何创建

- 指在后台运行的，没有控制终端与之相连的进程。它独立于控制终端，周期性地执行某种任务。Linux的大多数服务器就是用守护进程的方式实现的，如web服务器进程http等


- 创建守护进程要点：
  - 让程序在后台执行。方法是调用fork（）产生一个子进程，然后使父进程退出。
  - 调用setsid（）创建一个新对话期。setsid（）调用成功后，进程成为新的会话组长和进程组长，并与原来的登录会话、进程组和控制终端脱离。
  - 禁止进程重新打开控制终端，再一次通过fork（）创建新的子进程，使调用fork的进程退出。
  - 关闭不再需要的文件描述符。首先获得最高文件描述符值，然后用一个循环程序，关闭0到最高文件描述符值的所有文件描述符。
  - 将当前目录更改为根目录。
  - 子进程从父进程继承的文件创建屏蔽字可能会拒绝某些许可权。为防止这一点，使用unmask（0）将屏蔽字清零。
  - 处理SIGCHLD信号。对于服务器进程，在请求到来时往往生成子进程处理请求。如果子进程等待父进程捕获状态，则子进程将成为僵尸进程（zombie），从而占用系统资源。如果父进程等待子进程结束，将增加父进程的负担，影响服务器进程的并发性能。在Linux下可以简单地将SIGCHLD信号的操作设为SIG_IGN。这样，子进程结束时不会产生僵尸进程

### 磁盘相关

#### 一次IO的时间在什么量级（ms）

内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的，也就是说读取同样大小的数据，磁盘中读取的速度比从内存中读取的速度要慢上万倍，甚至几十万倍





### 基本Linux指令

#### vmstat命令了解吗？

- 动态的了解一下系统资源的使用状况，以及查看当前系统中到底是哪个环节最占用系统资源，就可以使用 vmstat 命令。
- vmstat命令，是 Virtual Meomory Statistics（虚拟内存统计）的缩写，可用来监控 CPU 使用、进程状态、内存使用、虚拟内存使用、硬盘输入/输出状态等信息
- vmstat 本身就是低开销工具，在非常高负荷的服务器上，需要查看并监控系统的健康情况,在控制窗口还是能够使用vmstat 输出结果

#### vim里面查找一个字符？

- 在**命令模式**下输入/然后输入你需要查找的字符串即可。向**光标之下**寻找你所输入的字符串；可以使用n继续向**下**查找下一个名称为job的字符串。可以使用N向**上**查找名称为job的字符串
- 在**命令模式**下输入?然后输入你需要查找的字符串即可。向**光标之上**寻找你所输入的字符串。注意：与/不同，可以使用n继续向**上**查找下一个名称为job的字符串。可以使用N向**下**查找名称为job的字符串。

#### 如何在 Linux 系统中查看 TCP 状态？

TCP 的连接状态查看，在 Linux 可以通过 `netstat -napt` 命令查看

#### 用过什么调试程序的工具吗？

gdb，排查段错误

#### 内存泄漏怎么排查？

用宏定义覆盖 new/delete 关键字，定位申请内存的代码位置；或者使用 mtrace 等工具检测

#### /proc 文件了解吗？里面放的是什么内容？

系统运行的状态信息：CPU信息、负载信息、虚拟内存信息、设备信息等







### 其余问题

#### 中断有哪几种类型？

- 输入输出中断：输入输出中断时当外部设备或通道操作正常结束或发生某种错误时发生的中断。例如：I/O传输出错、I/O传输结束等。
- 外中断：对某中央处理机而言，他的外部非通道式装置所引起的中断称为外部中断。例如：时钟中断、操作员控制台中断、多机系统中CPU到CPU的通信中断。
- 机器故障中断：当机器发生故障时所产生的中断叫硬件故障中断。例如:电源故障、通道与主存交换信息是主存储错、从主存取指令出错、取数据错、长线传输时的奇偶校验错等。
- 程序性中断：在现行程序执行过程中，发现了程序性的错误或出现了某些程序的特定状态而产生的中断称为程序性中断。这些程序性错误有定点溢出、十进制溢出、十进制数错、地址错、用户态下用核态指令、越界、非法操作等。程序的特定状态包括逐条指令跟踪、指令地址符合跟踪、转态跟踪、监视等。
- 访管中断：对操作系统提出某种需求（如请求I/O传输、建立进程等）时所发出的中断称为访管中断

#### 介绍一下缺页中断

- 缺页中断指的是当软件试图访问已映射在虚拟地址空间中，但是目前并未被加载在物理内存中的一个分页时，由中央处理器的内存管理单元所发出的中断。
- 进程运行过程中，如果发生缺页中断，而此时内存中有没有空闲的物理块是，为了能够把所缺的页面装入内存，系统必须从内存中选择一页调出到磁盘的对换区。但此时应该把那个页面换出，则需要根据一定的页面置换算法（Page Replacement Algorithm)来确定。
  - 最佳置换（Optimal， OPT)
  - 先进先出置换算法（First In First Out, FIFO)
  - 最近最久未使用置换算法（Least Recently Used， LRU）

#### 什么是内核呢？

- 计算机是由各种外部硬件设备组成的，比如内存、cpu、硬盘等，如果每个应用都要和这些硬件设备对接通信协议，那这样太累了，所以这个中间人就由内核来负责，**让内核作为应用连接硬件设备的桥梁**，应用程序只需关心与内核交互，不用关心硬件的细节。

- 现代操作系统，内核一般会提供 4 个基本能力：

  - 管理进程、线程，决定哪个进程、线程使用 CPU，也就是进程调度的能力；
  - 管理内存，决定内存的分配和回收，也就是内存管理的能力；
  - 管理硬件设备，为进程与硬件设备之间提供通信能力，也就是硬件通信能力；
  - 提供系统调用，如果应用程序要运行更高权限运行的服务，那么就需要有系统调用，它是用户程序与操作系统之间的接口

- 内核具有很高的权限，可以控制 cpu、内存、硬盘等硬件，而应用程序具有的权限很小，因此大多数操作系统，把内存分成了两个区域：

  - 内核空间，这个内存空间只有内核程序可以访问；
  - 用户空间，这个内存空间专门给应用程序使用；

  用户空间的代码只能访问一个局部的内存空间，而内核空间的代码可以访问所有内存空间。因此，当程序使用用户空间时，我们常说该程序在**用户态**执行，而当程序使内核空间时，程序则在**内核态**执行。应用程序如果需要进入内核空间，就需要通过系统调用

- 内核程序执行在内核态，用户程序执行在用户态。当应用程序使用系统调用时，会产生一个中断。发生中断后， CPU 会中断当前在执行的用户程序，转而跳转到中断处理程序，也就是开始执行内核程序。内核处理完后，主动触发中断，把 CPU 执行权限交回给用户程序，回到用户态继续工作

#### 操作系统内核态和用户态的区别是什么，为什么要有这么一个设定

- 所谓用户态和内核态针对是CPU，是不同权限的资源范围
  - 内核态可以执行一切特权代码
  - 用户态只能执行那些受限权限的代码
- 如此设计的本质意义是进行权限保护。限定用户的程序不能乱搞操作系统，如果人人都可以任意读写任意地址空间软件管理便会乱套

- CPU指令集是有权限分级的，毕竟CPU指令集是可以直接操作硬件的，若将全部的CPU指令集放开给非操作系统开发使用，则极其容易出现问题，因指令操作的不规范，导致操作系统内核、及其他所有正在运行的程序，都可能会因为操作失误操作不可挽回的错误
- 用户态和内核态切换的开销：
  - 保留用户态现场（上下文、寄存器、用户栈等）
  - 复制用户态参数，用户栈切到内核栈，进入内核态
  - 额外的检查（因为内核代码对用户不信任）
  - 执行内核态代码
  - 复制内核态代码执行结果，回到用户态
  - 恢复用户态现场（上下文、寄存器、用户栈等）
- 用户态到内核态切换的场景：
  - 系统调用：用户态进程主动切换到内核态的方式，用户态进程通过系统调用向操作系统申请资源完成工作
  - 异常：当 CPU在执行用户态的进程时，发生了一些没有预知的异常，这时当前运行进程会切换到处理此异常的内核相关进程中，也就是切换到了内核态，如缺页异常
  - 中断：当 CPU 在执行用户态的进程时，外围设备完成用户请求的操作后，会向 C P U 发出相应的中断信号，这时 CPU 会暂停执行下一条即将要执行的指令，转到与中断信号对应的处理程序去执行，也就是切换到了内核态

#### 哪些事情可以在用户态做，哪些事情可以在内核态做

- 内核态相当于一个介于硬件与应用之间的层，内核有ring 0的权限，可以执行任何cpu指令，也可以引用任何内存地址，包括外围设备, 例如硬盘, 网卡，权限等级最高。
- 用户态则权利有限，例如在内存分配中，有一部分内存是仅为内核态使用的，用户态code则不允许访问那些内存地址，每个进程只允许访问自己申请到的内存。而且不允许访问外围设备。另外在执行cpu指令的时候也可以被高优先级抢占。

- 大多数时间各类程序都是执行在用户态下，毕竟内核就是基础而已。





## 三、计算机网络

### 基础模型等

#### OSI 7 层 网络模型

- 应用层，负责给应用程序提供统一的接口；
- 表示层，负责把数据转换成兼容另一个系统能识别的格式；
- 会话层，负责建立、管理和终止表示层实体之间的通信会话；
- 传输层，负责端到端的数据传输；
- 网络层，负责数据的路由、转发、分片；
- 数据链路层，负责数据的封帧和差错检测，以及 MAC 寻址；
- 物理层，负责在物理网络中传输数据帧；

#### TCP/IP 网络模型

- 应用层，负责向用户提供一组应用程序，比如 HTTP、DNS、FTP 等;
- 传输层，负责端到端的通信，比如 TCP、UDP 等；
- 网络层，负责网络包的封装、分片、路由、转发，比如 IP、ICMP 等；
- 网络接口层，负责网络包在物理网络中的传输，比如网络包的封帧、 MAC 寻址、差错检测，以及通过网卡传输网络帧等；

#### 网络层会涉及到什么协议呢

- **网络层协议有：**ARP协议，IP协议，ICMP协议，IGMP协议。
- ARP协议
  - 地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到局域网络上的所有主机，并接收返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。
  - 地址解析协议是建立在网络中各个主机互相信任的基础上的，局域网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时不会检测该报文的真实性就会将其记入本机ARP缓存；由此攻击者就可以向某一主机发送伪ARP应答报文，使其发送的信息无法到达预期的主机或到达错误的主机，这就构成了一个ARP欺骗。
  - ARP命令可用于查询本机ARP缓存中IP地址和MAC地址的对应关系、添加或删除静态对应关系等。相关协议有RARP、代理ARP。NDP用于在IPv6中代替地址解析协议。
- **IP协议：**
  - IP是Internet Protocol（网际互连协议）的缩写，是TCP/IP体系中的网络层协议。设计IP的目的是提高网络的可扩展性：一是解决互联网问题，实现大规模、异构网络的互联互通；二是分割顶层网络应用和底层网络技术之间的耦合关系，以利于两者的独立发展。根据端到端的设计原则，IP只为主机提供一种无连接、不可靠的、尽力而为的数据包传输服务
- **ICMP协议：**
  - ICMP（Internet Control Message Protocol）Internet控制报文协议。它是TCP/IP协议簇的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。
  - ICMP使用IP的基本支持，就像它是一个更高级别的协议，但是，ICMP实际上是IP的一个组成部分，必须由每个IP模块实现。
- **IGMP协议：**
  - 互联网组管理协议（IGMP，Internet Group Management Protocol）是因特网协议家族中的一个组播协议。
  - TCP/IP协议族的一个子协议，用于IP主机向任一个直接相邻的路由器报告他们的组成员情况。允许Internet主机参加多播，也是IP主机用作向相邻多目路由器报告多目组成员的协议。多目路由器是支持组播的路由器，向本地网络发送IGMP查询。主机通过发送IGMP报告来应答查询。组播路由器负责将组播包转发到所有网络中组播成员。　

#### 字节序是什么

- 大于一个字节类型的数据在内存中的存放顺序。是在**跨平台和网络编程中，时常要考虑的问题**

- 字节序经常被分为两类：

  1. Big-Endian（大端，网络）：高位字节排放在内存的低地址端，低位字节排放在内存的高地址端。

  2. Little-Endian（小端，主机）：低位字节排放在内存的低地址端，高位字节排放在内存的高地址端。

- 高低地址与高低字节：

  - 内存的空间布局大致如下：

    最高内存地址 0xFFFFFFFF -> 栈区（从高内存地址，往 低内存地址发展。即栈底在高地址，栈顶在低地址） -> 堆区（从低内存地址 ，往 高内存地址发展）-> 全局区（常量和全局变量）-> 代码区-> 最低内存地址 0x00000000

  - 在十进制中靠左边的是高位，靠右边的是低位，在其他进制也是如此。例如 0x12345678，从高位到低位的字节依次是0x12、0x34、0x56和0x78。

    - 网络字节序 就是 大端字节序：4个字节的32 bit值以下面的次序传输，首先是0～7bit，其次8～15bit，然后16～23bit，最后是24~31bit
    - 主机字节序 就是 小端字节序，现代PC大多采用小端字节序
  
- 计算机处理字节序的时候，不知道什么是高位字节，什么是低位字节。它只知道按顺序读取字节，先读第一个字节，再读第二个字节…如果是大端字节序，先读到的就是高位字节，后读到的就是低位字节；小端字节序正好相反。为何还要弄出个小端序？

  - 这是因为计算机电路先处理低位字节，效率比较高，因为计算都是从低位开始的，所以计算机的内部处理都是小端字节序。
  - 但是人类还是习惯读写大端字节序，所以，除了计算机的内部处理，其他的场合几乎都是大端字节序，比如网络传输和文件储存

- 如果C++程序通过socket将变量a = 0x12345678的首地址传递给了Java程序，由于Java采取Big Endian方式存取数据，很自然的它会将你的数据解析为0x78563412，这样问题就出现了

  - C++提供了相应的函数接口，htonl、htons用于主机序转换到网络序，ntohl、ntohs用于网络序转换到主机序。htons用于主机序转换到网络序，ntohl、ntohs用于网络序转换到主机序。其中h表示host主机，n表示network网络。




#### 键入网址到网页显示，期间发生了什么？

- 浏览器做的第一步工作是解析 URL，浏览器确定了 Web 服务器和文件名，接下来就是根据这些信息来生成 HTTP 请求消息，然后委托操作系统将消息发送给 `Web` 服务器
- 下一步是通过`DNS` 服务器**查询服务器域名对应的 IP 地址**，浏览器缓存 -> 系统本地缓存 -> host文件 -> 本地DNS -> 根DNS -> 顶级域名服务器(.com) -> 权威域名服务器
- 然后应用程序（浏览器）通过调用 Socket 库，来委托协议栈工作。
- 协议栈的上半部分有两块，分别是负责收发数据的 TCP 和 UDP 协议，这两个传输协议会接受应用层的委托执行收发数据的操作
- 协议栈的下面一半是用 IP 协议控制网络包收发操作，在互联网上传数据时，数据会被切分成一块块的网络包，而将网络包发送给对方的操作就是由 IP 负责的；根据**路由表**规则，来判断哪一个网卡作为源地址 IP
- 生成了 IP 头部之后，接下来网络包还需要在 IP 头部的前面加上 **MAC 头部**。在 MAC 包头里需要**发送方 MAC 地址**和**接收方目标 MAC 地址**，用于**两点之间的传输**，**接收方**的 MAC 地址需要 `ARP` 协议帮我们找到路由器的 MAC 地址
- 网络包只是存放在内存中的一串二进制数字信息，通过网卡将**数字信息转换为电信号**在网线上传输，网卡会在其**开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列**
- 交换机的设计是将网络包**原样**转发到目的地。交换机工作在 MAC 层，也称为**二层网络设备**，交换机根据 MAC 地址表查找 MAC 地址，然后将信号发送到相应的端口
- 网络包经过交换机之后，现在到达了**路由器**，并在此被转发到下一个路由器或目标设备。**路由器**是基于 IP 设计的，俗称**三层**网络设备，路由器的各个端口都具有 MAC 地址和 IP 地址；检查 MAC 头部中的**接收方 MAC 地址**，看看是不是发给自己的包，如果是就放到接收缓冲区中，否则就丢弃这个包
- 数据包抵达服务器后，服务器会先扒开数据包的 MAC 头部，查看是否和服务器自己的 MAC 地址符合，符合就将包收起来。接着继续扒开数据包的 IP 头，发现 IP 地址符合，根据 IP 头中协议项，知道自己上层是 TCP 协议。于是，扒开 TCP 的头，里面有序列号，需要看一看这个序列包是不是我想要的，如果是就放入缓存中然后返回一个 ACK，如果不是就丢弃。TCP头部里面还有端口号， HTTP 的服务器正在监听这个端口号。于是，服务器自然就知道是 HTTP 进程想要这个包，于是就将包发给 HTTP 进程。服务器的 HTTP 进程看到，原来这个请求是要访问一个页面，于是就把这个网页封装在 HTTP 响应报文里。HTTP 响应报文也需要穿上 TCP、IP、MAC 头部，不过这次是源地址是服务器 IP 地址，目的地址是客户端 IP 地址

#### 交换机是什么？

- 交换机的设计是将网络包**原样**转发到目的地。交换机工作在 MAC 层，也称为**二层网络设备**。
- 信号到达网线接口，交换机里的模块进行接收，接下来交换机里的模块将电信号转换为数字信号。然后通过包末尾的 `FCS` 校验错误，如果没问题则放到缓冲区。这部分操作基本和计算机的网卡相同，但交换机的工作方式和网卡不同。计算机的网卡本身具有 MAC 地址，并通过核对收到的包的接收方 MAC 地址判断是不是发给自己的，如果不是发给自己的则丢弃；相对地，交换机的端口不核对接收方 MAC 地址，而是直接接收所有的包并存放到缓冲区中。因此，和网卡不同，**交换机的端口不具有 MAC 地址**。
- 将包存入缓冲区后，接下来需要查询一下这个包的接收方 MAC 地址是否已经在 MAC 地址表中有记录了。交换机的 MAC 地址表主要包含两个信息：
  - 一个是设备的 MAC 地址，
  - 另一个是该设备连接在交换机的哪个端口上。
- **交换机根据 MAC 地址表查找 MAC 地址，然后将信号发送到相应的端口**。
  - 地址表中找不到指定的 MAC 地址。这可能是因为具有该地址的设备还没有向交换机发送过包，或者这个设备一段时间没有工作导致地址被从地址表中删除了。
  - 这种情况下，交换机无法判断应该把包转发到哪个端口，只能将包转发到除了源端口之外的所有端口上，无论该设备连接在哪个端口上都能收到这个包。

#### 路由器和交换机的区别

- 网络包经过交换机之后，现在到达了**路由器**，并在此被转发到下一个路由器或目标设备。这一步转发的工作原理和交换机类似，也是通过查表判断包转发的目标。不过在具体的操作过程上，路由器和交换机是有区别的。
  - 因为**路由器**是基于 IP 设计的，俗称**三层**网络设备，路由器的各个端口都具有 MAC 地址和 IP 地址；
  - 而**交换机**是基于以太网设计的，俗称**二层**网络设备，交换机的端口不具有 MAC 地址

#### 路由器的工作原理

- 路由器的端口具有 MAC 地址，因此它就能够成为以太网的发送方和接收方；同时还具有 IP 地址，从这个意义上来说，它和计算机的网卡是一样的。当转发包时，首先路由器端口会接收发给自己的以太网包，然后**路由表**查询转发目标，再由相应的端口作为发送方将以太网包发送出去。
- 电信号到达网线接口部分，路由器中的模块会将电信号转成数字信号，然后通过包末尾的 `FCS` 进行错误校验。如果没问题则检查 MAC 头部中的**接收方 MAC 地址**，看看是不是发给自己的包，如果是就放到接收缓冲区中，否则就丢弃这个包。总的来说，路由器的端口都具有 MAC 地址，只接收与自身地址匹配的包，遇到不匹配的包则直接丢弃
- 完成包接收操作之后，路由器就会**去掉**包开头的 MAC 头部。**MAC 头部的作用就是将包送达路由器**，其中的接收方 MAC 地址就是路由器端口的 MAC 地址。因此，当包到达路由器之后，MAC 头部的任务就完成了，于是 MAC 头部就会**被丢弃**。
- 接下来，路由器会根据 MAC 头部后方的 `IP` 头部中的内容进行包的转发操作。
  - 转发操作分为几个阶段，首先是查询**路由表**判断转发目标。根据包的接收方 IP 地址查询路由表中的目标地址栏，以找到相匹配的记录。每个条目的子网掩码和 `192.168.1.100` IP 做 **& 与运算**后，得到的结果与对应条目的目标地址进行匹配，如果匹配就会作为候选转发目标，如果不匹配就继续与下个条目进行路由匹配。实在找不到匹配路由时，就会选择**默认路由**，路由表中子网掩码为 `0.0.0.0` 的记录表示「默认路由」
- **路由器的发送操作**：根据**路由表的网关列**判断对方的地址。
  - 如果网关是一个 IP 地址，则这个IP 地址就是我们要转发到的目标地址，**还未抵达终点**，还需继续需要路由器转发。
  - 如果网关为空，则 IP 头部中的接收方 IP 地址就是要转发到的目标地址，也是就终于找到 IP 包头里的目标地址了，说明**已抵达终点**。
- 知道对方的 IP 地址之后，接下来需要通过 `ARP` 协议根据 IP 地址查询 MAC 地址，并将查询的结果作为接收方 MAC 地址。路由器也有 ARP 缓存，因此首先会在 ARP 缓存中查询，如果找不到则发送 ARP 查询请求。接下来是发送方 MAC 地址字段，这里填写输出端口的 MAC 地址。还有一个以太类型字段，填写 `0800` （十六进制）表示 IP 协议。网络包完成后，接下来会将其转换成电信号并通过端口发送出去。这一步的工作过程和计算机也是相同的。
- 发送出去的网络包会通过**交换机**到达下一个路由器。由于接收方 MAC 地址就是下一个路由器的地址，所以交换机会根据这一地址将包传输到下一个路由器。接下来，下一个路由器会将包转发给再下一个路由器，经过层层转发之后，网络包就到达了最终的目的地。
- **源 IP 和目标 IP 始终是不会变的，一直变化的是 MAC 地址**，因为需要 MAC 地址在以太网内进行**两个设备**之间的包传输

#### NAT 基本原理

- NAT的基本工作原理是，当私有网主机和公共网主机通信的IP包经过NAT网关时，将IP包中的源IP或目的IP在私有IP和NAT的公共IP之间进行转换。
- NAT技术无可否认是在ipv4地址资源的短缺时候起到了缓解作用；在减少用户申请ISP服务的花费和提供比较完善的负载平衡功能等方面带来了不少好处。但是在ipv4地址在以后几年将会枯竭，NAT技术不能改变ip地址空间不足的本质。然而在安全机制上也潜在着威胁，在配置和管理上也是一个挑战。如果要从根本上解决ip地址资源的问题，ipv6才是最根本之路。在ipv4转换到ipv6的过程中，NAT技术确实是一个不错的选择，相对其他的方案优势也非常明显。

### TCP 基本认识

#### TCP 头格式有哪些？

- **序列号**：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「**数据字节数**」的大小。**用来解决网络包乱序问题。**
- **确认应答号**：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。**用来解决丢包的问题。**
- **控制位：**
  - *ACK*：该位为 `1` 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 `SYN` 包之外该位必须设置为 `1` 。
  - *RST*：该位为 `1` 时，表示 TCP 连接中出现异常必须强制断开连接。
  - *SYN*：该位为 `1` 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。
  - *FIN*：该位为 `1` 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 `FIN` 位为 1 的 TCP 段

#### 为什么需要 TCP 协议？ TCP 工作在哪一层？

- `IP` 层是「不可靠」的，它不保证网络包的交付、不保证网络包的按序交付、也不保证网络包中的数据的完整性。如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 `TCP` 协议来负责。
- TCP 是一个工作在**传输层**的**可靠**数据传输的服务，它能确保接收端接收的网络包是**无损坏、无间隔、非冗余和按序的**

#### 什么是 TCP ？

TCP 是**面向连接的、可靠的、基于字节流**的传输层通信协议。

- **面向连接**：一定是「一对一」才能连接，不能像 UDP 协议可以一个主机同时向多个主机发送消息，也就是一对多是无法做到的；
- **可靠的**：无论的网络链路中出现了怎样的链路变化，TCP 都可以保证一个报文一定能够到达接收端；
- **字节流**：用户消息通过 TCP 协议传输时，消息可能会被操作系统分组成多个的 TCP 报文，如果接收方的程序如果不知道消息的边界是无法读出一个有效的用户消息的。并且 TCP 报文是有序的，当前一个TCP 报文没有收到的时候，即使它先收到了后面的 TCP 报文，那么也不能扔给应用层去处理，同时对重复的 TCP 报文会自动丢弃

#### 什么是 TCP 连接？

- 简单来说就是，**用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗口大小称为连接**，建立一个 TCP 连接是需要客户端与服务器端达成上述三个信息的共识
  - **Socket**：由 IP 地址和端口号组成
  - **序列号**：用来解决乱序问题等
  - **窗口大小**：用来做流量控制(流量控制是作用于接收者的)

#### 如何唯一确定一个 TCP 连接呢？

- TCP 四元组可以唯一的确定一个连接，四元组包括如下：源地址、源端口、目的地址、目的端口
- 源地址和目的地址的字段（32位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。源端口和目的端口的字段（16位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程

#### 有一个 IP 的服务器监听了一个端口，它的 TCP 的最大连接数是多少？

- 服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，客户端 IP 和 端口是可变的
- 对 IPv4，客户端的 IP 数最多为 `2` 的 `32` 次方，客户端的端口数最多为 `2` 的 `16` 次方，也就是服务端单机最大 TCP 连接数，约为 `2` 的 `48` 次方。
- 当然，服务端最大并发 TCP 连接数远不能达到理论上限，会受以下因素影响：
  - 文件描述符限制，每个 TCP 连接都是一个文件，如果文件描述符被占满了，会发生 too many open files。Linux 对可打开的文件描述符的数量分别作了三个方面的限制：**系统级**，当前系统可打开的最大数量；**用户级**，指定用户可打开的最大数量；**进程级**，单个进程可打开的最大数量
  - **内存限制**，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的，如果内存资源被占满后，会发生 OOM。

#### UDP 和 TCP 有什么区别呢？分别的应用场景是？

- UDP 不提供复杂的控制机制，利用 IP 提供面向「无连接」的通信服务。UDP 协议头部只有 `8` 个字节（ 64 位），包含如下信息：
  - 目标和源端口：主要是告诉 UDP 协议应该把报文发给哪个进程。
  - 包长度：该字段保存了 UDP 首部的长度跟数据的长度之和。
  - 校验和：校验和是为了提供可靠的 UDP 首部和数据而设计，防止收到在网络传输中受损的 UDP包。
- **TCP 和 UDP 区别：**
  - *连接*：TCP 是面向连接的传输层协议，传输数据前先要建立连接；UDP 是不需要连接，即刻传输数据。
  - *服务对象*：TCP 是一对一的两点服务，即一条连接只有两个端点；UDP 支持一对一、一对多、多对多的交互通信
  - *可靠性*：TCP 是可靠交付数据的，数据可以无差错、不丢失、不重复、按序到达；UDP 是尽最大努力交付，不保证可靠交付数据
  - *拥塞控制、流量控制*：TCP 有拥塞控制和流量控制机制，保证数据传输的安全性；UDP 则没有，即使网络非常拥堵了，也不会影响 UDP 的发送速率
  - *首部开销*：TCP 首部长度较长，会有一定的开销，首部在没有使用「选项」字段时是 `20` 个字节，如果使用了「选项」字段则会变长的；UDP 首部只有 8 个字节，并且是固定不变的，开销较小
  - *传输方式*：TCP 是流式传输，没有边界，但保证顺序和可靠。UDP 是一个包一个包的发送，是有边界的，但可能会丢包和乱序
  - *分片不同*：TCP 的数据大小如果大于 MSS 大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。UDP 的数据大小如果大于 MTU 大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。
- **TCP 和 UDP 应用场景：**
  - 由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于：`FTP` 文件传输；HTTP / HTTPS；
  - 由于 UDP 面向无连接，它可以随时发送数据，再加上UDP本身的处理既简单又高效，因此经常用于：包总量较少的通信，如 `DNS` 、`SNMP` 等；视频、音频等多媒体通信；广播通信；

#### 怎么使得UDP具备可靠的性质

基于 UDP 实现的可靠传输协议，那么就要在应用层下功夫，也就是要设计好协议的头部字段，QUIC就是基于UDP实现的可靠传输协议

- Packet Header 首次建立连接时和日常传输数据时使用的 Header 是不同的; Packet Header 细分这两种：
  - Long Packet Header 用于首次建立连接。
  - Short Packet Header 用于日常传输数据。不需要在传输 Source Connection ID 字段了，只需要传输 Destination Connection ID。
- Short Packet Header 中的 `Packet Number` 是每个报文独一无二的编号，它是**严格递增**的，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值。
  - 解决了 TCP 重传的歧义问题从而导致的RTT计算不准的问题；ACK 的 Packet Number 是 N+M，就根据重传报文计算采样 RTT。如果 ACK 的 Pakcet Number 是 N，就根据原始报文的时间计算采样 RTT，没有歧义性的问题。
  - **QUIC 使用的 Packet Number 单调递增的设计，可以让数据包不再像 TCP 那样必须有序确认，QUIC 支持乱序确认，当数据包Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会继续向右滑动**
  - 待发送端获知数据包Packet N 丢失后，会将需要重传的数据包放到待发送队列，重新编号比如数据包Packet N+M 后重新发送给接收端，对重传数据包的处理跟发送新的数据包类似，这样就不会因为丢包重传将当前窗口阻塞在原地，从而解决了队头阻塞问题。
- 一个 Packet 报文中可以存放多个 QUIC Frame。每一个 Frame 都有明确的类型，针对类型的不同，功能也不同，自然格式也不同。引入 Frame Header 这一层，**通过 Stream ID + Offset 字段信息实现数据的有序性**，通过比较两个数据包的 Stream ID 与 Stream Offset ，如果都是一致，就说明这两个数据包的内容一致。
- QUIC 也是需要三次握手来建立连接的，主要目的是为了协商连接 ID。协商出连接 ID 后，后续传输时，双方只需要固定住连接 ID，从而实现连接迁移功能。

#### 为什么 UDP 头部没有首部长度字段，而 TCP 头部有首部长度字段呢？

- 原因是 TCP 有**可变长**的「选项」字段，而 UDP 头部长度则是**不会变化**的，无需多一个字段去记录 UDP 的首部长度。

#### 为什么 UDP 头部有包长度字段，而 TCP 头部则没有包长度字段呢？

- TCP 是如何计算负载数据长度：TCP数据总长度 = IP 总长度 - IP 首部长度 -TCP 首部长度 ；其中 IP 总长度 和 IP 首部长度，在 IP 首部格式是已知的。TCP 首部长度，则是在 TCP 首部格式已知的，所以就可以求得 TCP 数据的长度。
- UDP 也是基于 IP 层的呀，那 UDP 的数据长度也可以通过这个公式计算；这么一问，确实感觉 UDP 「包长度」是冗余的。估计是**因为为了网络设备硬件设计和处理方便，首部长度需要是 `4`字节的整数倍**









### TCP连接相关

#### TCP讲一下三次握手和四次挥手

三次握手：

- 一开始，客户端和服务端都处于 `CLOSED` 状态。先是服务端主动监听某个端口，处于 `LISTEN` 状态；随后客户端需要发起连接，其发送一个SYN报文， 在TCP 首部的「序号」字段中填入一个随机初始化序号，同时把 `SYN` 标志位置为 `1`，之后客户端处于SYN_SENT的状态；
- 然后服务端收到客户端的SYN报文后，生成一个SYN + ACK报文，其也产生自己的一个随机化序号填入首部「序号」字段中，TCP 首部的「确认应答号」字段填入 收到的syn + 1，并且将SYN和ACK标志位置为1，随后处于SYN_RCVD状态；
- 客户端若收到服务端的SYN+ACK报文，那么会产生一个ACK报文，其可以包含数据了，将报文的ACK标志位置为1即可，且ack num = 收到的syn + 1,之后客户端处于 `ESTABLISHED` 状态。

四次挥手：双方都可以主动断开连接，断开连接后主机中的「资源」将被释放；

- 客户端打算关闭连接，此时会发送一个 TCP 首部 `FIN` 标志位被置为 `1` 的报文，也即 `FIN` 报文，之后客户端进入 `FIN_WAIT_1` 状态。
- 服务端收到该报文后，就向客户端发送 `ACK` 应答报文，接着服务端进入 `CLOSED_WAIT` 状态。客户端收到服务端的 `ACK` 应答报文后，之后进入 `FIN_WAIT_2` 状态。
- **等待服务端处理完数据后**，也向客户端发送 `FIN` 报文，之后服务端进入 `LAST_ACK` 状态
- 客户端收到服务端的 `FIN` 报文后，回一个 `ACK` 应答报文，之后进入 `TIME_WAIT` 状态。服务器收到了 `ACK` 应答报文后，就进入了 `CLOSED` 状态，至此服务端已经完成连接的关闭。客户端在经过 `2MSL` 一段时间后，自动进入 `CLOSED` 状态，至此客户端也完成连接的关闭

#### listen底层实现有了解吗

Linux内核中会维护两个队列：

- 半连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态；
- 全连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态；

```c
int listen (int socketfd, int backlog)
```

- 参数一 socketfd 为 socketfd 文件描述符
- 参数二 backlog，这参数在历史版本有一定的变化

在早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。

在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，**所以现在通常认为 backlog 是 accept 队列。**

**但是上限值是内核参数 somaxconn 的大小，也就说 accpet 队列长度 = min(backlog, somaxconn)。**

#### 为什么要三次握手

- **三次握手才可以阻止重复历史连接的初始化**（主要原因）：在两次握手的情况下，被动发起方没有中间状态给主动发起方来阻止历史连接，导致被动发起方**可能建立一个历史连接，造成资源浪费**；要解决这种现象，最好就是在被动发起方发送数据前阻止掉历史连接，这样就不会造成资源浪费，而要实现这个功能，就需要三次握手
- **三次握手才可以同步双方的初始序列号**：两次握手只保证了一方的初始序列号能被对方成功接收，没办法保证双方的初始序列号都能被确认接收
- **三次握手才可以避免资源浪费**：如果只有两次握手，当客户端的 `SYN` 阻塞，重复发送多次 `SYN` 报文，那么服务器在收到请求后就会**建立多个冗余的无效链接，造成不必要的资源浪费**

总结：TCP 建立连接时，通过三次握手**能防止历史连接的建立，能减少双方不必要的资源开销，能帮助双方同步初始化序列号**。序列号能够保证数据包不重复、不丢弃和按序传输。不使用两次握手和四次握手的原因：

- 两次握手：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；
- 四次握手：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数

#### 为什么每次建立 TCP 连接时，初始化的序列号都要求不一样呢？

- 为了防止历史报文被下一个相同四元组的连接接收（主要方面）：如果每次建立连接客户端和服务端的初始化序列号都「不一样」，就有大概率因为历史报文的序列号「不在」对方接收窗口，从而很大程度上避免了历史报文
- 为了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收；

#### 初始序列号 ISN 是如何随机产生的？

- 起始 `ISN` 是基于时钟的，每 4 微秒 + 1，转一圈要 4.55 个小时
- ISN 随机生成算法：ISN = M + F(localhost, localport, remotehost, remoteport)。
  - `M` 是一个计时器，这个计时器每隔 4 微秒加 1。
  - `F` 是一个 Hash 算法，根据源 IP、目的 IP、源端口、目的端口生成一个随机数值

#### 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？

- `MTU`：一个网络包的最大长度，以太网中一般为 `1500` 字节；
- `MSS`：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度；

- 如果在 TCP 的整个报文（头部 + 数据）交给 IP 层进行分片，当 IP 层有一个超过 `MTU` 大小的数据（TCP 头部 + TCP 数据）要发送，那么 IP 层就要进行分片，把数据分片成若干片，保证每一个分片都小于 MTU。把一份 IP 数据报进行分片以后，由目标主机的 IP 层来进行重新组装后，再交给上一层 TCP 传输层。这看起来井然有序，但这存在隐患的，**那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传**。因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。当接收方发现 TCP 报文（头部 + 数据）的某一片丢失后，则不会响应 ACK 给对方，那么发送方的 TCP 在超时后，就会重发「整个 TCP 报文（头部 + 数据）」。因此，可以得知由 IP 层进行分片传输，是非常没有效率的
- 为了达到最佳的传输效能 TCP 协议在**建立连接的时候通常要协商双方的 MSS 值**，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了；经过 TCP 层分片后，如果一个 TCP 分片丢失后，**进行重发时也是以 MSS 为单位**，而不用重传所有的分片，大大增加了重传的效率

#### 第一次握手丢失了，会发生什么？

- 如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手），就会触发「超时重传」机制，重传 SYN 报文，而且**重传的 SYN 报文的序列号都是一样的**
- 客户端的 SYN 报文最大重传次数由 `tcp_syn_retries`内核参数控制，这个参数是可以自定义的，默认值一般是 5；**每次超时的时间是上一次的 2 倍**，如果还是没能收到服务端的第二次握手，那么客户端就会断开连接

#### 第二次握手丢失了，会发生什么？

- 第二次握手的 `SYN-ACK` 报文其实有两个目的 ：
  - 第二次握手里的 ACK， 是对第一次握手的确认报文；
  - 第二次握手里的 SYN，是服务端发起建立 TCP 连接的报文；
- 第二次握手报文里是包含对客户端的第一次握手的 ACK 确认报文，所以，如果客户端迟迟没有收到第二次握手，那么客户端就觉得可能自己的 SYN 报文（第一次握手）丢失了，于是**客户端就会触发超时重传机制，重传 SYN 报文**
- 第二次握手中包含服务端的 SYN 报文，所以当客户端收到后，需要给服务端发送 ACK 确认报文（第三次握手），服务端才会认为该 SYN 报文被客户端收到了。如果第二次握手丢失了，服务端就收不到第三次握手，于是**服务端这边会触发超时重传机制，重传 SYN-ACK 报文**。

#### 第三次握手丢失了，会发生什么？

- 第三次握手的 ACK 是对第二次握手的 SYN 的确认报文，所以当第三次握手丢失了，如果服务端那一方迟迟收不到这个确认报文，就会触发超时重传机制，重传 SYN-ACK 报文，直到收到第三次握手，或者达到最大重传次数
- **ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文**

#### 什么是 SYN 攻击？如何避免 SYN 攻击？

-  TCP 连接建立是需要三次握手，假设攻击者短时间伪造不同 IP 地址的 `SYN` 报文，服务端每接收到一个 `SYN` 报文，就进入`SYN_RCVD` 状态，但服务端发送出去的 `ACK + SYN` 报文，无法得到未知 IP 主机的 `ACK` 应答，久而久之就会**占满服务端的半连接队列**，使得服务器不能为正常用户服务
-  避免 SYN 攻击方式，可以有以下四种方法：
   - 调大 netdev_max_backlog：当网卡接收数据包的速度大于内核处理的速度时，会有一个队列保存这些数据包
   - 增大 TCP 半连接队列：需要同时增大下面这三个参数：
     - 增大 net.ipv4.tcp_max_syn_backlog
     - 增大 listen() 函数中的 backlog
     - 增大 net.core.somaxconn
   - 开启 tcp_syncookies：开启 syncookies 功能就可以在不使用 SYN 半连接队列的情况下成功建立连接，相当于绕过了 SYN 半连接来建立连接：
     - 当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不会丢弃，而是根据算法，计算出一个 `cookie` 值；
     - 将 cookie 值放到第二次握手报文的「序列号」里，然后服务端回第二次握手给客户端；
     - 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，将该连接对象放入到「 Accept 队列」
   - 减少 SYN+ACK 重传次数，以加快处于 SYN_REVC 状态的 TCP 连接断开。

#### 为什么挥手需要四次？

- 关闭连接时，客户端向服务端发送 `FIN` 时，仅仅表示客户端不再发送数据了但是还能接收数据。
- 服务器收到客户端的 `FIN` 报文时，先回一个 `ACK` 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 `FIN` 报文给客户端来表示同意现在关闭连接。
- 服务端通常需要等待完成数据的发送和处理，所以服务端的 `ACK` 和 `FIN` 一般都会分开发送，因此是需要四次挥手。但是**在特定情况下，四次挥手是可以变成三次挥手的**：当被动关闭方在 TCP 挥手过程中，如果「没有数据要发送」，同时「没有开启 TCP_QUICKACK（默认情况就是没有开启，没有开启 TCP_QUICKACK，等于就是在使用 TCP 延迟确认机制）」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。出现三次挥手现象是因为 TCP 延迟确认机制导致的

#### 第一次挥手丢失了，会发生什么？

- 正常情况下，如果能及时收到服务端（被动关闭方）的 ACK，则会很快变为 `FIN_WAIT2`状态。如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN 报文，重发次数由 `tcp_orphan_retries` 参数控制。
- 当客户端重传 FIN 报文的次数超过 `tcp_orphan_retries` 后，就不再发送 FIN 报文，则会在等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到第二次挥手，那么直接进入到 `close` 状态

#### 第二次挥手丢失了，会发生什么？

- ACK 报文是不会重传的，所以如果服务端的第二次挥手丢失了，客户端就会触发超时重传机制，重传 FIN 报文，直到收到服务端的第二次挥手，或者达到最大的重传次数
- 当客户端收到第二次挥手，也就是收到服务端发送的 ACK 报文后，客户端就会处于 `FIN_WAIT2` 状态，在这个状态需要等服务端发送第三次挥手，也就是服务端的 FIN 报文。
- 对于 close 函数关闭的连接，由于无法再发送和接收数据，所以`FIN_WAIT2` 状态不可以持续太久，而 `tcp_fin_timeout` 控制了这个状态下连接的持续时长，默认值是 60 秒。这意味着对于调用 close 关闭的连接，如果在 60 秒后还没有收到 FIN 报文，客户端（主动关闭方）的连接就会直接关闭
- 如果主动关闭方使用 shutdown 函数关闭连接，指定了只关闭发送方向，而接收方向并没有关闭，那么意味着主动关闭方还是可以接收数据的。此时，如果主动关闭方一直没收到第三次挥手，那么主动关闭方的连接将会一直处于 `FIN_WAIT2` 状态（`tcp_fin_timeout` 无法控制 shutdown 关闭的连接）

#### 第三次挥手丢失了，会发生什么？

- 当服务端（被动关闭方）收到客户端（主动关闭方）的 FIN 报文后，内核会自动回复 ACK，同时连接处于 `CLOSE_WAIT` 状态，顾名思义，它表示等待应用进程调用 close 函数关闭连接。此时，内核是没有权利替代进程关闭连接，必须由进程主动调用 close 函数来触发服务端发送 FIN 报文。
- 服务端处于 CLOSE_WAIT 状态时，调用了 close 函数，内核就会发出 FIN 报文，同时连接进入 LAST_ACK 状态，等待客户端返回 ACK 来确认连接关闭。如果迟迟收不到这个 ACK，服务端就会重发 FIN 报文，重发次数仍然由 `tcp_orphan_retrie`s 参数控制，这与客户端重发 FIN 报文的重传次数控制方式是一样的

#### 第四次挥手丢失了，会发生什么？

- 当客户端收到服务端的第三次挥手的 FIN 报文后，就会回 ACK 报文，也就是第四次挥手，此时客户端连接进入 `TIME_WAIT` 状态。在 Linux 系统，TIME_WAIT 状态会持续 2MSL 后才会进入关闭状态。
- 然后，服务端（被动关闭方）没有收到 ACK 报文前，还是处于 LAST_ACK 状态。如果第四次挥手的 ACK 报文没有到达服务端，服务端就会重发 FIN 报文，重发次数仍然由前面介绍过的 `tcp_orphan_retries` 参数控制

#### 为什么 TIME_WAIT 等待的时间是 2MSL？

- `MSL` 是 Maximum Segment Lifetime，**报文最大生存时间**，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 `TTL` 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机
- MSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 **MSL 应该要大于等于 TTL 消耗为 0 的时间**，以确保报文已被自然消亡。**TTL 的值一般是 64，Linux 将 MSL 设置为 30 秒，意味着 Linux 认为数据报文经过 64 个路由器的时间不会超过 30 秒，如果超过了，就认为报文已经消失在网络中了**
- TIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以**一来一回需要等待 2 倍的时间**
- **2MSL时长** 这其实是相当于**至少允许报文丢失一次**。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内到达，TIME_WAIT 状态的连接可以应对；为什么不是 4 或者 8 MSL 的时长呢？一个丢包率达到百分之一的糟糕网络，连续两次丢包的概率只有万分之一，这个概率实在是太小了，忽略它比解决它更具性价比
- `2MSL` 的时间是从**客户端接收到 FIN 后发送 ACK 开始计时的**。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 **2MSL 时间将重新计时**

#### 为什么需要 TIME_WAIT 状态？

- 防止历史连接中的数据，被后面相同四元组的连接错误的接收；
  - **序列号是一个 32 位的无符号数，因此在到达 4G 之后再循环回到 0**；初始化序列号可被视为一个 32 位的计数器，该计数器的数值每 4 微秒加 1，循环一次需要 4.55 小时；
  - **序列号和初始化序列号并不是无限递增的，会发生回绕为初始值的情况，这意味着无法根据序列号来判断新老数据**
  - 为了防止历史连接中的数据，被后面相同四元组的连接错误的接收，因此 TCP 设计了 TIME_WAIT 状态，状态会持续 `2MSL` 时长，这个时间**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的**
- 保证「被动关闭连接」的一方，能被正确的关闭
  - TIME-WAIT 作用是**等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭**
  - 假设客户端没有 TIME_WAIT 状态，而是在发完最后一次回 ACK 报文就直接进入 CLOSE 状态，如果该 ACK 报文丢失了，服务端则重传的 FIN 报文，而这时客户端已经进入到关闭状态了，在收到服务端重传的 FIN 报文后，就会回 RST 报文

#### TIME_WAIT 过多有什么危害？

- 第一是占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等；
- 第二是占用端口资源，端口资源也是有限的，一般可以开启的端口为 `32768～61000`，也可以通过 `net.ipv4.ip_local_port_range`参数指定范围。
  - **如果客户端（发起连接方）的 TIME_WAIT 状态过多**，占满了所有端口资源，那么就无法对「目的 IP+ 目的 PORT」都一样的服务器发起连接了，但是被使用的端口，还是可以继续对另外一个服务器发起连接的
  - **如果服务端（发起连接方）的 TIME_WAIT 状态过多**，并不会导致端口资源受限，因为服务端只监听一个端口，而且由于一个四元组唯一确定一个 TCP 连接，因此理论上服务端可以建立很多连接

#### 如何优化 TIME_WAIT？

- 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；
  - 可以**复用处于 TIME_WAIT 的 socket 为新的连接所用**。有一点需要注意的是，**tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用**
  - 由于引入了时间戳，在前面提到的 `2MSL` 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃
- net.ipv4.tcp_max_tw_buckets
  - 这个值默认为 18000，**当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置**，这个方法比较暴力
- 程序中使用 SO_LINGER ，应用强制使用 RST 关闭
  - 通过设置 socket 选项，来设置调用 close 关闭连接行为。如果`l_onoff`为非 0， 且`l_linger`值为 0，那么调用`close`后，会立该发送一个`RST`标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了`TIME_WAIT`状态，直接关闭。
  - 但这为跨越`TIME_WAIT`状态提供了一个可能，不过是一个非常危险的行为，不值得提倡
- 如果服务端要避免过多的 TIME_WAIT 状态的连接，就永远不要主动断开连接，让客户端去断开，由分布在各处的客户端去承受 TIME_WAIT

#### 如果已经建立了连接，但是客户端突然出现故障了怎么办？

- TCP 有一个机制是**保活机制**：定义一个时间段，在这个时间段内，如果没有任何连接相关的活动，TCP 保活机制会开始作用，每隔一个时间间隔，发送一个探测报文，该探测报文包含的数据非常少，如果连续几个探测报文都没有得到响应，则认为当前的 TCP 连接已经死亡，系统内核将错误信息通知给上层应用程序
- 应用程序若想使用 TCP 保活机制需要通过 socket 接口设置 `SO_KEEPALIVE` 选项才能够生效，如果没有设置，那么就无法使用 TCP 保活机制
- 如果开启了 TCP 保活，需要考虑以下几种情况：
  - 第一种，对端程序是正常工作的。当 TCP 保活的探测报文发送给对端, 对端会正常响应，这样 **TCP 保活时间会被重置**，等待下一个 TCP 保活时间的到来。
  - 第二种，对端程序崩溃并重启。当 TCP 保活的探测报文发送给对端后，对端是可以响应的，但由于没有该连接的有效信息，**会产生一个 RST 报文**，这样很快就会发现 TCP 连接已经被重置。
  - 第三种，是对端程序崩溃，或对端由于其他原因导致报文不可达。当 TCP 保活的探测报文发送给对端后，石沉大海，没有响应，连续几次，达到保活探测次数后，**TCP 会报告该 TCP 连接已经死亡**
- 可以自己在应用层实现一个心跳机制：比如，web 服务软件一般都会提供 `keepalive_timeout` 参数，用来指定 HTTP 长连接的超时时间。如果设置了 HTTP 长连接的超时时间是 60 秒，web 服务软件就会**启动一个定时器**，如果客户端在完成一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，**定时器的时间一到，就会触发回调函数来释放该连接。**

#### 如果已经建立了连接，但是服务端的进程崩溃会发生什么？

- TCP 的连接信息是由内核维护的，所以当服务端的进程崩溃后，内核需要回收该进程的所有 TCP 连接资源，于是内核会发送第一次挥手 FIN 报文，后续的挥手过程也都是在内核完成，并不需要进程的参与，所以即使服务端的进程退出了，还是能与客户端完成 TCP四次挥手的过程

#### 不存在丢包，也不存在忙于读写的情况，netstat发现close-wait过多，原因是什么？

- close_wait 按照正常操作的话应该很短暂的一个状态，接收到客户端的fin包并且回复客户端ack之后，会继续发送fin包告知客户端关闭连接之后迁移到Last_ACK状态。close_wait过多只能说明没有迁移到Last_ACK，也就是服务端是否发送fin包，只有发送fin包才会发生迁移，所以**问题定位在是否发送fin包**。fin包的底层实现其实就是调用socket的close方法，这里的问题出在**没有执行close方法。说明服务端socket忙于读写。**
- close_wait过多的解决方案：
  - 使用完socket调用close方法；
  - socket读控制，当读取的长度为0时（读到结尾），立即close；

  - 如果read返回-1，出现错误，检查error返回码，有三种情况：INTR（被中断，可以继续读取），WOULDBLOCK（表示当前socket_fd文件描述符是非阻塞的，但是现在被阻塞了），AGAIN（表示现在没有数据稍后重新读取）。如果不是AGAIN，立即close

  - 可以设置TCP的连接时长keep_alive_time还有tcp监控连接的频率以及连接没有活动多长时间被迫断开连接

#### 如果不想通过四次挥手直接关闭连接，怎么做？

- 程序中使用 SO_LINGER，可以通过设置 socket 选项，来设置调用 close 关闭连接行为
- 如果`l_onoff`为非 0， 且`l_linger`值为 0，那么调用`close`后，会立该发送一个`RST`标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了`TIME_WAIT`状态，直接关闭



### TCP传输相关

#### TCP数据流传输如何理解（怎么确定边界）

- **TCP 是面向字节流的协议**：TCP 是流式传输，没有边界，但保证顺序和可靠，不能认为一个用户消息对应一个 TCP 报文；用户消息通过 TCP 传输时，消息可能会被操作系统分组成多个的 TCP 报文，如果接收方的程序如果不知道消息的边界，是无法读出一个有效的用户消息的；TCP 报文是有序的，当前一个TCP 报文没有收到的时候，即使它先收到了后面的 TCP 报文，那么也不能扔给应用层去处理，同时对重复的 TCP 报文会自动丢弃；
- **TCP 粘包问题**：两个消息的某个部分内容被分到同一个 TCP 报文，接收方不知道消息的边界的话，是无法读出有效的消息；如果知道了边界在哪，接收方就可以通过边界来划分出有效的用户消息。
- 一般有**三种方式分包**：**固定长度的消息；特殊字符作为边界（HTTP ）；自定义消息结构**


#### TCP拥塞控制

- TCP的拥塞控制主要是为了避免「发送方」的数据填满整个**网络**；在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大

- 拥塞控制主要是四个算法：慢启动；拥塞避免；拥塞发生；快速恢复

  - 慢启动主要规则：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1；随着拥塞窗口的增大，可以发送的包的个数将称指数型增长，直到到达了慢启动门限 `ssthresh`，（一般来说 `ssthresh` 的大小是 `65535` 字节）超过了之后就开始拥塞避免算法
  - 拥塞控制的规则是：每当收到一个 ACK 时，cwnd 增加 1/cwnd，其变成了线性增长，还是增长阶段，但是增长速度缓慢了一些；就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传
  - 重传机制主要有两种：超时重传；快速重传，分别对应自己的拥塞发生算法
    - 超时重传的拥塞发生算法如下：慢启动门限`ssthresh`变为当前窗口值的一半，同时拥塞窗口值初始化，重新进入慢启动状态。这种方式太激进了，反应也很强烈，会造成网络卡顿
    - 快速重传的拥塞发生算法如下：`cwnd = cwnd/2` ，也就是设置为原来的一半;`ssthresh = cwnd`;进入快速恢复算法

  - 快速恢复算法：快速重传修改完拥塞窗口后开始重传丢失数据包，当收到重复的ACK时，拥塞窗口大小加1，直到收到新数据的ACK后，把拥塞窗口设置为快速重传中慢启动门限的值，重新开始拥塞避免的过程，有点类似动态平衡的意思，避免一直送







### HTTP/1.1相关问题


#### HTTP 基本概念

HTTP 是超文本传输协议，也就是**H**yperText **T**ransfer **P**rotocol，**是一个在计算机世界里专门在「两点」之间「传输」文字、图片、音频、视频等「超文本」数据的「约定和规范」**

- HTTP 是一个用在计算机世界里的**协议**。它使用计算机能够理解的语言确立了一种计算机之间交流通信的规范（**两个以上的参与者**），以及相关的各种控制和错误处理方式（**行为约定和规范**）
- HTTP 协议是一个**双向协议**。数据虽然是在 A 和 B 之间传输，但允许中间有**中转或接力**。在 HTTP 里，需要中间人遵从 HTTP 协议，只要不打扰基本的数据传输，就可以添加任意额外的东西。HTTP 是一个在计算机世界里专门用来在**两点之间传输数据**的约定和规范
- HTTP 传输的内容是「超文本」，其是文字、图片、视频等的混合体，最关键有超链接，能从一个超文本跳转到另外一个超文本

#### HTTP 常见的状态码：

- `2xx` 类状态码表示服务器**成功**处理了客户端的请求（200 OK，204 No Content，206 Partial Content）
- `3xx` 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是**重定向**（**301 Moved Permanently**，**302 Found**，**304 Not Modified**）
- `4xx` 类状态码表示客户端发送的**报文有误**，服务器无法处理，也就是错误码的含义（**400 Bad Request**，**403 Forbidden**，**404 Not Found**）
- `5xx` 类状态码表示客户端请求报文正确，但是**服务器处理时内部发生了错误**，属于服务器端的错误码（**500 Internal Server Error**，**501 Not Implemented**，**502 Bad Gateway**，**503 Service Unavailable**）

#### HTTP 常见字段有哪些：

- *Host* 字段：客户端发送请求时，用来指定服务器的域名
- *Content-Length 字段*：服务器在返回数据时，会有 `Content-Length` 字段，表明本次回应的数据长度
- *Connection 字段*：最常用于客户端要求服务器使用 TCP 持久连接，以便其他请求复用;HTTP/1.1 版本的默认连接都是持久连接，但为了兼容老版本的 HTTP，需要指定 `Connection` 首部字段的值为 `Keep-Alive`
- *Content-Type 字段*：用于服务器回应时，告诉客户端，本次数据是什么格式
- *Content-Encoding 字段*：表示服务器返回的数据使用了什么压缩格式

#### HTTP报文结构

- 请求报文结构：包括报文首部、空行、报文主体3部分。

```
报文首部： 第一行：请求行，请求方法，请求路径，HTTP版本 后续为各个首部：包括请求首部字段、通用首部字段和实体首部字段
空行：
报文主体： 向服务器发送的数据。如get请求中的各个参数。post请求中的参数。
```

- 响应报文结构：也是包括报文首部、空行、报文主体3部分。

```
报文首部： 第一行：状态行，包括HTTP版本 状态码 原因短语 后续为首部字段：响应首部字段、通用首部字段、实体首部字段
报文主体：服务器返回的响应体。如HTTM页面。
```

#### HTTP状态码301和302的区别

- 301 永久重定向：页面永久性转移，表示为资源或页面永久性地转移到了另一个位置。

​	（1）用于防止收藏夹中的旧地址因网页扩展名改变而出错

​	（2）用于多个域名跳转至同一域名

- 302 临时重定向：页面暂时性转移，表示资源或页面暂时转移到另一个位置；用作网址劫持，容易导致网站降权，严重时网站会被封掉，不推荐使用

#### 一次完整 HTTP 请求所经历的步骤

- 由域名→ IP 地址：寻找 IP 地址的过程依次经过了浏览器缓存、系统缓存、hosts 文件、路由器缓存、 递归搜索根域名服务器（DNS解析）。
- 建立 TCP/IP 连接（三次握手具体过程）。
- 由浏览器发送一个 HTTP 请求。
- 经过路由器的转发，通过服务器的防火墙，该 HTTP 请求到达了服务器。
- 服务器处理该 HTTP 请求，返回一个 HTML 文件。
- 浏览器解析该 HTML 文件，并且显示在浏览器端。
- 服务器关闭 TCP 连接（四次挥手具体过程）

#### GET和POST的区别

- **GET 的语义是从服务器获取指定的资源**，这个资源可以是静态的文本、页面、图片视频等。GET 请求的参数位置一般是写在 URL 中，URL 规定只能支持 ASCII，所以 GET 请求的参数只允许 ASCII 字符 ，而且浏览器会对 URL 的长度有限制
- **POST 的语义是根据请求负荷（报文body）对指定的资源做出处理**，具体的处理方式视资源类型而不同。POST 请求携带数据的位置一般是写在报文 body 中， body 中的数据可以是任意格式的数据，只要客户端与服务端协商好即可，而且浏览器不会对 body 大小做限制

#### GET 和 POST 方法都是安全和幂等的吗？

- 安全和幂等的概念：所谓的「安全」是指请求方法不会「破坏」服务器上的资源。所谓的「幂等」，意思是多次执行相同的操作，结果都是「相同」的
- 从 RFC 规范定义的语义来看:**GET 方法就是安全且幂等的**，因为它是「只读」操作，无论操作多少次，服务器上的数据都是安全的，且每次的结果都是相同的。**POST** 因为是「新增或提交数据」的操作，会修改服务器上的资源，所以是**不安全**的，且多次提交数据就会创建多个资源，所以**不是幂等**的。所以，**浏览器一般不会缓存 POST 请求，也不能把 POST 请求保存为书签**。
- 但是实际过程中，开发者不一定会按照 RFC 规范定义的语义来实现 GET 和 POST 方法。如果「安全」放入概念是指信息是否会被泄漏的话，虽然 POST 用 body 传输数据，而 GET 用 URL 传输，这样数据会在浏览器地址拦容易看到，但是并不能说 GET 不如 POST 安全的；因为 HTTP 传输的内容都是明文的，虽然在浏览器地址拦看不到 POST 提交的 body 数据，但是只要抓个包就都能看到了

#### GET 请求可以带 body 吗？

- 理论上，任何请求都可以带 body 的。只是因为 RFC 规范定义的 GET 请求是获取资源，所以根据这个语义不需要用到 body。另外，URL 中的查询参数也不是 GET 所独有的，POST 请求的 URL 中也可以有参数的

#### HTTP 缓存有哪些实现方式？

- 对于一些具有重复性的 HTTP 请求，比如每次请求得到的数据都一样的，可以把这对「请求-响应」的数据都**缓存在本地**，那么下次就直接读取本地的数据，不必在通过网络获取服务器的响应
- HTTP 缓存有两种实现方式，分别是**强制缓存和协商缓存**
  - 强缓存指的是只要浏览器判断缓存没有过期，则直接使用浏览器的本地缓存，决定是否使用缓存的主动性在于浏览器这边，利用Cache-Control和 Expires HTTP 响应头部（Response Header）字段实现的。**Cache-Control的优先级高于 Expires** 
  - **协商缓存就是与服务端协商之后，通过协商结果来判断是否使用本地缓存**。可以通过请求头部中的 `If-Modified-Since` 字段与响应头部中的 `Last-Modified` 字段实现（基于时间）；也可以通过请求头部中的 `If-None-Match` 字段与响应头部中的 `ETag` 字段实现（基于一个唯一标识） ；**Etag 的优先级更高**
  - 协商缓存这两个字段都需要配合强制缓存中 Cache-control 字段来使用，只有在未能命中强制缓存的时候，才能发起带有协商缓存字段的请求

#### HTTP常用的请求方法

- HTTP1.0定义了三种请求方法： GET， POST 和 HEAD方法。
  - GET：请求获取资源
  - POST：提交或修改数据
  - HEAD：获得报文首部，与 GET 方法类似，只是不返回报文主体，一般用于验证 URI 是否有效
- HTTP1.1新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法
  - OPTIONS：可使服务器传回该资源所支持的所有 HTTP 请求方法。
  - PUT：从客户端向服务器上传的数据取代指定的文件
  - DELETE：请求服务器删除指定的文件。
  - TRACE ：追踪路径。回显服务器收到的请求，主要用于测试或诊断。
  - CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器，通常用于SSL加密服务器的链接

#### HTTP/1.1 的优缺点有哪些

优点：

- *简单*：HTTP 基本的报文格式就是 `header + body`，头部信息也是 `key-value` 简单文本的形式，**易于理解**，降低了学习和使用的门槛
- *灵活和易于扩展*：HTTP协议里的各类请求方法、URI/URL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员**自定义和扩充**；工作在应用层（ `OSI` 第七层），则它**下层可以随意变化**（HTTPS 也就是在 HTTP 与 TCP 层之间增加了 SSL/TLS 安全传输层，HTTP/3 甚至把 TCP 层换成了基于 UDP 的 QUIC）
- *应用广泛和跨平台*：HTTP 的应用遍地开花，同时天然具有**跨平台**的优越性

缺点：

- *无状态双刃剑*：
  - 无状态的**好处**，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的 CPU 和内存用来对外提供服务
  - 无状态的**坏处**，既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦；解法方案有很多种，其中比较简单的方式用 **Cookie** 技术
- *明文传输双刃剑*：明文意味着在传输过程中的信息，是可方便阅读的，但是这正是这样，HTTP 的所有信息都暴露在了光天化日下，相当于**信息裸奔**；
- *不安全*：
  - 通信使用明文（不加密），内容可能会被窃听。比如，**账号信息容易泄漏**
  - 不验证通信方的身份，因此有可能遭遇伪装。比如，**访问假的淘宝、拼多多**
  - 无法证明报文的完整性，所以有可能已遭篡改。比如，**网页上植入垃圾广告**

#### HTTP/1.1 的性能如何？

- HTTP 协议是基于 **TCP/IP**，并且使用了「**请求 - 应答**」的通信模式，所以性能的关键就在这**两点**里
- HTTP/1.1 提出了**长连接**的通信方式，也叫持久连接。这种方式的好处在于减少了 TCP 连接的重复建立和断开所造成的额外开销，减轻了服务器端的负载
- *管道网络传输*：可在同一个 TCP 连接里面，客户端可以发起多个请求，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以**减少整体的响应时间**。但是**服务器必须按照接收请求的顺序发送对这些管道化请求的响应**。
- *队头阻塞*：「请求 - 应答」的模式加剧了 HTTP 的性能问题，当顺序发送的请求序列中的一个请求因为某种原因被阻塞时，在后面排队的所有请求也一同被阻塞了，会招致客户端一直请求不到数据，这也就是「**队头阻塞**」

#### 如何让浏览器记住登录状态

- 使用cookie保存：cookie是保存在客户端的，用户登录成功后，后台将加密后的用户信息发送到客户端浏览器，由客户端浏览器保存。

  - 内存cookie(进程中的cookie):大部分的session机制都使用进程中cookie来保存session id，关闭浏览器后这个进程自动消失，因此cookie也随之消失，再次连接到服务器时，无法找到对应的session，也就无法实现自动登录。
  - 硬盘cookie：硬盘中的cookie不会丢失session id，即使关闭浏览器后再打开，也仍能实现自动登录。比如“记住一周”，购物车信息可以在切换不同浏览器时依然可用，都是因为使用了硬盘cookie。

- 使用session保存：Session保存在服务器端，每个session都有一个唯一的session id 保存在cookie里，如果关闭浏览器再打开浏览器，cookie仍然记住了之前的session id，就可以自动登录

- 使用token：

  - 用户首次登录，输入账号密码，提交服务器。
  - 服务器对输入内容进行校验，若校验通过，登录成功，并生成一个token值，将该值存入数据库，并返回给客户端。
  - 客户端拿到返回的token，保存至本地(cookie/local storage)，作为公共参数，以后每次请求服务器时都携带该token（放在响应头里），提交给服务器进行校验。
  - 服务器接收请求后，验证是否携带token，若携带，则取出请求头中的token值与数据库存储的值进行匹配校验，若相同，则登录成功，返回数据，若数据库中不存在该值或者两者不一致，则说明原来登录已失效，返回错误码。

  注：用户每进行一次登录，登录成功后，服务器都会更新一个token新值返回给客户端

  - token的优点
    1. token可以存储在任何位置(比如cookie)
    2. token更容易跨域
    3. token过期时可以通过刷新token，让用户一直保持有效登录
    4. 如果api在不同终端上，token更方便安全

### HTTPS相关问题

#### HTTP 与 HTTPS 有哪些区别？

1. HTTP 是超文本传输协议，信息是明文传输，存在安全风险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。
2. HTTP 连接建立相对简单， TCP 三次握手之后便可进行 HTTP 的报文传输。而 HTTPS 在 TCP 三次握手之后，还需进行 SSL/TLS 的握手过程，才可进入加密报文传输。
3. HTTP 的端口号是 80，HTTPS 的端口号是 443。
4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的

#### HTTP与HTTPS的缺点，以及区别

（1）HTTP 的不足

- 窃听风险： 通信使用明文(不加密),内容可能会被窃听；
- 冒充风险： 不验证通信方的身份,因此有可能遭遇伪装；
- 篡改风险： 无法证明报文的完整性,所以有可能已遭篡改；

（2）HTTPS 的缺点

- HTTPS 协议多次握手，导致页面的加载时间延长近 50%；
- HTTPS 连接缓存不如 HTTP 高效，会增加数据开销和功耗；
- SSL 涉及到的安全算法会消耗 CPU 资源，对服务器资源消耗较大；

（3）区别

- 端口不同：HTTP 和 HTTPS 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443；
- 资源消耗：HTTP 是超文本传输协议，信息是明文传输，HTTPS 则是具有安全性的 ssl 加密传输协议，需要消耗更多的 CPU 和内存资源
- 开销：HTTPS 协议需要到 CA 申请证书，一般免费证书很少，需要交费；
- 安全性：HTTP 的连接很简单，是无状态的；HTTPS 协议是由 TLS+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 HTTP 协议安全

#### HTTPS协议为什么安全？(HTTPS 解决了 HTTP 的哪些问题？)

HTTP**S** 在 HTTP 与 TCP 层之间加入了 `SSL/TLS` 协议，可以很好的HTTP的**窃听风险**、**篡改风险**、**冒充风险**

- **混合加密**（**对称加密**和**非对称加密**结合）的方式实现信息的**机密性**解决了窃听的风险
  - **公钥加密，私钥解密**是为了**保证内容传输的安全**，因为被公钥加密的内容，其他人是无法解密的，只有持有私钥的人，才能解密出实际的内容；
  - **私钥加密，公钥解密**是为了**保证消息不会被冒充**，因为私钥是不可泄露的，如果公钥能正常解密出私钥加密的内容，就能证明这个消息是来源于持有私钥身份的人发送的。
- **摘要算法**的方式来实现**完整性**，它能够为数据生成独一无二的「指纹」，指纹用于校验数据的完整性，解决了篡改的风险。
- 将服务器公钥放入到**数字证书**(「个人信息 + 公钥 + 数字签名」打包而成)中，解决了冒充的风险

#### HTTPS的SSL连接过程

SSL/TLS 协议基本流程：

- 客户端向服务器索要并验证服务器的公钥。
- 双方协商生产「会话秘钥」。
- 双方采用「会话秘钥」进行加密通信。

前两步也就是 SSL/TLS 的建立过程，也就是 TLS 握手阶段。SSL/TLS 的「握手阶段」涉及**四次**通信：

- 首先，由客户端向服务器发起加密通信请求，也就是 `ClientHello` 请求。客户端主要向服务器发送以下信息：客户端支持的 SSL/TLS 协议版本；客户端生产的随机数（`Client Random`），后面用于生成「会话秘钥」条件之一；客户端支持的密码套件列表
- 服务器收到客户端请求后，向客户端发出响应，也就是 `SeverHello`。服务器回应的内容有如下内容：确认 SSL/ TLS 协议版本；服务器生产的随机数（`Server Random`），也是后面用于生产「会话秘钥」条件之一；确认的密码套件列表；服务器的数字证书
- 客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性。如果证书没有问题，客户端会**从数字证书中取出服务器的公钥**，然后使用它加密报文，向服务器发送如下信息：一个随机数（`pre-master key`，会被服务器公钥加密）；加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信；客户端握手结束通知，表示客户端的握手阶段已经结束（这一项同时把之前所有内容的发生的数据做个摘要，用来供服务端校验）
- **服务器和客户端有了这三个随机数（Client Random、Server Random、pre-master key），接着就用双方协商的加密算法，各自生成本次通信的「会话秘钥」**。然后，向客户端发送最后的信息：加密通信算法改变通知，表示随后的信息都将用「会话秘钥」加密通信；服务器握手结束通知，表示服务器的握手阶段已经结束。这一项同时把之前所有内容的发生的数据做个摘要，用来供客户端校验。

#### 客户端校验数字证书的流程是怎样的？

- CA 签发证书的过程：

  - 首先 CA 会把持有者的公钥、用途、颁发者、有效时间等信息打成一个包，然后对这些信息进行 Hash 计算，得到一个 Hash 值；
  - 然后 CA 会使用自己的私钥将该 Hash 值加密，生成 Certificate Signature，也就是 CA 对证书做了签名；
  - 最后将 Certificate Signature 添加在文件证书上，形成数字证书；

- 客户端校验服务端的数字证书的过程：

  - 首先客户端会使用同样的 Hash 算法获取该证书的 Hash 值 H1；
  - 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使用 CA 的公钥解密 Certificate Signature 内容，得到一个 Hash 值 H2 ；
  - 最后比较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。

  但事实上，证书的验证过程中**还存在一个证书信任链的问题**，因为我们向 CA 申请的证书一般不是根证书签发的，而是由中间证书签发的。**这是为了确保根证书的绝对安全性，将根证书隔离地越严格越好，不然根证书如果失守了，那么整个信任链都会有问题。**

#### HTTPS 的应用数据是如何保证完整性的？

- TLS 在实现上分为**握手协议**和**记录协议**两层：

  - TLS 握手协议就是我们前面说的 TLS 四次握手的过程，负责协商加密算法和生成对称密钥，后续用此密钥来保护应用程序数据（即 HTTP 数据）；
  - TLS 记录协议负责保护应用程序数据并验证其完整性和来源，所以对 HTTP 数据加密是使用记录协议；

- TLS 记录协议主要负责消息（HTTP 数据）的压缩，加密及数据的认证，具体过程如下：

  - 首先，消息被分割成多个较短的片段,然后分别对每个片段进行压缩。
  - 接下来，经过压缩的片段会被**加上消息认证码（MAC 值，这个是通过哈希算法生成的），这是为了保证完整性，并进行数据的认证**。通过附加消息认证码的 MAC 值，可以识别出篡改。与此同时，为了防止重放攻击，在计算消息认证码时，还加上了片段的编码。
  - 再接下来，经过压缩的片段再加上消息认证码会一起通过对称密码进行加密。
  - 最后，上述经过加密的数据再加上由数据类型、版本号、压缩后的长度组成的报头就是最终的报文数据。

  记录协议完成后，最终的报文数据将传递到传输控制协议 (TCP) 层进行传输。

#### HTTPS 握手会影响性能吗？

- 由裸数据传输的 HTTP 协议转成加密数据传输的 HTTPS 协议，给应用数据套了个「保护伞」，提高安全性的同时也带来了性能消耗
- 因为 HTTPS 相比 HTTP 协议多一个 TLS 协议握手过程，**目的是为了通过非对称加密握手协商或者交换出对称加密密钥**，这个过程最长可以花费掉 2 RTT，接着后续传输的应用数据都得使用对称加密密钥来加密/解密

#### HTTPS 一定安全可靠吗？

- 客户端通过浏览器向服务端发起 HTTPS 请求时，被「假基站」转发到了一个「中间人服务器」，于是客户端是和「中间人服务器」完成了 TLS 握手，然后这个「中间人服务器」再与真正的服务端完成 TLS 握手；具体过程如下：
  - 客户端向服务端发起 HTTPS 建立连接请求时，然后被「假基站」转发到了一个「中间人服务器」，接着中间人向服务端发起 HTTPS 建立连接请求，此时客户端与中间人进行 TLS 握手，中间人与服务端进行 TLS 握手；
  - 在客户端与中间人进行 TLS 握手过程中，中间人会发送自己的公钥证书给客户端，**客户端验证证书的真伪**，然后从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给中间人，中间人使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（A），后续客户端与中间人通信就用这个对称加密密钥来加密数据了。
  - 在中间人与服务端进行 TLS 握手过程中，服务端会发送从 CA 机构签发的公钥证书给中间人，从证书拿到公钥，并生成一个随机数，用公钥加密随机数发送给服务端，服务端使用私钥解密，得到随机数，此时双方都有随机数，然后通过算法生成对称加密密钥（B），后续中间人与服务端通信就用这个对称加密密钥来加密数据了。
  - 后续的通信过程中，中间人用对称加密密钥（A）解密客户端的 HTTPS 请求的数据，然后用对称加密密钥（B）加密 HTTPS 请求后，转发给服务端，接着服务端发送 HTTPS 响应数据给中间人，中间人用对称加密密钥（B）解密 HTTPS 响应数据，然后再用对称加密密钥（A）加密后，转发给客户端。
- 从客户端的角度看，其实并不知道网络中存在中间人服务器这个角色。那么中间人就可以解开浏览器发起的 HTTPS 请求里的数据，也可以解开服务端响应给浏览器的 HTTPS 响应数据。相当于，中间人能够 “偷看” 浏览器与服务端之间的 HTTPS 请求和响应的数据。
- 但是要发生这种场景是有前提的，前提是用户点击接受了中间人服务器的证书。中间人服务器与客户端在 TLS 握手过程中，实际上发送了自己伪造的证书给浏览器，而这个伪造的证书是能被浏览器（客户端）识别出是非法的，于是就会提醒用户该证书存在问题
- **HTTPS 协议本身到目前为止还是没有任何漏洞的，即使你成功进行中间人攻击，本质上是利用了客户端的漏洞（用户点击继续访问或者被恶意导入伪造的根证书），并不是 HTTPS 不够安全**

#### 为什么抓包工具能截取 HTTPS 数据？

- 很多抓包工具 之所以可以明文看到 HTTPS 数据，工作原理与中间人一致的。对于 HTTPS 连接来说，中间人要满足以下两点，才能实现真正的明文代理:
  1. 中间人，作为客户端与真实服务端建立连接这一步不会有问题，因为服务端不会校验客户端的身份；
  2. 中间人，作为服务端与真实客户端建立连接，这里会有客户端信任服务端的问题，也就是服务端必须有对应域名的私钥；
- 中间人要拿到私钥只能通过如下方式：去网站服务端拿到私钥；去CA处拿域名签发私钥；自己签发证书，且要被浏览器信任；抓包工具只能使用第三种方式取得中间人的身份。
- 使用抓包工具进行 HTTPS 抓包的时候，需要在客户端安装 Fiddler 的根证书，这里实际上起认证中心（CA）的作用。
- 抓包工具能够抓包的关键是客户端会往系统受信任的根证书列表中导入抓包工具生成的证书，而这个证书会被浏览器信任，也就是抓包工具给自己创建了一个认证中心 CA，客户端拿着中间人签发的证书去中间人自己的 CA 去认证，当然认为这个证书是有效的。

#### 如何避免被中间人抓取数据？

- 可以通过 **HTTPS 双向认证**来避免这种问题
- 如果用了双向认证方式，不仅客户端会验证服务端的身份，而且服务端也会验证客户端的身份。服务端一旦验证到请求自己的客户端为不可信任的，服务端就拒绝继续通信，客户端如果发现服务端为不可信任的，那么也中止通信

### HTTP改进相关问题

#### HTTP/1.1 相比 HTTP/1.0 提高了什么性能？

- 使用**长连接的方式**改善了 HTTP/1.0 短连接造成的性能开销。
- 支持**管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。**

但 HTTP/1.1 还是有性能瓶颈：

- 请求 / 响应头部（Header）未经压缩就发送，首部信息越多延迟越大。只能压缩 `Body` 的部分；
- 发送冗长的首部。每次互相发送相同的首部造成的浪费较多；
- 服务器是按请求的顺序响应的，如果服务器响应慢，会招致客户端一直请求不到数据，也就是队头阻塞；
- 没有请求优先级控制；
- 请求只能从客户端开始，服务器只能被动响应

#### HTTP/2 做了什么优化？

- *头部压缩*：HTTP/2 会**压缩头**（Header）如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你**消除重复的部分**。这就是所谓的 `HPACK` 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就**提高速度**了
- *二进制格式*：HTTP/2 不再像 HTTP/1.1 里的纯文本形式的报文，而是全面采用了**二进制格式**，头信息和数据体都是二进制，并且统称为帧（frame）：**头信息帧（Headers Frame）和数据帧（Data Frame）**。计算机收到报文后，无需再将明文的报文转成二进制，而是直接解析二进制报文，这**增加了数据传输的效率**
- *并发传输*： HTTP/2 引出了 Stream 概念，多个 Stream 复用在一条 TCP 连接，Stream 里可以包含 1 个或多个 Message，Message 对应 HTTP/1 中的请求或响应，由 HTTP 头部和包体构成。Message 里包含一条或者多个 Frame，Frame 是 HTTP/2 最小单位，以二进制压缩格式存放 HTTP/1 中的内容（头部和包体）
  - **针对不同的 HTTP 请求用独一无二的 Stream ID 来区分，接收端可以通过 Stream ID 有序组装成 HTTP 消息，不同 Stream 的帧是可以乱序发送的，因此可以并发不同的 Stream ，也就是 HTTP/2 可以并行交错地发送请求和响应**
- *服务器推送*：HTTP/2 还在一定程度上改善了传统的「请求 - 应答」工作模式，服务端不再是被动地响应，可以**主动**向客户端发送消息。客户端和服务器**双方都可以建立 Stream**， Stream ID 也是有区别的，客户端建立的 Stream 必须是奇数号，而服务器建立的 Stream 必须是偶数号。

#### HTTP/2 有什么缺陷？

- HTTP/2 通过 Stream 的并发能力，解决了 HTTP/1 队头阻塞的问题，看似很完美了，但是 HTTP/2 还是存在“队头阻塞”的问题，只不过问题不是在 HTTP 这一层面，而是在 TCP 这一层
- **HTTP/2 是基于 TCP 协议来传输数据的，TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP/2 应用层才能从内核中拿到数据，这就是 HTTP/2 队头阻塞问题**。一旦发生了丢包现象，就会触发 TCP 的重传机制，这样在一个 TCP 连接中的**所有的 HTTP 请求都必须等待这个丢了的包被重传回来**

#### HTTP/3 做了哪些优化？

- HTTP/1.1 中的管道（ pipeline）虽然解决了请求的队头阻塞，但是**没有解决响应的队头阻塞**，因为服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等响应完这个请求后， 才能处理下一个请求，这属于 HTTP 层队头阻塞。

- HTTP/2 虽然通过多个请求复用一个 TCP 连接解决了 HTTP 的队头阻塞 ，但是**一旦发生丢包，就会阻塞住所有的 HTTP 请求**，这属于 TCP 层队头阻塞。

- HTTP/2 队头阻塞的问题是因为 TCP，所以 **HTTP/3 把 HTTP 下层的 TCP 协议改成了 UDP**，基于 UDP 的 **QUIC 协议** 可以实现类似 TCP 的可靠性传输

  - *无队头阻塞*：

    QUIC 协议也有类似 HTTP/2 Stream 与多路复用的概念，也是可以在同一条连接上并发传输多个 Stream，Stream 可以认为就是一条 HTTP 请求。

    QUIC 有自己的一套机制可以保证传输的可靠性的，每个数据包都有一个序号唯一标识。**当某个流发生丢包时，只会阻塞这个流，其他流不会受到影响，因此不存在队头阻塞问题**。这与 HTTP/2 不同，HTTP/2 只要某个流中的数据包丢失了，其他流也会因此受影响。所以，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，某个流发生丢包了，只会影响该流，其他流不受影响。

  - *更快的连接建立*：

    对于 HTTP/1 和 HTTP/2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。

    HTTP/3 在传输数据前虽然需要 QUIC 协议握手，这个握手过程只需要 1 RTT，握手的目的是为确认双方的「连接 ID」，连接迁移就是基于连接 ID 实现的。

    但是 HTTP/3 的 QUIC 协议并不是与 TLS 分层，而是**QUIC 内部包含了 TLS**，它在自己的帧会携带 TLS 里的“记录”，再加上 QUIC 使用的是 TLS/1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与密钥协商。甚至，在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。

  - *连接迁移*

    基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。

    那么**当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接**。而建立连接的过程包含 TCP 三次握手和 TLS 四次握手的时延，以及 TCP 慢启动的减速过程，给用户的感觉就是网络突然卡顿了一下，因此连接的迁移成本是很高的。

    而 QUIC 协议没有用四元组的方式来“绑定”连接，而是通过**连接 ID**来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本，没有卡顿感，达到了**连接迁移**的功能

- QUIC 是一个在 UDP 之上的**伪** TCP + TLS + HTTP/2 的多路复用的协议

### 应用层其余问题

#### 了解RPC框架或微服务吗

- RPC全称Remote Procedure Call，即远程过程调用。其本质上不算是协议，而是一种调用方式，其实就是主机A通过某种网络协议向支持相同协议的主机B发送一个任务执行命令，并且在某些情况下，还能支持任务执行结果的返回
- 几乎每一个RPC都有着自己的网络协议定义，如果要按照TCP/IP协议栈划分，这些R**PC协议通HTTP/HTTPS协议一样属于应用层协议**，不过相比较于HTTP/HTTPS协议来说，**RPC协议在功能和性能之间更偏重与性能，即RPC框架是一种高性能的网络通信框架**
- 在实际开发中，**RPC框架多用于微服务架构**中。一个个应用层之间使用的API，也可以被应用层看做一个个的服务，这就是微服务架构；**微服务是一种用于构建应用的架构方案**。微服务架构有别于更为传统的单体式方案，**可将应用拆分成多个核心功能**。每个功能都被称为一项服务，可以单独构建和部署，这意味着各项服务在工作（和出现故障）时不会相互影响

### IP层相关问题

#### 网络IP地址转化流程，外网IP和内网IP是怎么转换的

- 公网ip具有世界范围的唯一性，而内网ip只在局域网内部具有唯一性。并且，一个局域网里所有电脑的内网IP是互不相同的,但共用一个外网IP
- 在局域网中，每台电脑都可以自己分配自己的IP，但是这个IP只在局域网中有效。而如果你将电脑连接到互联网，你的网络提供商的服务器会为你分配一个IP地址，这个IP地址才是你在外网的IP。两个IP同时存在，一个对内，一个对外。内网主要作用有：
  - 共享传输信道：简单地理解就是不需要每台电脑一个外网IP地址;
  - 传输速率高：内网之间的电脑因为没有外网网络拓扑的复杂性，所以互相通信的网络可以很快，比如从一个台电脑向另一台电脑复制一个几G的文件可能只需要数十秒时间。
  - 误码率低：因为通信距离很近，所以误码率很低，换句话说就是网络很稳定
- 互联网上的IP（即外网IP）地址统一由一个叫“IANA”的组织来管理。由于分配不合理以及IPv4协议本身存在的局限，现在互联网的IP地址资源越来越紧张。
- NAT技术：实现内网电脑访问外网的能力
  - 假如电脑A想要访问百度，百度的IP我们假设为：172.168.30.3；电脑A的IP是我们虚构的，实际上可能并不存在这样一个IP，如果用电脑A的IP去访问百度，那肯定行不通。由于百度和电脑A不在一个局域网内，所以A要访问百度，那么必须得经过网关。而网关的这个IP地址，是真实存在的，是可以访问百度的。为了让 A 可以访问百度，那么我们可以采取这样的方法：让网关去帮助 A 访问，然后百度把结果传递给网关，而网关再把结果传递给 A
  - 不过电脑A、B、C都可能拜托网关去帮忙访问百度，而百度返回的结果 的目的IP都是网关的IP=192.168.1.1，去访问百度的时候，把 A的IP + 端口 映射成 网关的IP+端口就可以唯一确定身份
  - 百度把结果返回给网关的80端口之后，网关再通过映射表，就可以把结果返回给 A的60端口了。
  - 这种方法地址的映射转换，我们也称之为网络地址转换，英文为 Network Address Translation，简称NAT。
- 像A、B、C这样的IP地址我们也称之为内网IP，即内网IP;而像网关，百度这样的IP我们称之为外网IP(即互联网公网IP)。

#### ping 的工作原理

- ICMP 全称是 **Internet Control Message Protocol**，也就是**互联网控制报文协议**, 主要的功能包括：**确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等**；ICMP 报文封装在 IP 包里面，工作在网络层，是 IP 协议的助手，ICMP 包头的**类型**字段，大致可以分为两大类：查询报文类型（用于诊断）和差错报文类型（通知出错原因）

- `ping` 的**发送和接收过程**：

  - ping 命令执行的时候，源主机首先会构建一个 **ICMP 回送请求消息**数据包，最重要的是两个：第一个是**类型**，对于回送请求消息而言该字段为 `8`；另外一个是**序号**，主要用于区分连续 ping 的时候发出的多个数据包。每发出一个请求数据包，序号会自动加 `1`。为了能够计算往返时间 `RTT`，它会在报文的数据部分插入发送时间；由 ICMP 协议将这个数据包连同地址一起交给 IP 层，构建一个 `IP` 数据包
  - 接下来，需要加入 `MAC` 头；如果在本地 ARP 映射表中查找出 IP 地址的MAC地址则可以直接使用，否则发送 `ARP` 协议查询 MAC 地址，由数据链路层构建一个数据帧
  - 主机 `B` 收到这个数据帧后，先检查它的目的 MAC 地址，并和本机的 MAC 地址对比，如符合，则接收，否则就丢弃；接收后检查该数据帧，将 IP 数据包从帧中提取出来，交给本机的 IP 层。同样，IP 层检查后，将有用的信息提取后交给 ICMP 协议
  - 主机 `B` 会构建一个 **ICMP 回送响应消息**数据包，回送响应数据包的**类型**字段为 `0`，**序号**为接收到的请求数据包中的序号，然后再发送出去给主机 A
  - 在规定的时候间内，源主机如果没有接到 ICMP 的应答包，则说明目标主机不可达；如果接收到了 ICMP 回送响应消息，则说明目标主机可达。此时，源主机会检查，用当前时刻减去该数据包最初从源主机上发出的时刻，就是 ICMP 数据包的时间延迟。

   ping 这个程序是**使用了 ICMP 里面的 ECHO REQUEST（类型为 8 ） 和 ECHO REPLY （类型为 0）**

#### ICMP常见报文类型

- **查询报文**：回送消息 —— 类型 `0` 和 `8`

  - 用于进行通信的主机或路由器之间，判断所发送的数据包是否已经成功到达对端的一种消息，**`ping` 命令就是利用这个消息实现的**；

  - 可以向对端主机发送**回送请求**的消息（`ICMP Echo Request Message`，类型 `8`）也可以接收对端主机发回来的**回送应答**消息（`ICMP Echo Reply Message`，类型 `0`）

  - 相比原生的 ICMP，这里多了两个字段：

    - **标识符**：用以区分是哪个应用程序发 ICMP 包，比如用进程 `PID` 作为标识符；
    - **序号**：序列号从 `0` 开始，每发送一次新的回送请求就会加 `1`， 可以用来确认网络包是否有丢失。

    在**选项数据**中，`ping` 还会存放发送请求的时间值，来计算往返时间，说明路程的长短

- **差错报文**的类型：

  - 目标不可达消息 —— 类型 为 `3`：IP 路由器无法将 IP 数据包发送给目标地址，在这个消息中显示不可达的具体原因，原因记录在 ICMP 包头的**代码**字段。
    - 网络不可达代码为 `0`，路由器中的路由器表匹配不到接收方 IP 的网络号
    - 主机不可达代码为 `1`，路由表中没有该主机的信息，或者该主机没有连接到网络
    - 协议不可达代码为 `2`，对端主机的防火墙已经禁止 TCP 协议访问
    - 端口不可达代码为 `3`，端主机没有进程监听 8080 端口
    - 需要进行分片但设置了不分片位代码为 `4`， IP 首部的**分片禁止标志位**设置为`1`。根据这个标志位，途中的路由器遇到超过 MTU 大小的数据包时，不会进行分片，而是直接抛弃
  - 原点抑制消息 —— 类型 `4`：路由器向低速线路发送数据时，其发送队列的缓存变为零而无法发送出去时，可以向 IP 包的源地址发送一个 ICMP **原点抑制消息**
  - 重定向消息 —— 类型 `5`，路由器发现发送端主机使用了「不是最优」的路径发送数据，会返回一个 ICMP **重定向消息**给这个主机，在这个消息中包含了**最合适的路由信息和源数据**。
  - 超时消息 —— 类型 `11`，IP 包中有一个字段叫做 `TTL` （`Time To Live`，生存周期），它的**值随着每经过一次路由器就会减 1，直到减到 0 时该 IP 包会被丢弃。**

#### 网络ping不通是什么原因？

- ping的原理是利用网络上机器IP地址的唯一性，给目标IP地址发送一个数据包，再要求对方返回一个同样大小的数据包来确定两台网络机器是否连接相通，时延是多少

- ping命令不通，主要有两种情况，一种是同网段内的ip地址ping不通，另一种是不同网段的ip地址ping不通

  - **同网段ping不通，结果是“无法访问目标主机”**，说明此时，ping的需求并没有成功发出，这时，要检查：

    1、对方是否开机？ip是否存在？

    2、有跨交换机vlan的话，检查对应的中间trunk链路是否导通？

    3、走直连路由是否正确？是否应该走默认路由，而走了直连路由。

    4、子网掩码是否错误。

    5、默认网关是否填写正确

  - **同网段ping不通，结果是“超时（time out)”**，这种情况是ping已经成功发出了，到达了主机，但时没有得到响应，要检查：

    1、检查下防火墙，防火墙禁止了对ping的回应。

    2、子网掩码的设置错误，导致不在同一个网段。

    3、设备硬件故障，导致设备没有对应的mac地址，无法生成路由表，而走默认路由。

    4、ip冲突，或ip地址与直联路由不在同一个网段。

    5、网关没有设置好

  - **跨网段ping不通，结果是“无法访问目标主机”**,说明请求没有成功发出，获取不了目的ip地址与mac地址。可能出现的原因是：

    1、目的ip地址不存在。

    2、检查路由表是否有缺省的路由

    3、检查arp表是否有网关的mac地址

    4、有网关设置错误

    5、走了默认路由

  - **跨网段ping不通，结果是“time out”**，表示ping的request消息已经发出，目的ip的网关已经获取到目的ip的mac地址，但是目的主机没有回复，或源主机无法收到。这些应该检查回程路由和节点回程路由。可能的原因有：

    1、检查下防火墙，是否拦截了ping的请求消息。

    2、检查经过节点的路由是否正确，或者是否有回程路由。

    3、回程路由的硬件网卡出口和ping的request的入口网卡不是同一个

    4、交换机vlan对应的接口全部down了，导致vlan状态down，vlan的对应路由没有生成。

## 四、C++

### 基本语法

#### 什么是“引用”？声明和使用“引用”要注意哪些问题？

- 引用就是某个目标变量的“别名”，对应用的操作与变量直接操作效果完全相同。
- 声明一个引用的时候，切记要对其进行初始化。
- 引用声明完毕后，相当于目标变量名有两个名称，即该目标原名称和引用名，不能再把该引用名作为其他变量名的别名
- 声明一个引用，不是新定义了一个变量，它只表示该引用名是目标变量名的一个别名，它本身不是一种数据类型，因为该引用本身不占存储单元，系统也不给引用分配存储单元
- 不能建立数组的引用。

#### 引用和指针有什么区别？

- 本质：**引用是别名，指针是地址**
- 指针可以在运行时改变其所指向的值，引用一旦和某个对象绑定就不再改变
- 从内存上看，指针会分配内存区域，而引用不会，它仅仅是一个别名
- 在参数传递时，引⽤用会做类型检查，而指针不会
- 引用不能为空，指针可以为空

#### C++ 中的指针参数传递和引用参数传递

- 指针参数传递**本质上是值传递**，它所传递的是一个地址值。值传递过程中，被调函数的形式参数作为被调函数的局部变量处理，会在栈中开辟内存空间以存放由主调函数传递进来的实参值，从而形成了实参的一个副本（替身）。值传递的特点是，被调函数对形式参数的任何操作都是作为局部变量进行的，不会影响主调函数的实参变量的值（形参指针变了，实参指针不会变）。
- 引用参数传递过程中，**被调函数的形式参数也作为局部变量在栈中开辟了内存空间**，但是这时存放的是由主调函数放进来的实参变量的地址。被调函数对形参（本体）的任何操作都**被处理成间接寻址**，即通过栈中存放的地址访问主调函数中的实参变量（根据别名找到主调函数中的本体）。因此，**被调函数对形参的任何操作都会影响主调函数中的实参变量**
- 引用传递和指针传递是不同的，虽然他们都是在被调函数栈空间上的一个局部变量，但是任何对于引用参数的处理都会通过一个间接寻址的方式操作到主调函数中的相关变量。而对于指针传递的参数，如果改变被调函数中的指针地址，它将应用不到主调函数的相关变量。如果想通过指针参数传递来改变主调函数中的相关变量（地址），那就得使用指向指针的指针或者指针引用。
- 从编译的角度来讲，**程序在编译时分别将指针和引用添加到符号表上**，符号表中记录的是变量名及变量所对应地址。指针变量在符号表上对应的地址值为指针变量的地址值，而**引用在符号表上对应的地址值为引用对象的地址值**（与实参名字不同，地址相同）。**符号表生成之后就不会再改，因此指针可以改变其指向的对象**（指针变量中的值可以改），而引用对象则不能修改。

####  形参与实参的区别？

- 形参变量只有在被调用时才分配内存单元，在调用结束时， 即刻释放所分配的内存单元。因此，**形参只有在函数内部有效**。 函数调用结束返回主调函数后则不能再使用该形参变量
- 实参可以是常量、变量、表达式、函数等， 无论实参是何种类型的量，**在进行函数调用时，它们都必须具有确定的值**， 以便把这些值传送给形参。 因此应预先用赋值，输入等办法使实参获得确定值，会产生一个临时变量
- 实参和形参在数量上，类型上，顺序上应严格一致， 否则会发生“类型不匹配”的错误。
- 函数调用中发生的数据传送是单向的。 即只能把实参的值传送给形参，而不能把形参的值反向地传送给实参。 因此在函数调用过程中，形参的值发生改变，而实参中的值不会变化
- 当形参和实参不是指针类型时，在该函数运行时，形参和实参是不同的变量，他们在内存中位于不同的位置，形参将实参的内容复制一份，在该函数运行结束的时候形参被释放，而实参内容不会改变
- 值传递：有一个形参向函数所属的栈拷贝数据的过程，如果值传递的对象是类对象 或是大的结构体对象，将耗费一定的时间和空间。（传值）
- 指针传递：同样有一个形参向函数所属的栈拷贝数据的过程，但拷贝的数据是一个固定为4字节的地址。（传值，传递的是地址值）
- 引用传递：同样有上述的数据拷贝过程，但其是针对地址的，相当于为该数据所在的地址起了一个别名。（传地址）
  效率上讲，指针传递和引用传递比值传递效率高。一般主张使用引用传递，代码逻辑上更加紧凑、清晰

####  int fun() 和 int fun(void) 的区别?

- 在c中，int fun() 会解读为返回值为int(即使前面没有int，也是如此，但是在c++中如果没有返回类型将报错)，输入类型和个数没有限制， 而int fun(void)则限制输入类型为一个void
- 在c++下，这两种情况都会解读为返回int类型，输入void类型

#### C语言 struct 和 C++ struct 区别

- C语言中：struct是用户自定义数据类型（UDT）；C++中struct是抽象数据类型（ADT），支持成员函数的定义，（C++中的struct能继承，能实现多态）。
- C中struct是没有权限的设置的，且struct中只能是一些变量的集合体，可以封装数据却不可以隐藏数据，而且成员不可以是函数。
- C++中，struct的成员默认访问说明符为public（为了与C兼容），class中的默认访问限定符为private，struct增加了访问权限，且可以和类一样有成员函数。
- struct作为类的一种特例是用来自定义数据结构的。一个结构标记声明后，在C中必须在结构标记前加上struct，才能做结构类型名



#### const 和 define 有什么区别？

- 本质：**define只是字符串替换，const参与编译运行**
- define不会做**类型检查**，const拥有类型，会执行相应的类型检查
- define仅仅是宏替换，**不占用内存**，而const会占用内存
- const**内存效率更高，编译器通常将const变量保存在符号表中**，而不会分配存储空间，这使得**它成为一个编译期间的常量，没有存储和读取的操作**

#### define 和 inline 有什么区别？

- 本质：**define只是字符串替换，inline由编译器控制**
- define只是简单的宏替换，通常会产生二义性；而inline会真正地编译到代码中
- inline函数是否展开由编译器决定，有时候当函数太大时，编译器可能选择不展开相应的函数

#### C 和 C++ static 中的作用

- 在 C 语言中，使用 static 可以定义局部静态变量、外部静态变量、静态函数。
- 在 C++ 中，使用 static 可以定义局部静态变量、外部静态变量、静态函数、静态成员变量和静态成员函数。因为 C++ 中有类的概念，静态成员变量、静态成员函数都是与类有关的概念

- static 全局静态变量:
  - 普通全局变量和 static 全局静态变量都为静态存储方式。普通全局变量的作用域是整个源程序，当一个源程序由多个源文件组成时，**普通全局变量在各个源文件中都是有效的**
  - 静态全局变量则限制了其作用域，即只在定义该变量的源文件内有效，在同一源程序的其它源文件中不能使用它。由于**静态全局变量的作用域限于一个源文件内**，只能为该源文件内的函数公用，因此可以避免在其他源文件中引起错误。静态全局变量只初始化一次，防止在其他文件中使用
  - 实际上全局静态变量存储在内存的静态存储区，**生命周期贯穿于整个程序运行期间**。静态变量会被放在程序的静态数据存储区，这样可以在下一次调用的时候还可以保持原来的赋值，静态变量用 static 告知编译器，自己仅仅在变量的作用范围内可见，实际是依靠编译器来控制作用域
- static 局部静态变量:
  - 局部静态变量只能被初始化一次。与全局静态变量不同的是静态局部变量的作用域仅限于函数内部，它的作用域与函数内部的局部变量相同。实际上局部静态变量同样也存储在静态存储区，因此它的生命周期贯穿于整个程序运行期间

- `static` 静态函数:`static` 函数限制函数的作用域，仅可在定义该函数的文件内部调用 

- static 静态成员变量:
  - 静态成员变量是在类内进行声明，在类外进行定义和初始化，在类外进行定义和初始化的时候不要出现 static 关键字和 private、public、protected 访问规则。
  - 静态成员变量相当于类域中的全局变量，被类的所有对象所共享，包括派生类的对象，且只能该变量只能被初始化一次，不能在类的构造函数中对静态成员变量进行初始化
  - 静态成员变量可以作为成员函数的参数，而普通成员变量不可以
  - 静态数据成员的类型可以是所属类的类型，而普通数据成员的类型只能是该类类型的指针或引用。

- static 静态成员函数:
  - 静态成员函数不能调用非静态成员变量或者非静态成员函数，因为静态成员函数没有 this 指针。静态成员函数做为类作用域的全局函数。
  - 静态成员函数不能声明成虚函数（virtual）、const 函数和 volatile 函数

- `static` 对象: 静态对象的生存周期为整个程序的生命周期，而非静态对象的生命周期只存在于某个循环中

#### 全局变量和static变量的区别

- 全局变量（外部变量）的说明之前再冠以static就构成了静态的全局变量。**全局变量本身就是静态存储方式，静态全局变量也是静态存储方式**。这两者**在存储方式上并无不同**
- 这两者的区别在**于非静态全局变量的作用域是整个源程序**，当一个源程序由多个原文件组成时，**非静态的全局变量在各个源文件中都是有效的**。而**静态全局变量则限制了其作用域，即只在定义该变量的源文件内有效，在同一源程序的其它源文件中不能使用它**。由于静态全局变量的作用域限于一个源文件内，只能为该源文件内的函数公用，因此**可以避免在其他源文件中引起错误。**

#### 静态成员函数与普通成员函数的区别

-  静态成员函数实际上是一个全局函数,不依赖一个类的对象. 而属于类，不创建对象也可调用。普通成员函数依赖一个类的对象,也就是它有一个隐藏的调用参数（this）指针,必须指向一个类的对象。

-  静态函数只能访问类中的静态成员变量；
- 如果类的成员函数想作为回调函数来使用，如创建线程等，一般只能将它定义为静态成员函数才行。

####  C++ 中 const 关键字作用有哪些？

- const 修饰普通变量或者成员变量，定义成 const 常量，相较于宏常量可进行类型检查，节省内存空间，提高了效率。
- const 修饰函数参数，使得传递过来的函数参数的值不能改变。
- const 修饰成员函数，使得成员函数不能修改任何类型的成员变量（mutable 修饰的变量除外），也不能调用非 const 成员函数，因为非 const 成员函数可能会修改成员变量。
- `const` 变量: 定义成 `const` 常量，相较于宏常量，可进行类型检查，节省内存空间，提高了效率。被定义为 `const` 的变量是不可修改的。
- `const` 指针:
  - `const` 修饰指针指向的内容，则指针指向的内容不可变，但是指针本身的内容可以改变，指针常量
  - `const` 修饰指针，则指针为不可变量，指针指向的内容可以变，但指针本身不能变，常量指针
  - `const` 修饰指针和指针指向的内容，则指针和指针指向的内容都为不可变量
- const 引用:
  - const 引用是指向 const 对象的引用，可以读取变量，但不能通过引用修改指向的对象
  - 可以将 const 引用指向非 const 变量，但不能使用非 const 引用指向 const 变量。
  - const 引用可以初始化为不同类型的对象或者右值（如字面值常量），但非 const 引用不可以

- const 成员变量：
  - const 成员变量只能在类内声明、定义，在构造函数初始化列表中初始化。
  - const 成员变量只在某个对象的生存周期内是常量，对于整个类而言却是可变的，因为类可以创建多个对象，不同类的 const 成员变量的值是不同的。因此不能在类的声明中初始化 const 成员变量。
- const 函数参数与返回值:
  - 用 const 修饰函数参数，表明函数参数为常量，在函数内部不可以修改参数的内容，一般我们使用 const 指针或者 const 引用。函数返回值如果为指针或者引用，我们可以用 const 指针或者引用接受返回值，此时指向的内容则不可以修改。
- const 成员函数：
  - 不能修改成员变量的值，除非有 mutable 修饰；只能访问成员变量。
  - 不能调用非常量成员函数，以防修改成员变量的值。
- const 对象只能访问 const 成员函数,而非 const 对象可以访问任意的成员函数，包括 const 成员函数。
- 加上 mutable 修饰符的数据成员,对于任何情况下通过任何手段都可修改,自然此时的 const 成员函数是可以修改它的

####  C++ 中成员函数能够同时用 static 和 const 进行修饰？

- 否，因为static表示该函数为静态成员函数，为类所有；而const是用于修饰成员函数的，两者相矛盾

#### 全局变量和局部变量有什么区别？是怎么实现的？操作系统和编译器是怎么知道的？

- 生命周期不同：全局变量随主程序创建和创建，随主程序销毁而销毁；局部变量在局部函数内部，甚至局部循环体等内部存在，退出就不存在；
- 使用方式不同：通过声明后全局变量程序的各个部分都可以用到；局部变量只能在局部使用；分配在栈区。
- 内存分配位置不同：全局变量分配在全局数据段并且在程序开始运行的时候被加载。局部变量则分配在堆栈里面 

#### 类对象的大小受哪些因素影响？

- 类的非静态成员变量大小，**静态成员不占据类的空间**，成员函数也不占据类的空间大小；
- 内存对齐另外分配的空间大小，类内的数据也是需要进行内存对齐操作的；
- **虚函数的话，会在类对象插入vptr指针，加上指针大小；**
- 当该该类是某类的派生类，那么**派生类继承的基类部分的数据成员也会存在在派生类中的空间**中，也会对派生类进行扩展

#### C++ 中包含哪几种强制类型转换？他们有什么区别和联系？

- const_cast<type_id> (expression)：该运算符用来修改类型的const或volatile属性。除了const 或volatile修饰之外， type_id和expression的类型是一样的。它也允许从一个指针转换为整数类型,反之亦然. 这个操作符能够在非相关的类型之间转换. 操作结果只是简单的从一个指针到别的指针的值的二进制拷贝. 在类型之间指向的内容不做任何类型的检查和转换
- const_cast<type_id> (expression)：该运算符用来修改类型的const或volatile属性。除了const 或volatile修饰之外， type_id和expression的类型是一样的
- static_cast < type-id > (expression)：该运算符把expression转换为type-id类型，但没有运行时类型检查来保证转换的安全性。允许执行任意的隐式转换和相反转换动作（即使它是不允许隐式的）,例如：应用到类的指针上, 意思是说它允许子类类型的指针转换为父类类型的指针(这是一个有效的隐式转换), 同时, 也能够执行相反动作: 转换父类为它的子类 
- dynamic_cast <type-id> (expression)：该运算符把expression转换成type-id类型的对象。type-id 必须是类的指针、类的引用或者void*；如果 type-id 是类指针类型，那么expression也必须是一个指针，如果 type-id 是一个引用，那么 expression 也必须是一个引用.
  - 当用于多态类型时，它允许任意的隐式类型转换以及相反过程. 不过，与static_cast不同，在后一种情况里（注：即隐式转换的相反过程）,dynamic_cast 会检查操作是否有效. 也就是说, 它会检查转换是否会返回一个被请求的有效的完整对象。检测在 运行时进行. 如果被转换的指针不是一个被请求的有效完整的对象指针，返回值为NULL. 对于引用 类型，会抛出bad_cast异常


#### 结构与联合有何区别？

- 结构和联合都是由多个不同的数据类型成员组成, 但在任何同一时刻, 联合中只存放了一个被选中的成员（所有成员共用一块地址空间）, 而结构的所有成员都存在（不同成员的存放地址不同）
- 对于联合的不同成员赋值, 将会对其它成员重写, 原来成员的值就不存在了, 而对于结构的不同成员赋值是互不影响的。

#### 分别写出 bool, int, float, 指针类型的变量 a 与“零”的比较语句

- bool : if(!a) or if(a)

- int : if(a == 0)

- float : const EXPRESSION EXP = 0.000001

  if (a < EXP && a >-EXP)

- pointer : if(a != NULL) or if(a == NULL)

#### 指针数组和数组指针的区别？

- 指针数组，首先它是一个数组，数组的元素都是指针，也就是说该数组存储的是指针，数组占多少个字节由数组本身决定；
  - 指针数组是指数组内的元素是指针类型的，例如`int*p1[5]`，因为“[]”的优先级要比“\*”要高，所以 p1 先与“[]”结合，构成一个数组的定义，数组名为 p1，而“int*”修饰的是数组的内容，即数组的每个元素。也就是说，该数组包含 5 个指向 int 类型数据的指针
- 数组指针，首先它是一个指针，它指向一个数组，也就是说它是指向数组的指针，在 32 位系统下永远占 4 字节，至于它指向的数组占多少字节，这个不能够确定，要看具体情况
  - 数组指针是指指向数组的一个指针。例如语句“`int(*p2)[5]`”，“()”的优先级比“[]”高，“*”号和 p2 构成一个指针的定义，指针变量名为 p2，而 int 修饰的是数组的内容，即数组的每个元素。也就是说，p2 是一个指针，它指向一个包含 5 个 int 类型数据的数组，它是一个数组指针，数组在这里并没有名字，是个匿名数组

#### 函数指针怎么定义？

- 一个函数在编译时系统就会为这个函数代码分配一段存储空间，这段存储空间的首地址称为这个函数的地址。而且函数名表示的就是这个地址。既然是地址我们就可以定义一个指针变量来存放，这个指针变量就叫作函数指针变量，简称函数指针
- 函数指针的定义方式为：

```c++
// 函数返回值类型 (* 指针变量名) (函数参数列表);
int(*p)(int, int);
```

- “函数返回值类型”表示该指针变量可以指向具有什么返回值类型的函数；

- “函数参数列表”表示该指针变量可以指向具有什么参数列表的函数。这个参数列表中只需要写函数的参数类型即可。
- 需要注意的是：“（*指针变量名）”两端的括号不能省略，**括号改变了运算符的优先级**。如果省略了括号，就不是定义函数指针而是一个函数声明了，即声明了一个返回值类型为指针型的函数。

#### 如何对字符串string的operator=函数进行重载？

- 返回*this引用，如果不这样做，就会导致不能连续赋值，或导致调用时的隐式类型转换不能进行，或两种情况同时发生
- 重载函数中的参数应该是const string&，防止不小心修改原string
- 函数体判断自赋值情况；防止出现自己给自己赋值的情况，特别是通过自己的别名给自己赋值的情况，通过比较对象占用内存是否一致（if *this == rhs），不是则深拷贝，返回\*this

#### 深拷贝用什么函数？

- 对于字符串的深拷贝可以直接使用strcpy函数；使用strcpy时会有两个“字符串”；而使用“=”只有一个字符串；strcpy是将字符串拷贝到指定的地址
- 进行深拷贝的时候，会去重新开辟一块和待拷贝空间一样大的空间，然后把源空间里的数据拷贝到已开辟的目标空间里面去；所以深拷贝调用拷贝构造函数的过程当中，会先创建一个临时对象，然后利用swap交换目标串和临时串，实现深拷贝
- 所谓的浅拷贝就是让当前的指针指向一块已存在的区域，和其他指针共享同一块地址空间；浅拷贝所带来的问题是当程序结束的时候，两个对象都会去调用析构函数清零这块空间，而一块空间析构两次可能就会导致程序崩溃

#### strcpy会有什么问题？

- strcpy函数并不检查目的缓冲区的大小边界，而是将源字符串逐一的全部赋值给目的字符串地址起始的一块连续的内存空间，同时加上字符串终止符，如果参数dest所指的内存空间不够大，可能会造成缓冲溢出的错误情况；并且如果原串中存在'\0'的话会导致提前终止
- 具体原因：strcpy函数最重要一条语句为：`while ((*strDest++ = *strSrc++) != '\0')` ，意思是先将第二参数当前指针所指内容赋值给第一个参数当前指针所指空间，直到第二参数当前指针所指内容为'\0'，将'\0'赋值给第一个参数后结束，因为第一个参数当前指针也是不断自加的，跟第一个参数所具有的存储空间的大小没有关系，他们会一直赋值，超过第一参数存储空间也没有关系，直到第二参数遇到'\0'为止
- 解决方式：
  - 可以使用strncpy()用来复制字符串的前n个字符，不像strcpy()，strncpy()不会向dest追加结束标记'\0'；但其也存在部分缺陷：如果src的长度小于n个字节，则以NULL填充dest直到复制完n个字节；src 和 dest 所指的内存区域不能重叠，且dest 必须有足够的空间放置n个字符
  - 对于遇到"\0"提前停止的问题，可以使用memcpy解决；C++程序还是尽量使用string字符串

#### strcpy和memcpy的区别

- 复制的内容不同。strcpy只能复制字符串，而memcpy可以复制任意内容，例如字符数组、整型、结构体、类等。 
- 复制的方法不同。strcpy不需要指定长度，它遇到被复制字符的串结束符"\0"才结束，所以容易溢出。memcpy则是根据其第3个参数决定复制的长度。 
- 用途不同。通常在复制字符串时用strcpy，而需要复制其他类型数据时则一般用memcpy

#### C++ 是不是类型安全的？

- 不是。两个不同类型的指针之间可以强制转换（用reinterpret cast)。C#是类型安全的。
- C++使用得当，它将远比C更有类型安全性。相比于C，C++提供了一些新的机制保障类型安全：
  - 操作符**new**返回的指针类型严格与对象匹配，而不是void*；
  - C中很多以void*为参数的函数可以改写为C++模板函数，而**模板是支持类型检查**的；
  - **引入const关键字**代替#define constants，它是有类型、有作用域的，而#define constants只是简单的文本替换；
  - 一些#define宏可被**改写为inline函数**，结合函数的重载，可在类型安全的前提下支持多种类型，当然改写为模板也能保证类型安全；
  - C++提供了**dynamic_cast关键字**，使得转换过程更加安全，因为dynamic_cast比static_cast涉及更多具体的类型检查

#### 针对lambda捕获变量，怎么保证他是有效的呢

- 引用捕获和返回引用有相同的问题和限制，如果采用引用方式捕获一个变量，就必须确保被引用对象在lambda执行时时存在的。所以一般建议保持lambda变量捕获简单化，对于普通变量，如int,float和string等类型，通常采用值捕获。
- 对于捕获指正，迭代器或采用引用捕获，就必须要保证其在lambda执行时，迭代器、指正和引用的对象依然存在，且有预期值

### C++11 新特性

#### 移动构造函数和拷贝构造函数的区别

- 拷贝构造函数是先将传入的参数对象进行一次深拷贝，再传给新对象。这就会有一次拷贝对象的开销，并且进行了深拷贝，就需要给对象分配地址空间。而移动构造函数就是为了解决这个拷贝开销而产生的。移动构造函数首先将传递参数的内存地址空间接管，然后将内部所有指针设置为nullptr，并且在原地址上进行新对象的构造，最后调用原对象的的析构函数，这样做既不会产生额外的拷贝开销，也不会给新对象分配内存空间。
- 对于指针参数来讲，移动构造函数是对传递参数进行一次浅拷贝。也就是说如果参数为指针变量，进行拷贝之后将会有两个指针指向同一地址空间，这个时候如果前一个指针对象进行了析构，则后一个指针将会变成野指针，从而引发错误。所以当变量是指针的时候，要将指针置为空，这样在调用析构函数的时候会进行判断指针是否为空，如果为空则不回收指针的地址空间，这样就不会释放掉前一个指针。
- 当类中同时包含拷贝构造函数和移动构造函数时，如果使用临时对象初始化当前类的对象，编译器会优先调用移动构造函数来完成此操作。只有当类中没有合适的移动构造函数时，编译器才会退而求其次，调用拷贝构造函数。在实际开发中，通常在类中自定义移动构造函数的同时，会再为其自定义一个适当的拷贝构造函数，由此当用户利用右值初始化类对象时，会调用移动构造函数；使用左值（非右值）初始化类对象时，会调用拷贝构造函数。


#### 你是怎么理解右值的

- 通俗一点讲：左值就是那些能够出现在赋值符号左边的东西，右值就是那些可以出现在赋值符号右边的东西。（前提是合法）
- 左值：指表达式结束后依然存在的持久对象。可以取地址，可以通过内置（不包含重载） & 来获取地址，我们可以将一个右值赋给左值。
- 右值：表达式结束就不再存在的临时对象。不可取地址，不可以通过内置（不包含重载） & 来获取地址。由于右值不可取地址，因此我们不能将任何值赋给右值。
- 使用 = 进行赋值时，= 的左边必须为左值，右值只能出现在 = 的右边。函数返回值即可以是左值，也可以是右值
- 左值转换为右值：可以通过 `std::move` 可以将一个左值强制转化为右值，继而可以通过右值引用使用该值，以用于移动语义，从而完成将资源的所有权进行转移。

#### 右值引用是什么？简单介绍下

- 右值引用 （Rvalue Referene） 是 C++ 11 中引入的新特性 , 它实现了转移语义 （Move Sementics）和精确传递 （Perfect Forwarding），&& 作为右值引用的声明符。右值引用必须绑定到右值的引用，通过 && 获得。**右值引用只能绑定到一个将要销毁的对象上，因此可以自由地移动其资源**。
- 从实践角度讲，它能够完美解决 C++ 中长久以来为人所诟病的临时对象效率问题。从语言本身讲，它健全了 C++ 中的引用类型在左值右值方面的缺陷。从库设计者的角度讲，它给库设计者又带来了一把利器。从使用者的角度来看，可以获得效率的提升，避免对象在传递过程中重复创建。
- 右值引用两个主要功能：
  - 消除两个对象交互时不必要的对象拷贝，节省运算存储资源，提高效率。
  - 能够更简洁明确地定义泛型函数。

#### move函数的作用

- 标准库 move() 函数：通过该函数可获得绑定到左值上的右值引用。通过 move 获取变量的右值引用，从而可以调用对象的移动拷贝构造函数和移动赋值构造函数。

### STL相关

#### STL 迭代器如何实现

- 迭代器是一种抽象的设计理念，通过迭代器可以在不了解容器内部原理的情况下遍历容器，除此之外，**STL中迭代器一个最重要的作用就是作为容器与STL算法的粘合剂**
- 迭代器的作用就是提供**一个遍历容器内部所有元素的接口**，因此迭代器内部必须保存一个与容器相关联的指针，然后重载各种运算操作来遍历，其中**最重要的是*运算符与->运算**符，以及++、--等可能需要重载的运算符重载。这和C++中的智能指针很像，智能指针也是将一个指针封装，然后通过引用计数或是其他方法完成自动释放内存的功能。
- 最常用的迭代器的相应型别有五种：value type、difference type、pointer、reference、iterator catagoly;

#### STL 中 list 与 deque 之间的区别？

- list不再能够像vector一样以普通指针作为迭代器，因为其节点不保证在存储空间中连续存在；l
- ist插入操作和结合才做都不会造成原有的list迭代器失效;
- list不仅是一个双向链表，而且还是一个环状双向链表，所以它只需要一个指针；
- list不像vector那样有可能在空间不足时做重新配置、数据移动的操作，所以插入前的所有迭代器在插入操作之后都仍然有效；
- deque是一种双向开口的连续线性空间，所谓双向开口，意思是可以在头尾两端分别做元素的插入和删除操作；可以在头尾两端分别做元素的插入和删除操作；
- deque和vector最大的差异，一在于deque允许常数时间内对起头端进行元素的插入或移除操作，二在于deque没有所谓容量概念，因为它是动态地以分段连续空间组合而成，随时可以增加一段新的空间并链接起来，deque没有所谓的空间保留功能

#### STL里面的queue是线程安全的嘛

- STL中的queue是非线程安全的，一个组合操作：front(); pop()先读取队首元素然后删除队首元素，若是有多个线程执行这个组合操作的话，可能会发生执行序列交替执行，导致一些意想不到的行为
- 在队列内部增加条件变量和互斥锁实现线程安全，wait_pop，try_pop等接口定义；在push的同时进行加锁以及条件变量+1的操作等等
- STL容器不是线程安全的。对于vector，即使写方（生产者）是单线程写入，但是并发读的时候，由于潜在的内存重新申请和对象复制问题，会导致读方（消费者）的迭代器失效。实际表现也就是招致了core dump。另外一种情况，如果是多个写方，并发的push_back()，也会导致core dump。

#### map 如何保证key唯一

- 自定义排序规则，重载<操作符或者定义一个函数对象
- 

#### vector的push_back和emplace_back这两个有什么区别

- push_back() 向容器尾部添加元素时，首先会创建这个元素，然后再将这个元素拷贝或者移动到容器中（如果是拷贝的话，事后会自行销毁先前创建的这个元素）；push_back() 在底层实现时，会优先选择调用移动构造函数，如果没有才会调用拷贝构造函数
-  emplace_back() 在实现时，则是直接在容器尾部创建这个元素，省去了拷贝或移动元素的过程。emplace_back() 的执行效率比 push_back() 高

### 内存管理相关

#### malloc和new区别

- 从**定义**来说，malloc和free是标准库函数，不支持重载，new和delete是运算符；
- 从**空间操作**来说，malloc仅仅分配内存空间，free仅仅回收空间，不具备调用构造函数和析构函数的功能，用malloc分配空间存储类的对象存在风险，而newdelete会调用构造函数和析构函数；对于非内部数据类型的对象而言，光用maloc/free无法满足动态对象的要求。
- 从**返回值**来看，malloc和free返回的是void类型指针（必须进行类型转换），new和delete返回的是具体类型指针

#### delete 和delete [] 的区别

- delete 只会调用一次析构函数，而 delete[] 会调用每一个成员函数的析构函数。

#### 子类析构时要调用父类的析构函数吗？

- 析构函数调用的次序是先派生类的析构后基类的析构，也就是说在基类的的析构调用的时候,派生类的信息已经全部销毁了
- 定义一个对象时先调用基类的构造函数、然后调用派生类的构造函数；析构的时候恰好相反：先调用派生类的析构函数、然后调用基类的析构函数。

#### 介绍一下C++里的内存管理

在C++中，内存分成5个区，他们分别是堆、栈、全局/静态存储区和常量存储区和代码区。

- 栈，在执行函数时，函数内局部变量的存储单元都可以在栈上创建，**函数执行结束时这些存储单元自动被释放**。栈内存分配运算内置于处理器的指令集中，**效率很高**，但是分配的内存容量有限。
- 堆，就是那些由new分配的内存块，他们的释放编译器不去管，由应用程序去控制，一般一个new就要对应一个delete。如果程序员在堆区申请了空间，又忘记将这片内存释放掉，就会造成内存泄露的问题，导致后面一直无法访问这片存储区域。但程序退出后，系统自动回收资源。分配方式类似于链表。
- 全局/静态存储区，内存在程序编译的时候就已经分配好，这块内存在程序的整个运行期间都存在。它主要存放静态数据（局部static变量，全局static变量）、全局变量和常量。
- 常量存储区，这是一块比较特殊的存储区，他们里面存放的是常量字符串，不允许修改。
- 代码区，存放程序的二进制代码

#### 堆和栈的区别

- 申请方式不同:栈由系统自动分配(连续存储空间);堆是自己申请和释放的（一个记录空闲内存地址的链表）
- 申请大小限制不同:栈顶和栈底是之前预设好的，栈是向栈底扩展，大小固定； 堆向高地址扩展，是不连续的内存区域，大小可以灵活调整。

- 申请效率不同：栈由系统分配，速度快，不会有碎片；堆由程序员分配，速度慢，且会有碎片

- 栈用来保存定义在函数内的非static对象，如局部变量，仅在其定义的程序块运行时才存在；堆存放动态分配的对象生存期由程序控制

- PS：静态内存用来保存static对象，类static数据成员以及定义在任何函数外部的变量，static对象在使用之前分配，程序结束时销毁；栈和静态内存的对象由编译器自动创建和销毁

#### 堆和自由存储区的区别？

- 总的来说，堆是C语言和操作系统的术语，是操作系统维护的一块动态分配内存；自由存储是C++中通过new与delete动态分配和释放对象的抽象概念。他们并不是完全一样
- 从技术上来说，堆（heap）是C语言和操作系统的术语。堆是操作系统所维护的一块特殊内存，它提供了动态分配的功能，当运行程序调用malloc()时就会从中分配，稍后调用free可把内存交还。而自由存储是C++中通过new和delete动态分配和释放对象的抽象概念，通过new来申请的内存区域可称为自由存储区。基本上，所有的C++编译器默认使用堆来实现自由存储，也即是缺省的全局运算符new和delete也许会按照malloc和free的方式来被实现，这时藉由new运算符分配的对象，说它在堆上也对，说它在自由存储区上也正确


#### 栈的生长方向？

对于堆来讲，生长方向是向上的，也就是向着内存地址增加的方向；对于栈来讲，它的生长方式是向下的，是向着内存地址减小的方向（高地址向低地址方向）增长。

#### 野指针是什么？如何避免野指针？

- 野指针：指向内存被释放的内存或者没有访问权限的内存的指针。
- “野指针”的成因主要有3种：
  - 指针变量没有被初始化。任何指针变量刚被创建时不会自动成为NULL指针，它的缺省值是随机的，它会乱指一气。所以，指针变量在创建的同时应当被初始化，要么将指针设置为NULL，要么让它指向合法的内存
  - 指针p被free或者delete之后，没有置为NULL；
  - 指针操作超越了变量的作用范围
- 避免方法：对指针进行初始化
  - 将指针初始化为NULL。char * p = NULL;
  - 用malloc分配内存；char * p = (char * )malloc(sizeof(char));
  - 用已有合法的可访问的内存地址对指针初始化
  - 指针用完后释放内存，将指针赋NULL



#### 什么是内存泄漏？面对内存泄漏和指针越界，你有哪些方法？你通常采用哪些方法来避免和减少这类错误？

- 用动态存储分配函数动态开辟的空间，在使用完毕后未释放，结果导致一直占据该内存单元即为内存泄露。
- **避免内存泄露的几种方式**：
  - 计数法：使用new或者malloc时，让该数+1，delete或free时，该数-1，程序执行完打印这个计数，如果不为0则表示存在内存泄露

  - 一定要将基类的析构函数声明为**虚函数**

  - 对象数组的释放一定要用**delete []**

  - 有new就有delete，有malloc就有free，保证它们一定成对出现
- **检测工具**：Linux下可以使用**Valgrind工具**；Windows下可以使用**CRT库**

#### 内存泄漏的检测原理

- 检测内存泄漏的关键是要能截获住对分配内存和释放内存的函数的调用。截获住这两个函数，我们就能跟踪每一块内存的生命周期，比如，每当成功的分配一块内存后，就把它的指针加入一个全局的list中；每当释放一块内存，再把它的指针从list中删除。这样，当程序结束的时候，list中剩余的指针就是指向那些没有被释放的内存。
-  如果要检测堆内存的泄漏，那么需要截获住malloc / realloc / free和new/delete就可以了。对于其他的泄漏，可以采用类似的方法，截获住相应的分配和释放函数
- 要想检测内存泄漏，就必须对程序中的内存分配和释放情况进行记录，所能够采取的办法就是重载所有形式的operator new 和 operator delete，截获 new operator 和 delete operator 执行过程中的内存操作信息。

#### 编译器是什么

- 编译器能够识别代码中的词汇、句子以及各种特定的格式，并将他们转换成计算机能够识别的二进制形式，这个过程称为编译（Compile）
- 编译器就是**将“一种语言（通常为高级语言）”翻译为“另一种语言（通常为低级 语言 ）”的程序**。 一个现代编译器的主要工作流程：源代码 (source code) → 预处理器 (preprocessor) → 编译器 (compiler) → 目标代码 (object code) → 链接器 (Linker) → 可执行 程序 (executables)

#### main 函数执行之前，还会执行什么代码？

- 全局对象的构造函数会在main函数之前执行。

#### 静态链接和动态链接

- 静态链接：函数和数据被编译进一个二进制文件。在使用静态库的情况下，在编译链接可执行文件时，链接器从库 中**复制这些函数和数据并把它们和应用程序的其它模块组合起来创建最终的可执行文件**。
  - 空间浪费：因为每个可执行程序中对所有需要的目标文件都要有一份副本，所以如果多个程序对同一个 目标文件都有依赖，会出现同一个目标文件都在内存存在多个副本；
  - 更新困难：每当库函数的代码修改了，这个时候就需要重新进行编译链接形成可执行程序。
  - 运行速度快：但是静态链接的优点就是，在可执行程序中已经具备了所有执行程序所需要的任何东西， 在执行的时候运行速度快。

- 动态链接：动态链接的基本思想是把程序**按照模块拆分成各个相对独立部分**，在**程序运行时才将它们链接在一起形成一个完整的程序**，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件。
  - 共享库：就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多分副本，而是这多个程序在执行时共享同一份副本；
  - 更新方便：更新时只需要替换原来的目标文件，而无需将所有的程序再重新链接一遍。当程序下一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。
  - 性能损耗：因为把链接推迟到了程序运行时，所以每次执行程序都需要进行链接，所以性能会有一定损失

#### C++源文件到可执行文件的一个过程是什么

- 预处理阶段：处理以 # 开头的预处理命令；删除所有的注释等
- 编译阶段：翻译成汇编文件（词法分析、语法分析、语义分析及优化）；
- 汇编阶段：将汇编文件翻译成可重定位目标文件（机器可以执行的指令(机器码文件)）；
- 链接阶段：将可重定位目标文件和 .o 等单独预编译好的目标文件（不同的源文件产生的目标文件）进行合并，得到最终的可执行目标文件

#### C++为什么要有内存对齐，怎么实现的

- 使用内存对齐的原因：
  - 性能原因：数据结构(尤其是栈)应该尽可能地在自然边界上对齐。原因在于，为了访问未对齐的内存，处理器需要作两次内存访问；而对齐的内存访问仅需要一次访问。
  -  平台原因：不是所有的硬件平台都能访问任意地址上的任意数据的；某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常。
  - 空间原因：没有进行内存对齐的结构体或类会浪费一定的空间，当创建对象越多时，消耗的空间越多
- 在 C/C++ 中，结构体/类是一种复合数据类型，编译器为每个成员按其自然边界（alignment）分配空间。各个成员按照它们被声明的顺序在内存中顺序存储，第一个成员的地址和整个结构的地址相同；如果一个变量的内存地址正好位于它长度的整数倍，他就被称做**自然对齐**。
- 由于各个成员按照它们被声明的顺序在内存中顺序存储，所以**不同的声明顺序导致了结构体所占空间的不同**
- 对齐原则
  1. 数据类型自身的对齐值：对于 char 型数据，其自身对齐值为1，对于 short 型为2，对于 int，float，double 类型，其自身对齐值为 4，单位字节。

  2. 结**构体或者类的自身对齐值：其成员中自身对齐值最大的那个值**。

  3. 指定对齐值：#pragma pack (value) 时的指定对齐值 value。

  4. 数据成员、结构体和类的有效对齐值：**自身对齐值和指定对齐值中小的那个值**。







### 多态、虚函数相关问题

#### 多态、虚函数如何实现的

- 同一事物表现出不同事物的能力，即向不同对象发送同一消息，不同的对象在接收时会产生不同的行为**（重载实现编译时多态，虚函数实现运行时多态）**多态性是允许将父对象设置成为和一个或更多他的子对象相等的技术，赋值之后，父对象就可以根据**当前赋值给它的子对象的特性**以不同的方式运作。**简单一句话：允许将子类类型的指针赋值给父类类型的指针**

- **虚函数是通过一张虚函数表实现的**，有多少个虚函数，就有多少个指针；在这个表中，主要是**一个类的虚函数的地址表**，这张表解决了继承、覆盖的问题；实际上在编译的时候，编译器会自动加上虚表
- 虚函数的作用实现**动态联编**，也就是说在**程序运行阶段动态的选择合适的成员函数**，在定义了虚函数之后，可以在基类的派生类中对虚函数重新定义。
- 虚表的使用方法是**如果派生类在自己定义中没有修改基类的虚函数，就指向基类的虚函数**；如果派生类改写了基类的虚函数，这时续表则将原来指向基类的虚函数的地址**替换为指向自身虚函数的指针**。
- **必须通过基类类型的引用或指针进行函数调用才会发生多态**

#### 虚函数表什么时候初始化？

- 在派生类定义对象时，程序运行会自动调用构造函数，在构造函数中创建虚表并对虚表初始化;构造函数是在**程序的执行到达定义这个对象的位置时**调用的（运行期）

#### 一个类创建多个对象有几个虚函数表

- 一个类的多个对象是**共享一个虚函数表的**，虚函数表内**有几个虚函数就有多少个指针**
- **子类对象具有两个虚函数表指针**，一个指向自己的，一个指向父类的虚函数表指针；子类是有父类的**整个数据的**，子类对象内存中存在父类的一个实例
- 针对**单**继承：若没有覆盖虚函数，则父类的虚函数在子类的虚函数前面；若覆盖了虚函数则**覆盖的f()函数被放到了虚表中原来父类虚函数的位置，没有被覆盖的函数依旧**；
- 针对**多重**继承：**若没有覆盖虚函数，每个父类都有自己的虚表， 子类的成员函数被放到了第一个父类的表中**（所谓的第一个父类是按照声明顺序来判断的）；**若有覆盖虚函数f，则父类虚函数表中的f()的位置被替换成了子类的函数指针**

<img src="https://img-blog.csdn.net/20181023164040667?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzU5MDIy/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="在这里插入图片描述" style="zoom:67%;" />

#### 纯虚函数的作用是什么

- 纯虚函数也可以叫抽象函数，一般来说它只有函数名、参数和返回值类型，不需要函数体；在许多情况下，在基类中不能对虚函数给出有意义的实现，而把它声明为纯虚函数，它的实现留给该基类的派生类去做。这就是纯虚函数的作用。
- 纯虚函数主要是为了实现一个接口，起到一个规范的作用，规范继承这个类的程序员必须实现这个函数；
- 使用情况：当想在基类中抽象出一个方法，且该基类只做能被继承，而不能被实例化；这个方法必须在派生类（derived class）中被实现

#### 一个对象访问普通成员函数和虚函数哪个更快？

- 访问普通成员函数更快，因为普通成员函数的地址在编译阶段就已确定，因此在访问时直接调用对应地址的函数，而虚函数在调用时，需要首先在虚函数表中寻找虚函数所在地址，因此相比普 通成员函数速度要慢一些


#### 重载、重写、隐藏的区别？

- 重载：是指同一可访问区内被声明的几个具有不同参数列（参数的类型，个数，顺序不同）的同名函数，根据参数列表确定调用哪个函数，重载不关心函数返回类型。编译器根据函数不同的参数表，对同名函数的名称做修饰，然后这些同名函数就成了不同的函数

- 重写(覆盖)：是指**派生类中存在重新定义的函数**。其**函数名，参数列表，返回值类型，所有都必须同基类中被重写的函数一致**。只有函数体不同，派生类调用时会调用派生类的重写函数，不会调用被重写函数。重写的基类中被重写的函数必须有**virtual修饰**；在编译期间是无法确定的

- 隐藏：是指**派生类**的函数屏蔽了与其同名的基类函数，注意只要**同名函数**，不管参数列表是否相同，基类函数都会被隐藏

- **重载和重写的区别：**

  （1）**范围区别**：重写和被重写的函数在不同的类中，重载和被重载的函数在同一类中。

  （2）**参数区别**：重写与被重写的函数参数列表一定相同，重载和被重载的函数参数列表一定不同。

  （3）**virtual的区别**：重写的基类必须要有virtual修饰，重载函数和被重载函数可以被virtual修饰，也可以没有

- **隐藏和重写，重载的区别：**

  （1）**与重载范围不同**：隐藏函数和被隐藏函数在不同类中。

  （2）参数的区别：**隐藏函数和被隐藏函数参数列表可以相同，也可以不同**，但**函数名一定同**；当参数不同时，无论基类中的函数是否被virtual修饰，基类函数都是被隐藏，而不是被重写。 

#### 有继承关系的父类析构函数定义成virtual，为什么？

- 由于类的多态性，基类指针可以指向派生类的对象，如果删除该基类的指针，就会调用该指针指向的派生类析构函数，而派生类的析构函数又自动调用基类的析构函数，这样整个派生类的对象完全被释放
- 如果析构函数不被声明成虚函数，则**编译器实施静态绑定**，在删除基类指针时，**只会调用基类的析构函数而不调用派生类析构函数**，这样就会造成派生类对象析构不完全，造成内存泄漏

#### 内联函数、构造函数、静态成员函数可以是虚函数吗？

都不可以

- 内联函数需要在编译阶段展开，而虚函数是运行时动态绑定的，编译时无法展开； 
- 构造函数在进行调用时还不存在父类和子类的概念，父类只会调用父类的构造函数，子类调用子类的，因此不存在动态绑定的概念；
- 静态成员函数是以类为单位的函数，与具体对象无关，虚函数是 与对象动态绑定的，因此是两个不冲突的概念；

#### 构造函数和析构函数中可以调用虚函数吗？

- 可以，但是没有动态绑定的效果，父类构造函数中调用的仍然是父类版本的函数，子类中调用的仍然是子类版本的函数
- 因为父类对象会在子类之前进行构造，此时子类部分的数据成员还未初始化，因此调用子类的虚函数时不安全的，故而C++不会进行动态联编
- 析构函数是用来销毁一个对象的，在销毁一个对象时，先调用子类的析构函数，然后再调用基类的析构函数。所以在调用基类的析构函数时，派生类对象的数据成员已经销毁，这个时候再调用子类的虚函数没有任何意义

#### 简述 C++ 中虚继承的作用及底层实现原理？

- 虚继承用于解决多继承条件下的菱形继承问题，底层实现原理与编译器相关，**一般通过虚基类指针实现，即各对象中只保存一份父类的对象**，多继承时通过虚基类指针引用该公共对象，从而避 免菱形继承中的二义性问题

#### 什么是类的继承？

类与类之间的关系

- has-A包含关系，用以描述一个类由多个部件类构成，实现has-A关系用类的成员属性表示，即一个类的成员属性是另一个已经定义好的类；
- use-A，一个类使用另一个类，通过类之间的成员函数相互联系，定义友元或者通过传递参数的方式来实现；
- is-A，继承关系，关系具有传递性；



### 智能指针

#### 简述智能指针原理

- 智能指针是一种资源管理类，通过对原始指针进行封装，在资源管理对象进行析构时对指针指向的内存进行释放；通常使用引用计数方式进行管理

#### sharedPtr和uniquePtr，weakptr

- shared_ptr采用引用计数器的方法，允许多个智能指针指向同一个对象，每当多一个指针指向该对象时，指向该对象的所有智能指针内部的引用计数加1，每当减少一个智能指针指向对象时，引用计数会减1，当计数为0的时候会自动的释放动态分配的资源
  - 将一个计数器与类指向对象相关联，引用计数器跟踪共有多少个类对象共享同一指针
  - 每次创建类的新对象时，初始化指针并将引用计数置为1
  - 当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数
  - 对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数
  - 调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）
- **unique_ptr**采用的是独享所有权语义，一个非空的unique_ptr总是拥有它所指向的资源。转移一个unique_ptr将会把所有权全部从源指针转移给目标指针，源指针被置空；
  - unique_ptr不支持普通的拷贝和赋值操作，不能用在STL标准容器中；局部变量的返回值除外（因为编译器知道要返回的对象将要被销毁）；如果拷贝一个unique_ptr，那么拷贝结束后，这两个unique_ptr都会指向相同的资源，造成在结束时对同一内存指针多次释放而导致程序崩溃。
  - unique_ptr指针本身的生命周期：从unique_ptr指针创建时开始，直到离开作用域。离开作用域时，若其指向对象，则将其所指对象销毁(默认使用delete操作符，用户可指定其他操作)。
  - unique_ptr指针与其所指对象的关系：在智能指针生命周期内，可以改变智能指针所指对象，如创建智能指针时通过构造函数指定、通过reset方法重新指定、通过release方法释放所有权、通过移动语义转移所有权
- **weak_ptr** 是一种不控制对象生命周期的智能指针, 它指向一个 shared_ptr 管理的对象. 进行该对象的内存管理的是那个强引用的 shared_ptr. 解决循环计数问题
  - weak_ptr只是提供了对管理对象的一个访问手段。
  - weak_ptr 设计的目的是为配合 shared_ptr 而引入的一种智能指针来协助 shared_ptr 工作, 它只可以从一个 shared_ptr 或另一个 weak_ptr 对象构造, 它的构造和析构不会引起引用记数的增加或减少

#### C++智能指针循环引用如何解决

- 虽然智能指针会减少内存泄漏的可能性，但是如果使用智能指针的方式不对，一样会造成内存泄漏。比较典型的情况是循环引用问题
- shared_ptr改成weak_ptr即可，由于weak_ptr不会增加shared_ptr的引用计数，因此**引用计数对象当ref_count减至zero时会销毁其管理的资源**

#### 多线程下的shared_ptr不会导致资源一直使用嘛

- shared_ptr引用计数是原子的(shared_ptr底层中在对引用计数进行访问之前，首先对其加锁，当访问完毕之后，在对其进行解锁)，它的析构函数原子地将引用计数减去1，当多个线程对同一对象析构时，也只会出现执行顺序的交错，不会有内存泄露。
- 在我们通过shared_ptr.get()和*解引用获得了指向该内存的原始指针，那么后面都是对原始指针的操作，所以需要自己控制线程的安全
- 如何使多个线程可以对同一个shared_ptr实体进行同时读写？运用weak_ptr这个助手检测指针是否被释放，使用**weak_ptr.lock**函数就可以得到一个shared_ptr的指针，如果该指针已经被其它地方释放，它则返回一个空的shared_ptr，也可以使用weak_ptr.expired()来判断一个指针是否被释放
- **多个shared_ptr对象对其所管理的资源的访问不是线程安全的**。如果不使用锁这会造成线程安全问题

## 五、MySQL

### MySQL索引相关问题

#### 什么是索引？

- 索引的定义就是帮助存储引擎快速获取数据的一种数据结构，形象的说就是**索引是数据的目录**
- 存储引擎就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法，索引和数据位于存储引擎中

#### 索引的分类

- 按「数据结构」分类：**B+tree索引、Hash索引、Full-text索引**。
  - InnoDB 是在 MySQL 5.5 之后成为默认的 MySQL 存储引擎，B+Tree 索引类型也是 MySQL 存储引擎采用最多的索引类型
  - **创建的主键索引和二级索引默认使用的是 B+Tree 索引**
- 按「物理存储」分类：**聚簇索引（主键索引）、二级索引（辅助索引）**。
  - 主键索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；
  - 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。
- 按「字段特性」分类：**主键索引、唯一索引、普通索引、前缀索引**。
  - 主键索引就是建立在主键字段上的索引，通常在创建表的时候一起创建，一张表最多只有一个主键索引，索引列的值不允许有空值
  - 唯一索引建立在 UNIQUE 字段上的索引，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值
  - 普通索引就是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为 UNIQUE
  - 前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上；使用前缀索引的目的是为了减少索引占用的存储空间，提升查询效率
- 按「字段个数」分类：**单列索引、联合索引**
  - 通过将多个字段组合成一个索引，该索引就被称为联合索引，联合索引存在**最左匹配原则**，按照最左优先的方式进行索引的匹配
  - MySQL 5.6 引入的**索引下推优化**（index condition pushdown)， **可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数**
  - 实际开发工作中**建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到**

#### 什么时候需要 / 不需要创建索引？

- 索引也是有缺点的，比如：

  - 需要占用物理空间，数量越大，占用空间越大；

  - 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增大；

  - 会降低表的增删改的效率，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护

- 什么时候适用索引？

  - 字段有唯一性限制的，比如商品编码；
  - 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
  - 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的

- 什么时候不需要创建索引？

  - `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。
  - 字段中存在大量重复数据，不需要创建索引，MySQL 有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描
  - 表数据太少的时候，不需要创建索引；
  - 经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的

#### 有什么优化索引的方法？

- 前缀索引优化；
  - 使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度
  - 不过前缀索引有一定的局限性，例如：
    - order by 就无法使用前缀索引；
    - 无法把前缀索引用作覆盖索引
- 覆盖索引优化；
  - 覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作
- 主键索引最好是自增的；
  - **如果使用自增主键**，每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次**插入一条新记录，都是追加操作，不需要重新移动数据**，因此这种插入数据的方法效率非常高
  - **如果使用非自增主键**，由于每次插入主键的索引值都是随机的，因此可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，通常将这种情况称为**页分裂**。**页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率**。
- 索引最好设置为 NOT NULL：
  - 索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂
  - NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题，会导致更多的存储空间占用
- 防止索引失效；
  - 使用左或者左右模糊匹配
  - 在查询条件中对索引列做了计算、函数、类型转换操作
  - 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效
  - WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效

#### mysql索引如何实现的

- Innodb 存储引擎采用了 B+ 树作为了索引的数据结构，Innodb 使用的 B+ 树有一些特别的点，比如：

  - B+ 树的叶子节点之间是用双向链表进行连接，这样的好处是既能向右遍历，也能向左遍历

  - B+ 树点节点内容是数据页，数据页里存放了用户的记录以及各种信息，每个数据页默认大小是 16 KB

- Innodb 根据索引类型不同，分为聚集和二级索引。他们区别在于，聚集索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚集索引的叶子节点，而二级索引的叶子节点存放的是主键值，而不是实际数据。

  - 因为表的数据都是存放在聚集索引的叶子节点里，所以 InnoDB 存储引擎一定会为表创建一个聚集索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个，而二级索引可以创建多个

#### 采用B+作为索引的原因：

- B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更矮胖，查询底层节点的磁盘 I/O次数会更少。
- B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化
- B+ 树叶子节点之间用链表连接了起来，有利于范围查询，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树

#### 介绍聚簇索引，非聚簇索引，索引覆盖和回表查询

- 表的数据都是存放在聚簇索引的叶子节点里，所以 InnoDB 存储引擎一定会为表创建一个聚簇索引，且由于数据在物理上只会保存一份，所以聚簇索引只能有一个；聚簇索引的叶子节点存放的是实际数据，所有完整的用户记录都存放在聚簇索引的叶子节点
- 一张表只能有一个聚簇索引，那为了实现非主键字段的快速搜索，就引出了二级索引（非聚簇索引/辅助索引），它也是利用了 B+ 树的数据结构，但是二级索引的叶子节点存放的是主键值，不是实际数据。
- 如果某个查询语句使用了二级索引，但是查询的数据不是主键值，这时在二级索引找到主键值后，需要去聚簇索引中获得数据行，这个过程就叫作回表，也就是说要查两个 B+ 树才能查到数据。
- 当查询的数据是主键值时，因为只在二级索引就能查询到，不用再去聚簇索引查，这个过程就叫作索引覆盖，也就是只需要查一个 B+ 树就能找到数据

#### 给身份证创建索引，如何做

- **前缀索引**定义好长度的话，就可以做到既节省空间，又不用额外增加太多的查询成本（会增加查询扫描次数，且不能使用覆盖索引）
- 针对身份证这个场景，身份证的前6位是地区编码，然后8位是个人出生日期，最后是6位随机编码；如果维护的数据库是一个市的公民信息系统，为了提高索引的**区分度**就不可以选用前六位做前缀索引了，可能需要创建长度为 12 以上的前缀索引，才能够满足区分度要求。但是索引选取的越长，占用的磁盘空间就越大，相同的数据页能放下的索引值就越少，搜索的效率也就会越低，为了改善这一个问题，**可以采用倒序索引或者hash字段进行创建索引**

```sql
select field_list from t where id_card = reverse('input_id_card_string');
```

- 但是，倒序索引的方法每次写和读的时候，都需要额外调用一次 reverse 函数，hash 字段的方式需要额外调用一次 crc32() 函数，也会带来一定的CPU损耗，从查询效率上看，使用 hash 字段方式的查询性能相对更稳定一些

### MySQL锁相关问题

#### 如何打破MySql死锁

死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。

在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：

- **设置事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。

- **开启主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

#### 乐观锁和悲观锁本质的区别是什么

- **乐观锁**：乐观锁在操作数据时非常乐观，认为别人不会同时修改数据。因此乐观锁不会上锁，只是在执行更新的时候判断一下在此期间别人是否修改了数据：如果别人修改了数据则放弃操作，否则执行操作。
  - **乐观锁适用于多读的应用类型，这样可以提高吞吐量**，像数据库提供的类似于**write_condition机制**
  - **乐观锁的实现方式主要有两种：CAS**（Compare And Swap）**机制和版本号机制**
  - **CAS的缺点**：
    - **ABA问题**，栈顶问题：一个栈的栈顶经过两次(或多次)变化又恢复了原值，但是栈可能已发生了变化，比较有效的方案是引入版本号，内存中的值每发生一次变化
    - **高竞争下的开销问题 在并发冲突概率大的高竞争环境下，如果CAS一直失败，会一直重试，CPU开销较大**
    - **功能限制 CAS的功能是比较受限的，例如CAS只能保证单个变量（或者说单个内存值）操作的原子性**
- **悲观锁**：悲观锁在操作数据时比较悲观，认为别人会同时修改数据。因此操作数据时直接把数据锁住，直到操作完成后才会释放锁；上锁期间其他人不能修改数据。（**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**）
  - 悲观锁的实现方式是加锁，加锁既可以是对代码块加锁，也可以是对数据加锁
- **乐观锁适用于写比较少的情况下（多读场景）**，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以**一般多写的场景下用悲观锁就比较合适**









### 事务相关问题

#### 介绍mysql的事务

- 事务是由 MySQL 的引擎来实现的，常见的 InnoDB 引擎它是支持事务的

- 事务看起来感觉简单，但是要实现事务必须要遵守 4 个特性，分别如下：

  - **原子性（Atomicity）**：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样

  - **一致性（Consistency）**：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态

  - **隔离性（Isolation）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。

  - **持久性（Durability）**：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。


- InnoDB 引擎通过什么技术来保证事务的这四个特性的呢：

  - 持久性是通过 redo log （重做日志）来保证的；

  - 原子性是通过 undo log（回滚日志） 来保证的；

  - 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的；

  - 一致性则是通过持久性+原子性+隔离性来保证；

#### 介绍隔离级别

- 在同时处理多个事务的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题；SQL 标准提出了四种隔离级别来规避这些现象，隔离级别越高，性能效率就越低，这四个隔离级别如下：

  - **读未提交（*read uncommitted*）**，指一个事务还没提交时，它做的变更就能被其他事务看到；

  - **读提交（*read committed*）**，指一个事务提交之后，它做的变更才能被其他事务看到

  - **可重复读（*repeatable read*）**，指一个事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，**MySQL InnoDB 引擎的默认隔离级别**；

  - **串行化（*serializable* ）**；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；


- **要解决脏读现象，就要升级到「读提交」以上的隔离级别；要解决不可重复读现象，就要升级到「可重复读」的隔离级别**。要解决幻读现象不建议将隔离级别升级到「串行化」，因为这样会导致数据库在并发事务时性能很差。
- InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它通过**next-key lock 锁**（行锁和间隙锁的组合）来锁住记录之间的“间隙”和记录本身，防止其他事务在这个记录之间插入新的记录，这样就避免了幻读现象。

#### 如何解决不可重复读问题（MVCC）

- **可重复读隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View**。可以把 Read View 理解成一个数据快照，就像相机拍照那样，定格某一时刻的风景。
- 这就解决了不可重复读问题，也就是实现了MVCC，通过「事务的 Read View 里的字段，m_ids，min_trx_id ，max_trx_id ，creator_trx_id 」和「记录中的两个隐藏列，trx_id，roll_pointer」的比对，来控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）

#### 提交读和可重复读隔离级别实现上有什么区别

- 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
- 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。





### 分布式相关问题

#### 分布式，高并发有了解吗

- 分布式更多的一个概念，**是为了解决单个物理服务器容量和性能瓶颈问题而采用的优化手段**，从理念上讲，分布式的实现有两种形式：
  - **水平扩展：**当一台机器扛不住流量时，就通过添加机器的方式，将流量平分到所有服务器上，所有机器都可以提供相当的服务；
  - **垂直拆分：**前端有多种查询需求时，一台机器扛不住，可以将不同的需求分发到不同的机器上，比如A机器处理余票查询的请求，B机器处理支付的请求
- 高并发在解决的问题上会集中一些，其反应的是同时有多少量：比如在线直播服务；高并发可以通过分布式技术去解决，将并发流量分到不同的物理服务器上。但除此之外，还可以有很多其他优化手段：比如使用缓存系统，将所有的，静态内容放到CDN等；还可以使用多线程技术将一台服务器的服务能力最大化
- 区别：
  - 分布式是从物理资源的角度去将不同的机器组成一个整体对外服务，技术范围非常广且难度非常大，有了这个基础，高并发、高吞吐等系统很容易构建；
  - 高并发是从业务角度去描述系统的能力，实现高并发的手段可以采用分布式，也可以采用诸如缓存、CDN等，当然也包括多线程；
  - 多线程则聚焦于如何使用编程语言将CPU调度能力最大化

#### 高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？

- 高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换
- 并发不高、任务执行时间长的业务要区分开看：
  - 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务
  - 假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换
  - 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦

#### 了解分布式缓存，mq之类的吗

- 分布式缓存由一个服务端实现管理和控制，有多个客户端节点存储数据，可以进一步提高数据的读取速率，其能够高性能地读取数据、能够动态地扩展缓存节点、能够自动发现和切换故障节点、能够自动均衡数据分区，而且能够为使用者提供图形化的管理界面，部署和维护都十分方便；Redis也是分布式缓存里面的一员

- 随着体量的增加以及业务场景越来越复杂，很多场景单机的技术栈和中间件以及不够用了，而且对系统的友好性也下降了，最后做了很多技术选型的工作，此时便可引入**消息队列中间件**（MQ）；**异步、削峰、解耦**是消息队列的经典场景

  - **异步**：原本因为业务简单因此只需要一个流程就可以解决，随着功能需求增多，流程链路越来越长也就变慢了；如果增加的功能并没有拓扑关系便可以用到异步了；

    **异步用线程，线程池去做不是一样的么？**：

    **每次加一个业务功能需要调用一个接口然后还要重新发布系统**，而且真的全部都写在一起的话，不单单是耦合这一个问题，出问题排查也麻烦，流程里面随便一个地方出问题搞不好会影响到其他的点

  - **解耦**：通过MQ将消息发送给别的系统即可，只需要走完自己的流程，把自己的消息发出去，那后面要接入什么系统直接订阅发送的成功消息，成功了监听就好了
  - **削峰**：**Redis**，**MySQL**各自的承受能力都不一样，直接**全部流量照单全收**很可能直接就打挂了，把请求放到队列里面，然后至于每秒消费多少请求，就看自己的**服务器处理能力**，能处理5000QPS你就消费这么多，可能会比正常的慢一点，但是**不至于打挂服务器**，等流量高峰下去了，服务也就没压力了

#### 使用了消息队列有啥问题么？

- 系统复杂性：接入一个中间件就要考虑去维护他，而且使用的过程中要考虑各种问题，比如消息**重复消费**、**消息丢失**、**消息的顺序消费**等等

- 数据一致性：这个其实是分布式服务本身就存在的一个问题，**不仅仅是消息队列的问题**，用了消息队列这个问题会暴露得比较严重一点；**所有的服务都成功才能算这一次下单是成功的**，那怎么才能保证数据一致性呢？把下单，优惠券，积分。。。都放在一个事务里面一样，要成功一起成功，要失败一起失败。
- 可用性：系统本身没啥问题，现在突然接入一个中间件在那放着，万一挂了怎么办

## 六、Redis

### Redis数据结构

#### redis哪个数据类型用跳表

- Redis 只有在 Zset 对象（有序集合）的底层实现用到了跳表，跳表的优势是能支持平均 O(logN) 复杂度的节点查找；
- Zset 对象能支持范围查询（如 ZRANGEBYSCORE 操作），这是因为它的数据结构设计采用了跳表，而又能以常数复杂度获取元素权重（如 ZSCORE 操作），这是因为它同时采用了哈希表进行索引

#### 跳表查找，插入，删除时间复杂度

- 当相邻两层的节点数量比例是 2:1，查找复杂度可以降低到 O(logN)
- 原始链表需要保持有序，所以会向查找元素一样，找到元素应该插入的位置，因此查找和删除时间复杂度也都为 O(logN)
- **跳表在创建节点的时候，随机生成每个节点的层数**，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。具体的做法是，**跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数**。

#### redis中zset怎么做的

- Zset 类型的底层数据结构是由**压缩列表或跳表**实现的：
- 如果有序集合的元素个数小于 `128` 个，并且每个元素的值小于 `64` 字节时，Redis 会使用**压缩列表**作为 Zset 类型的底层数据结构；每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个节点保存元素的分值。并且压缩列表内的集合元素按分值从小到大的顺序进行排列，小的放置在靠近表头的位置，大的放置在靠近表尾的位置
- 如果有序集合的元素不满足上面的条件，Redis 会使用**跳表**作为 Zset 类型的底层数据结构；使用 zset 结构作为底层实现，一个 zset 结构同时包含一个字典（哈希，实现单点查询）和一个跳表（实现范围查询）；字典的键保存元素的值，字典的值则保存元素的分值；跳跃表节点的 object 属性保存元素的成员，跳跃表节点的 score 属性保存元素的分值



### Redis缓存淘汰相关

#### redis中lru怎么做的

- 传统的 LRU 算法存在两个问题：

  - 需要用链表管理所有的缓存数据，这会带来额外的空间开销；
  - 当有数据被访问时，需要在链表上把该数据移动到头端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。
- Redis 实现的是一种**近似 LRU 算法**，目的是为了更好的节约内存，它的**实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间**
- 当 Redis 进行内存淘汰时，会使用**随机采样的方式来淘汰数据**，它是随机取 5 个值（此值可配置），然后**淘汰最久没有使用的那个**

#### 有哪些缓存淘汰算法

- Redis 内存淘汰策略共有八种，主要分成不进行数据淘汰（不再提供服务，直接返回错误）和进行数据淘汰，后者分为针对设置了过期时间的volatile淘汰和all key淘汰，volatile淘汰有四种策略，分别是随机淘汰、优先淘汰更早过期的键值、lru和lfu；all key则少了ttl淘汰方法其余一样

#### lru和lfu有什么优缺点，应用场景

- Redis采用一种近似**LRU**算法，为了更好的节约内存，它的**实现方式是在 Redis 的对象结构体中添加一个额外的字段，用于记录此数据的最后一次访问时间**。当 Redis 进行内存淘汰时，会使用**随机采样的方式来淘汰数据**，它是随机取 5 个值（此值可配置），然后**淘汰最久没有使用的那个**；但是针对单次访问大量数据的场景下会产生缓存污染问题
- LFU算法就是针对该缺陷提出的，其根据数据访问次数来淘汰数据的，会记录每个数据的访问次数，当一个数据被再次访问时，就会增加该数据的访问次数，这样子就能够解决偶尔访问一次导致的缓存污染问题；LFU算法针对redis中的lru字段进行了修改，其高16bit仍然记录时间戳，低8bit则用为记录访问频次；其访问频次不是简单的+1-1，而是先按照上次访问距离当前的时长，来对 logc 进行衰减；然后，再按照一定概率增加 logc 的值，这样子主要是为了充份利用仅有的低8bit的255数据范围
- **LRU策略更关注数据的时效性，而实际应用的负载具有较好的时间局部性**，因此LRU应用更加广泛；但是再扫描式查询的应用场景中，**LFU可以更好的解决缓存污染问题**；如果业务中有短时高频访问的数据，可以优先使用volatile-LFU策略，并根据这些数据的访问时限设置他们的过期时间

#### redis做缓存有哪些问题，如何解决？

- 为了保证缓存中的数据与数据库中的数据一致性，会给 Redis 里的数据设置过期时间，当缓存数据过期后，用户访问的数据如果不在缓存里，业务系统需要重新生成缓存，因此就会访问数据库，并将数据更新到 Redis 里，这样后续请求都可以直接命中缓存

- **缓存雪崩：**当**大量缓存数据在同一时间过期（失效）**时，如果此时有大量的用户请求，都无法在 Redis 中处理，于是全部请求都直接访问数据库，从而导致数据库的压力骤增，严重的会造成数据库宕机，从而形成一系列连锁反应，造成整个系统崩溃，这就是**缓存雪崩**的问题;可以采用两种方案解决：

  - **将缓存失效时间随机打散：** 我们可以在原有的失效时间基础上增加一个随机值（比如 1 到 10 分钟）这样每个缓存的过期时间都不重复了，也就降低了缓存集体失效的概率

  - **设置缓存不过期：** 我们可以通过后台服务来更新缓存数据，从而避免因为缓存失效造成的缓存雪崩，也可以在一定程度上避免缓存并发问题。


- **缓存击穿：**业务通常会有几个数据会被频繁地访问，比如秒杀活动，这类被频地访问的数据被称为热点数据。如果缓存中的**某个热点数据过期**了，此时大量的请求访问了该热点数据，就无法从缓存中读取，直接访问数据库，数据库很容易就被高并发的请求冲垮，这就是**缓存击穿**的问题。 应对缓存击穿可以采取前面说到两种方案：

  - 互斥锁方案（Redis 中使用 setNX 方法设置一个状态位，表示这是一种锁定状态），保证同一时间只有一个业务线程请求缓存，未能获取互斥锁的请求，要么等待锁释放后重新读取缓存，要么就返回空值或者默认值。

  - 不给热点数据设置过期时间，由后台异步更新缓存，或者在热点数据准备要过期前，提前通知后台线程更新缓存以及重新设置过期时间；


- **缓存穿透：**当用户访问的数据，**既不在缓存中，也不在数据库中**，导致请求在访问缓存时，发现缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据，没办法构建缓存数据，来服务后续的请求。那么当有大量这样的请求到来时，数据库的压力骤增，这就是**缓存穿透**的问题；缓存穿透的发生一般有这两种情况：业务误操作，缓存中的数据和数据库中的数据都被误删除了，所以导致缓存和数据库中都没有数据；黑客恶意攻击，故意大量访问某些读取不存在数据的业务；应对缓存穿透的方案，常见的方案有三种：

  - **非法请求的限制**：在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库。

  - **设置空值或者默认值**：可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库。

  - **使用布隆过滤器快速判断数据是否存在，避免通过查询数据库来判断数据是否存在**：可以在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在，即使发生了缓存穿透，大量请求只会查询 Redis 和布隆过滤器，而不会查询数据库，保证了数据库能正常运行


- **缓存一致性**：「先更新数据库，再删除缓存」的方案可以保证了数据库与缓存的数据一致性（可以使用重试机制以及订阅 MySQL binlog，再操作缓存来保证这两个操作都成功），但是每次更新数据的时候，缓存的数据都会被删除，这样会对缓存的命中率带来影响；如果业务对缓存命中率有很高的要求，可以采用「更新数据库 + 更新缓存」的方案，因为更新缓存并不会出现缓存未命中的情况，但是在两个更新请求并发执行的时候，会出现数据不一致的问题，因为更新数据库和更新缓存这两个操作是独立的，而我们又没有对操作做任何并发控制，那么当两个线程并发更新它们的话，就会因为写入顺序的不同造成数据的不一致，一下两种方法可以解决不一致问题

  - 在更新缓存前先加个**分布式锁**，保证同一时间只运行一个请求更新缓存，就会不会产生并发问题了，当然引入了锁后，对于写入的性能就会带来影响。

  - 在更新完缓存时，给缓存加上较短的**过期时间**，这样即时出现缓存不一致的情况，缓存的数据也会很快过期，对业务还是能接受的。

#### 缓存穿透布隆过滤器怎么起作用的

- 布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成。当我们在写入数据库数据时，在布隆过滤器里做个标记，这样下次查询数据是否在数据库时，只需要查询布隆过滤器，如果查询到数据没有被标记，说明不在数据库中。
- 布隆过滤器会通过 3 个操作完成标记：
  - 第一步，使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值；
  - 第二步，将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位置。
  - 第三步，将每个哈希值在位图数组的对应位置的值设置为 1；
- 布隆过滤器由于是基于哈希函数实现查找的，高效查找的同时**存在哈希冲突的可能性**，比如数据 x 和数据 y 可能都落在第 1、4、6 位置，而事实上，可能数据库中并不存在数据 y，存在误判的情况。所以，**查询布隆过滤器说数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，数据库中一定就不存在这个数据**

#### 如何解决redis和数据库一致性的问题

**由于引入了缓存，那么在数据更新时，不仅要更新数据库，而且要更新缓存，这两个更新操作存在前后的问题**，但是无论是「先更新数据库，再更新缓存」，还是「先更新缓存，再更新数据库」，这两个方案都存在并发问题，**当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象**。

所以应该在更新数据时，**不更新缓存，而是删除缓存中的数据。然后，到读取数据时，发现缓存中没了数据之后，再从数据库中读取数据，更新到缓存中。**也就是 **Cache Aside 策略**（旁路缓存策略），该策略又可分为读策略和写策略

**写策略的步骤：**

- 更新数据库中的数据；
- 删除缓存中的数据。

**读策略的步骤：**

- 如果读取的数据命中了缓存，则直接返回数据；
- 如果读取的数据没有命中缓存，则从数据库中读取数据，然后将数据写入到缓存，并且返回给用户。

对于写策略，**缓存的写入通常要远远快于数据库的写入**，所以在实际中很难出现请求 B 已经更新了数据库并且删除了缓存，请求 A 才更新完缓存的情况，所以，**「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的**；但是每次更新数据的时候，缓存的数据都会被删除，这样会对缓存的命中率带来影响。



## 七、场景题

#### 一个文件，有几千万行，有IP地址、访问时间、url，访问topk出现频率的IP或url的命令是什么样的

- 首先，1000亿条记录全部放到内存肯定不够，那就是分成小文件了，然后整合； 公共的时间段，因为精确到分钟，我们把这每一分钟建成一个小文件，每个小文件肯定会有许多重复的ip，url
-  现在统计每个小的文件中url的访问量和ip的访问次数，方法就是建立索引；那么来了一条url，先看一级索引是不是匹配，匹配再看二级索引，相同的话就是我们要的url目标
-  假定给定了某个时间段，找出url的访问量，那么先找到给定的时间段，对应着刚开始分割的小的文件（每一个分钟）中搜索，通过索引找到相同的url之后，开始统计，直到搜索完所有的给定时间段内的所有的小的文件

#### 设计一个秒杀系统，思路是怎样的

一般在`秒杀时间点`（比如：12点）前几分钟，用户并发量才真正突增，达到秒杀时间点时，并发量会达到顶峰。但由于这类活动是大量用户抢少量商品的场景，必定会出现`狼多肉少`的情况，所以其实绝大部分用户秒杀会失败，只有极少部分用户能够成功。

正常情况下，大部分用户会收到商品已经抢完的提醒，收到该提醒后，他们大概率不会在那个活动页面停留了，如此一来，用户并发量又会急剧下降。所以这个峰值持续的时间其实是非常短的，这样就会出现瞬时高并发的情况

像这种瞬时高并发的场景，传统的系统很难应对，我们需要设计一套全新的系统。可以从以下几个方面入手：

1. 页面静态化

活动页面是用户流量的第一入口，所以是并发量最大的地方。如果这些流量都能直接访问服务端，恐怕服务端会因为承受不住这么大的压力，而直接挂掉；活动页面绝大多数内容是固定的，比如：商品名称、商品描述、图片等。为了减少不必要的服务端请求，通常情况下，会对活动页面做`静态化`处理。用户浏览商品等常规操作，并不会请求到服务端。只有到了秒杀时间点，并且用户主动点了秒杀按钮才允许访问服务端，这样能过滤大部分无效请求。

2. CDN加速

如何才能让用户最快访问到活动页面呢？这就需要使用CDN，它的全称是Content Delivery Network，即[内容分发网络](https://cloud.tencent.com/product/cdn?from=10680)。

使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。

3. 缓存

在秒杀的过程中，系统一般会先查一下库存是否足够，如果足够才允许下单，写数据库。如果不够，则直接返回该商品已经抢完。由于大量用户抢少量商品，只有极少部分用户能够抢成功，所以绝大部分用户在秒杀时，库存其实是不足的，系统会直接返回该商品已经抢完。这是非常典型的：`读多写少` 的场景。如果有数十万的请求过来，同时通过数据库查缓存是否足够，此时数据库可能会挂掉。因为数据库的连接资源非常有限，比如：mysql，无法同时支持这么多的连接。而应该改用缓存，比如：redis。即便用了redis，也需要部署多个节点。

通常情况下，我们需要在redis中保存商品信息，里面包含：商品id、商品名称、规格属性、库存等信息，同时数据库中也要有相关信息，毕竟缓存并不完全可靠。用户在点击秒杀按钮，请求秒杀接口的过程中，需要传入的商品id参数，然后服务端需要校验该商品是否合法。

根据商品id，先从缓存中查询商品，如果商品存在，则参与秒杀。如果不存在，则需要从数据库中查询商品，如果存在，则将商品信息放入缓存，然后参与秒杀。如果商品不存在，则直接提示失败

缓存部分可能会出现缓存击穿、缓存穿透的现象，针对击穿问题最好在项目启动之前，先把缓存进行预热，并且添加分布式锁防止数据库可能扛不住压力，直接挂掉；针对穿透问题则采用布隆过滤器过滤掉不存在的请求

真正的秒杀商品的场景，不是说扣完库存，就完事了，如果用户在一段时间内，还没完成支付，扣减的库存是要加回去的。所以，在这里引出了一个`预扣库存`的概念，扣减库存中除了上面说到的`预扣库存`和`回退库存`之外，还需要特别注意的是库存不足和库存超卖问题；其中主要是要保证原子性

4. mq（消息队列）异步处理

在真实的秒杀场景中，有三个核心流程：秒杀、下单、支付

而这三个核心流程中，真正并发量大的是秒杀功能，下单和支付功能实际并发量很小。所以，我们在设计秒杀系统时，有必要把下单和支付功能从秒杀的主流程中拆分出来，特别是下单功能要做成mq异步处理的。而支付功能，比如支付宝支付，是业务场景本身保证的异步。

5. 限流

有必要识别这些非法请求，做一些限制。目前有两种常用的限流方式：基于nginx限流；基于redis限流；可细分为对同一用户限流、对同一ip限流、对接口限流、加验证码、提高业务门槛

#### 磁盘上有100亿个正整数，找到最大的1000个你应该怎么操作

- **分治法**，**大数据里最常用的MapReduce**。

  - 将100亿个数据分为1000个大分区，每个区1000万个数据

  - 每个大分区再细分成100个小分区。总共就有1000*100=10万个分区

  - 计算每个小分区上最大的1000个数。（可以用堆）

  - 合并每个大分区细分出来的小分区。每个大分区有100个小分区，我们已经找出了每个小分区的前1000个数。将这100个分区的1000*100个数合并，找出每个大分区的前1000个数
  - 合并大分区。我们有1000个大分区，上一步已找出每个大分区的前1000个数。我们将这1000*1000个数合并，找出前1000.这1000个数就是所有数据中最大的1000个数

- **Hash法**。如果这1亿个数里面有很多重复的数，先通过Hash法，把这1亿个数字去重复，这样如果重复率很高的话，会减少很大的内存用量，从而缩小运算空间，然后通过分治法或最小堆法查找最大的1000个数。

#### 3个人玩斗地主，设计一个洗牌发牌算法，效率尽可能高

- 朴素的想法，抽牌：每次随机从牌堆中选一个位置抽牌，如果该位置的牌已被抽走，则继续随机选取位置，直到将所有牌抽完。

  - 每次随机从牌堆中选一个位置抽牌，然后将该位置后面的牌依次向前移动一个位置，下一次从新牌堆（数量-1）中抽取，循环到抽完所有牌为止。
  - 每次随机从牌堆中选一个位置抽牌，然后将最后一张牌移动到被选择的位置，下一次从新牌堆（数量-1）中抽取，循环到抽完所有牌为止。时间复杂度降低到了O(N)，应该能满足要求

- **经典的洗牌算法——交换**

  - 还是每次随机一个位置，然后将该位置上的牌与 i 位置上的牌交换，直到 i 遍历玩所有位置为止。

  ```c++
   void MySwap(int &x, int &y)  
   2 {  
   3     int temp = x;  
   4     x = y;  
   5     y = temp;  
   6 }  
   7   
   8 void Shuffle(int n)  
   9 {  
  10     for(int i=n-1; i>=1; i--)  
  11     {  
  12         MySwap(num[i], num[rand()%(i+1)]);  
  13     }  
  14 }  
  ```

  　该洗牌算法的时间复杂度为O(N)，空间复杂度为O(N)。

#### **如何从2.5亿个整数中找出不重复的整数**

由于无法直接将2.5亿个整数一次性加载到内存处理，所以我们需要采用分治法，将一个大文件分割成若干个小文件，从而能将每个小文件分别加载到内存中进行处理，然后使用HashMap分别统计出每个小文件中每个整数出现的次数，最后遍历HashMap输出value值为1的整数即可

- 首先遍历这个大文件，对文件中遍历到的每个整数digit执行hash(digit)%1000操作，将结果为n的整数存放到第fn个文件中。整个大文件遍历结束之后，我们就可以将2.5亿个整数划分到1000个小文件中。那么相同的整数会存储到同一个文件中，分割后的每个小文件的大小为大文件的1/1000。如果有的小文件仍然无法加载到内存中，则可以采用同样的方式继续进分解，直到每个小文件都可以加载到内存中为止。
- 然后在每个小文件中找出不重复的整数，最简单的方法是通过HashMap来实现，其中key为整数，value为该整数出现的次数。具体做法是：遍历每个小文件中的所有记录，对于遍历到的整数digit，如果digit在map中不存在，那么就执行map.put(digit,1)，将digit出现次数设置为1；如果digit在map中存在，那么就执行map.put(digit,map.get(digit)+1)，将digit出现的次数加1
- 最后针对每个小文件，遍历HashMap输出value为1的所有整数，就可以找出这2.5亿个整数中所有的不重复的数。这里不用再对每个小文件输出的整数进行重复筛重，因为每个整数经过hash函数处理后，相同的整数只会被划分到同一个小文件中，不同的文件中不会出现重复的整数。

对于整数相关的算法的求解，位图法是一种非常实用的算法。假设整数占用4B，即32bit，那么可以表示的整数的个数为2\^32。那么对于本题目来说，我们只需要查找不重复的数，而无需关心具体整数出现的次数，所以可以分别使用2个bit来表示各个数字的状态：00表示这个数字没有出现过；01表示这个数字出现过一次；10表示这个数字出现过多次。那么这2\^32个整数，总共需要的内存为2^32*2b=1GB。因此，当可用内存超过1GB时，可以采用位图法求解该题目。

- 首先需要开辟一个用2Bitmap法标志的2^32个整数的桶数组，并初始化标记位为00，其存储的数据量远远大于2.5亿个整数。
- 然后遍历2.5亿个整数，并查看每个整数在位图中对应的位，如果位值为00，则修改为01，如果位值为01，则修改为10，如果位值为10则保持不变。
- 最后当所有数据都遍历完成之后，可以再遍历一遍位图，把对应位值是01的整数输出，即可统计出2.5亿个整数中所有不重复的数。

#### **如何找出CSDN网站最热门的搜索关键词**

**题目描述：**CSDN网站搜索引擎会通过日志文件把用户每次搜索使用关键词都记录下来，每个查询关键词限定长度为1~255个字节。假设目前有1000万个搜索记录，现要求统计最热门的10个搜索关键词。备注：现有内存不超过1GB。

从题目中给出的信息可知，每个搜索关键词最长为255个字节，1000万个搜索记录需要占用约10000000*255B≈2.55GB内存，因此，我们无法将所有搜索记录全部读入内存中处理。

- 分治法依然是一个非常实用的方法。首先将整个搜索记录文件分割为多个小文件，保证单个小文件中的搜索记录可以全部加载到内存中处理，然后统计出每个小文件中出现次数最多的10个搜索关键词，最后设计一个小顶堆统计出所有文件中出现最多的10个搜索关键词。分治法虽然可行，但不是最好的方法，因为需要2次遍历文件，分割文件的Hash函数被调用1000万次，所以性能不是很好
- 虽然题目中搜索关键词的总数比较多，但是一般关键词的重复度比较高，去重之后搜索关键词不超过300万个，因此可以考虑把所有搜索关键词及出现的次数保存到HashMap中，由于存储次数的整数一般占用4个字节，所以HashMap所需要占用的空间为300万*（255+4）≈800M，因此题目中限定的1GB内存完全够用。
  - 首先遍历所有搜索关键词，如果关键词存在与map中，则value值累加1；如果关键词不在map中，则value值设置为1。
  - 然后遍历map集合，构建一个包含10个元素的小顶堆，如果遍历到的关键词出现的次数大于堆顶关键词出现的次数，则进行替换，并将堆调整为小顶堆。
  - 最后直接取出堆中的10个关键词就是出现次数最多的字符串。
- 当这些关键词有大量相同的前缀时，可以考虑使用前缀树来统计搜索关键词出现的次数，树的结点可以保存关键词出现的次数。
  - 首先遍历所有搜索关键词，针对每个关键词在前缀树中查找，如果能找到，则把结点中保存的关键词次数加1，否则就为这个关键词构建新的结点，构建完成之后把叶子结点中关键词的出现次数设置为1。当遍历完所有关键词之后，就可以知道每个关键词的出现次数了
  - 然后遍历前缀树，就可以找出出现次数最多的关键词。

## 八、智力题

#### 四人过桥

问题：晚上有四个人要过桥，只有一个手电筒，每次过桥都需要手电筒，每次最多可同时过两个人，其中甲过桥要1分钟，乙要2分钟，丙要5分钟，丁要10分钟。求最短的过桥时间。

解答：

1）：甲乙先过，用时2分钟，然后甲把手电筒送回来，总用时3分钟，再丙丁同时过去，总用时变为13分钟，再乙把手电筒送回来，总用时变为15分钟，再甲乙一起过去，四人都已到对岸，总用时17分钟。

2）：甲乙先过，用时2分钟，然后乙把手电筒送回来，总用时4分钟，再丙丁同时过去，总用时变为14分钟，再甲把手电筒送回来，总用时变为15分钟，再甲乙一起过去，四人都已到对岸，总用时17分钟。

#### 三人三鬼过桥

有三个人跟三个鬼要过河,河上没桥只有条小船,然后船一次只能渡一个人和一个鬼,或者两个鬼或者两个人,无论在哪边岸上,只有是人比鬼少的情况下(如两鬼一人,三鬼两人,三鬼一人)人会被鬼吃,然而船又一定需要人或鬼操作才能航行(要有人或鬼划船),问,如何安全的把三人三鬼渡过河对岸?

参考回答

- 先两鬼过去。在一鬼回来。对面有一鬼。这边有三人两鬼。
- 再两鬼过去。在一鬼回来。对面有两鬼。这边有三人一鬼。
- 再两人过去。一人一鬼回来。对面一人一鬼。这边两人两鬼。
- 最后两人过去。一鬼回来。对面三人。这边三鬼。
- 剩下的就三个鬼二个过去一个回来在接另外个就OK了

#### 给定随机函数，生成别的随机数

给定生成1到5的随机数Rand5()，如何得到生成1到7的随机数函数Rand7()？

思路

由大的生成小的容易，比如由Rand7()生成Rand5()，所以我们先构造一个大于7的随机数生成函数；

RandNN= N( RandN()-1 ) + RandN() ;// 生成1到N^2之间的随机数
可以看作是在数轴上撒豆子。N是跨度/步长，是RandN()生成的数的范围长度，

#### 砝码称轻重，找出最轻的

有一个天平，九个砝码，其中一个砝码比另八个要轻一些，问至少要用天平称几次才能将轻的那个找出来？

参考回答：至少2次。第一次，一边3个，哪边轻就在哪边，一样重就是剩余的3个； 第二次，一边1个，哪边轻就是哪个，一样重就是剩余的那个；至少称2次．

#### 毒药毒白鼠，找出哪个瓶子中是毒药

8瓶酒一瓶有毒，用小老鼠测试。每次测试结果8小时后才会得出，而你只有8个小时的时间。最少需要（ ）老鼠测试？ 

用3位2进制代表8瓶酒，如下表所示

瓶序号 二进制 中毒情况

第一瓶 000 全没中毒

第二瓶 001 只有第一个老鼠中毒

第三瓶 010 只有第二个老鼠中毒

第四瓶 011 第一个老鼠、第三个老鼠同时中毒

第五瓶 100 只有第三个老鼠中毒

第六瓶 101 第一个老鼠、第三个老鼠同时中毒

第七瓶 110 第二个老鼠、第三个老鼠同时中毒

第八瓶 111 三个老鼠同时中毒

其中，第一个老鼠喝下最低位为1对应的酒，第二个老鼠喝下中间位为1对应的酒，第三个老鼠喝下最高位为1对应的酒

最后将所有中毒的老鼠，对应的位次进行与操作即可以知道那瓶毒药有毒了

#### 在24小时里面时针分针秒针可以重合几次

24小时中时针走2圈，而分针走24圈，时针和分针重合24-2=22次，而只要时针和分针重合，秒针一定有机会重合，所以总共重合22次

#### 利用烧绳子计算时间

现有若干不均匀的绳子，烧完这根绳子需要一个小时，问如何准确计时15分钟，30分钟，45分钟，75分钟。。。

计算15分钟：对折之后两头烧(要求对折之后绑的够紧，否则看45分钟解法)

计算30分钟：两头烧

计算45分钟：两根，一根两头烧一根一头烧，两头烧完过了30分钟，立即将第二根另一头点燃，到烧完又过15分钟，加起来45分钟

计算75分钟：将30和45分钟的方式加起来就可以了

其余类似

#### 100个奴隶猜帽子颜色

一百个奴隶站成一纵列，每人头上随机带上黑色或白色的帽子，各人不知道自己帽子的颜色，但是能看见自己前面所有人帽子的颜色． 然后从最后一个奴隶开始，每人只能用同一种声调和音量说一个字： ”黑”或”白”， 如果说中了自己帽子的颜色，就存活，说错了就拉出去斩了，说的参考回答所有奴隶都能听见。 是否说对，其他奴隶不知道。 在这之前，所有奴隶可以聚在一起商量策略，问如果奴隶都足够聪明而且反应足够快，100个人最大存活率是多少？

参考回答：这是一道经典推理题

1、最后一个人如果看到奇数顶黑帽子报“黑”否则报“白”，他可能死

2、其他人记住这个值（实际是黑帽奇偶数），在此之后当再听到黑时，黑帽数量减一

3、从倒数第二人开始，就有两个信息：记住的值与看到的值，相同报“白”，不同报“黑”

99人能100%存活，1人50%能活

另外，此题还有变种：每个奴隶只能看见前面一个人帽子颜色又能最多存活多少人？

参考回答：增加限制条件后，上面的方法就失效了，此时只能约定偶数位奴隶说他前一个人的帽子颜色，奇数奴隶获取信息100%存活，偶数奴隶50几率存活

#### 小猴子搬香蕉

一个小猴子边上有100根香蕉，它要走过50米才能到家，每次它最多搬50根香蕉，（多了就被压死了），它每走 1米就要吃掉一根，请问它最多能把多少根香蕉搬到家里？（提示：他可以把香蕉放下往返的走，但是必须保证它每走一米都能有香蕉吃。也可以走到n米时，放下一些香蕉，拿着n根香蕉走回去重新搬50根。）

本题关键点在于：猴子搬箱子的过程其实分为两个阶段，第一阶段：来回搬，当香蕉数目大于50根时，猴子每搬一米需要吃掉三根香蕉。第二阶段：香蕉数《=50，直接搬回去。每走一米吃掉1根。

我们分析第一阶段：假如把100根香蕉分为两箱。一箱50根。

第一步，把A箱搬一米，吃一根。

第二步，往回走一米，吃一根。

第三步，把B箱搬一米，吃一根。

这样，把所有香蕉搬走一米需要吃掉三根香蕉。

这样走到第几米的时候，香蕉数刚好小于50呢？

100-(n\*3)<50 && 100-(n-1*3)>50

走到16米的时候，吃掉48根香蕉，剩52根香蕉。这步很有意思，它可以直接搬50往前走，也可以再来回搬一次，但结果都是一样的。

到17米的时候，猴子还有49根香蕉。这时猴子就轻松啦，直接背着走就行。

第二阶段：

走一米吃一根。

把剩下的50-17=33米走完。还剩49-33=16根香蕉

#### 高楼扔鸡蛋（经典问题）

有2个鸡蛋，从100层楼上往下扔，以此来测试鸡蛋的硬度。比如鸡蛋在第9层没有摔碎，在第10层摔碎了，那么鸡蛋不会摔碎的临界点就是9层。问：如何用最少的尝试次数，测试出鸡蛋不会摔碎的临界点？

- 最笨的测试方法，是什么样的呢？把其中一个鸡蛋，从第1层开始往下扔。如果在第1层没碎，换到第2层扔；如果在第2层没碎，换到第3层扔.......如果第59层没碎，换到第60层扔；如果第60层碎了，说明不会摔碎的临界点是第59层。在最坏情况下，这个方法需要扔100次。

- 采用类似于二分查找的方法，把鸡蛋从一半楼层（50层）往下扔。如果第一枚鸡蛋，在50层碎了，第二枚鸡蛋，就从第1层开始扔，一层一层增长，一直扔到第49层。如果第一枚鸡蛋在50层没碎了，则继续使用二分法，在剩余楼层的一半（75层）往下扔......这个方法在最坏情况下，需要尝试50次

- 如何让第一枚鸡蛋和第二枚鸡蛋的尝试次数，尽可能均衡呢？

  - 很简单，做一个平方根运算，100的平方根是10。因此，我们尝试每10层扔一次，第一次从10层扔，第二次从20层扔，第三次从30层......一直扔到100层。
  - 这样的最好情况是在第10层碎掉，尝试次数为 1 + 9 = 10次。最坏的情况是在第100层碎掉，尝试次数为 10 + 9 = 19次。不过，这里有一个小小的优化点，我们可以从15层开始扔，接下来从25层、35层扔......一直到95层。这样最坏情况是在第95层碎掉，尝试次数为 9 + 9 = 18次

- 最优解法是反向思考的经典：如果最优解法在最坏情况下需要扔X次，那第一次在第几层扔最好呢？

  参考回答是：从X层扔

  假设最优的尝试次数的x次，为什么第一次扔就要选择第x层呢？

  这里的解释会有些烧脑，请小伙伴们坐稳扶好：

  **假设第一次扔在第x+1层：**

  如果第一个鸡蛋碎了，那么第二个鸡蛋只能从第1层开始一层一层扔，一直扔到第x层。

  这样一来，我们总共尝试了x+1次，和假设尝试x次相悖。由此可见，第一次扔的楼层必须小于x+1层。

  **假设第一次扔在第x-1层：**

  如果第一个鸡蛋碎了，那么第二个鸡蛋只能从第1层开始一层一层扔，一直扔到第x-2层。

  这样一来，我们总共尝试了x-2+1 = x-1次，虽然没有超出假设次数，但似乎有些过于保守。

  **假设第一次扔在第x层：**

  如果第一个鸡蛋碎了，那么第二个鸡蛋只能从第1层开始一层一层扔，一直扔到第x-1层。

  这样一来，我们总共尝试了x-1+1 = x次，刚刚好没有超出假设次数。

  因此，要想尽量楼层跨度大一些，又要保证不超过假设的尝试次数x，那么第一次扔鸡蛋的最优选择就是第x层。

  那么算最坏情况，第二次你只剩下x-1次机会，按照上面的说法，你第二次尝试的位置必然是X+（X-1）；

  以此类推我们可得：

  x + (x-1) + (x-2) + ... + 1 = 100

  这个方程式不难理解：

  左边的多项式是各次扔鸡蛋的楼层跨度之和。由于假设尝试x次，所以这个多项式共有x项。

  右边是总的楼层数100。

  下面我们来解这个方程：

  x + (x-1) + (x-2) + ... + 1 = 100 转化为

  (x+1)*x/2 = 100

  最终x向上取整，得到 x = 14

  因此，最优解在最坏情况的尝试次数是14次，第一次扔鸡蛋的楼层也是14层。

  最后，让我们把第一个鸡蛋没碎的情况下，所尝试的楼层数完整列举出来：

  14，27， 39， 50， 60， 69， 77， 84， 90， 95， 99， 100

  举个例子验证下：

  假如鸡蛋不会碎的临界点是65层，那么第一个鸡蛋扔出的楼层是14，27，50，60，69。这时候啪的一声碎了。

  第二个鸡蛋继续，从61层开始，61，62，63，64，65，66，啪的一声碎了。

  因此得到不会碎的临界点65层，总尝试次数是 6 + 6 = 12 < 14 。

  
