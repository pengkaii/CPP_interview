# 小林coding-MySQL篇

# 基础篇

## MySQL架构

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/mysql%E6%9F%A5%E8%AF%A2%E6%B5%81%E7%A8%8B.png)

MySQL的架构分为：**Server层**和**存储引擎层**

- **Server层负责建立连接、分析和执行SQL**。包括连接器、查询缓存、解析器、预处理器、优化器、执行器等
- **存储引擎层负责数据的存储和提取**。支持InnoDB、MyISAM、Memory等引擎，现在默认是InnoDB引擎，索引类型是B+树。

⚠️MySQL实际是个服务器。

## MySQL查询过程

1. **连接器**：与客户端TCP三次握手建立连接、校验用户信息，读取用户权限。
2. **查询缓存**：查询语句是否命中查询缓存，如果命中直接返回。（只要表有更新操作，那么这个表的查询缓存就会被清空，所以MySQL8.0移除了它）
3. **解析器**：词法分析：识别关键字，构建SQL语法树；语法分析：根据语法规则，判断输入的SQL语句是否满足MySQL语法。
4. **执行SQL**：
   - **预处理阶段**：检查表或字段是否存在；将 `select *` 中的 `*` 符号扩展为表上的所有列。
   - **优化阶段**：基于查询成本的考虑， 选择查询成本最小的执行计划；
   - 执行阶段：根据执行计划执行SQL查询语句，从存储引擎读取记录，返回给客户端；

## 什么是关系型数据库

关系型数据库（Relational Database）是一种基于关系模型的数据库。在关系模型中，数据被组织成一个或多个表，每个表由行（称为记录）和列（称为字段）组成。关系型数据库的主要特点是表之间的数据可以通过共享的键（键）相互关联。

## 创建数据库和创建表

`CREATE DATABASE example_db;`

```mysql
USE example_db;
CREATE TABLE example_table (
  id INT AUTO_INCREMENT,
  name VARCHAR(50),
  age INT,
  PRIMARY KEY (id)
);
```

## 数据库三大范式精要总结

**（1）简单归纳：**

第一范式（1NF）：字段不可分； 　　

第二范式（2NF）：有主键，非主键字段依赖主键； 　　

第三范式（3NF）：非主键字段不能相互依赖。

**(2）解释：**

1NF：原子性。 字段不可再分,否则就不是关系数据库; 　　

2NF：唯一性 。一个表只说明一个事物； 　　

3NF：每列都与主键有直接关系，不存在传递依赖。

## Compact行格式

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/row_format/COMPACT.drawio.png)

一条完整的记录由**记录的额外信息**和**记录的真实数据**组成。

**记录的额外信息包括：变长字段长度列表、NULL值列表、记录头信息**：

1. **变长字段长度列表**：变长字段实际存储的数据的长度（大小）不固定的。变长字段长度列表会把真实数据占用的字节数(转成16进制，比如1字节就是0x01)**按照列的顺序逆序排放**。如varchar 是变长的，实际存储的数据的**长度不固定**。因此存储数据时要把数据占用的大小保存起来到**变长字段长度列表**中，根据这个「变长字段长度列表」去读取对应长度的数据。如果都是int类型，那么行格式里就不会有变长字段长度列表。

  - **为什么要逆序？**「记录头信息」中指向下一个记录的指针，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，向左读就是连接头信息，向右读就是真实数据。逆序存放**使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个CPU Cache Line中，这样就可以提高 CPU Cache 的命中率**。（CPU Cache Line是在CPU Cache中一块一块的数据，称为缓存块）

2. **NULL值列表**：表中的某些列可能会存储 NULL 值，如果把这些 NULL 值都放到记录的真实数据中会比较浪费空间，所以 Compact 行格式把这些值为 NULL 的列存储到 NULL值列表中。如果存在允许 NULL 值的列，则**每个列对应一个二进制位（bit）**，二进制位按照列的顺序逆序排列。**1代表为NULL0代表不为NULL**，NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 `0`。比如一行数据id：1，name：a，phone：NULL，age：NULL，那么它的NULL值列表就是110，然后高位补0变成00000110，存在表里就是06。NULL不是固定1字节，如果9个列式NULL就是2字节。
3. **记录头信息**：delete_mask （标识此条数据是否被删除）、next_record（下一条记录的位置）、record_type（表示当前记录的类型，0表示普通记录，1表示B+树非叶子节点记录，2表示最小记录，3表示最大记录）

**记录的真实数据**：3个隐藏字段（row_id、trx_id、roll_pointer）+真实数据

1. **row_id**：如果我们建表的时候指定了主键或者唯一约束(UNIQUE字段)列，那么就没有 row_id 隐藏字段了。如果**既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段**。row_id不是必需的，占用 6 个字节。
2. **trx_id**：**记录事务id，表示这个数据是由哪个事务生成的**。 trx_id是必需的，占用 6 个字节。
3. **roll_pointer**：**这条记录上一个版本的指针**，也就是指向undo日志。roll_pointer 是必需的，占用 7 个字节。

**扩展**：每个变长字段的「变长字段长度」需要用多少字节具体情况分为：

- 条件一：如果变长字段允许存储的最大字节数小于等于 255 字节，就会用 1 字节表示「变长字段长度」；
- 条件二：如果变长字段允许存储的最大字节数大于 255 字节，就会用 2 字节表示「变长字段长度」；

## 行溢出后MySQL 是怎么处理的？

如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。

Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。

Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。

## 数据库引擎InnoDB与MyISAM的区别

MyISAM不支持事务，不支持行级锁，MyISAM崩溃后发生损坏的概率比InnoDB高很多，而且恢复的速度也更慢。MyISAM采用非聚集索引，B+树叶子存储指向数据文件的指针。InnoDB主键索引采用聚集索引，B+树叶子存储数据

MyISAM支持压缩表和空间数据索引

**适用场景**： MyISAM适合： 插入不频繁，查询非常频繁，如果执行大量的SELECT，MyISAM是更好的选择， 没有事务。 InnoDB适合： 可靠性要求比较高，或者要求事务； 表更新和查询都相当的频繁， 大量的INSERT或UPDATE

## 视图和游标

视图是一种虚拟的表，通常是有一个表或者多个表的行或列的子集，具有和物理表相同的功能 

游标是对查询出来的结果集作为一个单元来有效的处理。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。

## 数据库中的主键、超键、候选键、外键

- **超键**：在关系中能唯一标识**元组的属性集**称为关系模式的超键
- **候选键**：不含有**多余属性的超键**称为候选键。也就是在候选键中，若再删除属性，就不是键了！
- **主键**：**用户选作元组标识的一个候选键程序主键**
- **外键**：如果关系模式**R中属性K是其它模式的主键**，那么**k在模式R中称为外键**。

**主键为候选键的子集，候选键为超键的子集，而外键的确定是相对于主键的**。

![](../../%25E6%25A1%258C%25E9%259D%25A2/%25E5%25BE%2585%25E5%2590%2588%25E5%25B9%25B6/%25E5%2588%25AB%25E4%25BA%25BA%25E7%259A%2584%25E9%259D%25A2%25E7%25BB%258F/Typora%25E7%2594%25A8%25E5%2588%25B0%25E5%259B%25BE%25E7%2589%2587/afca574cd23055ec6e0a0922879aafc.png)

1. 超键：于是我们从例子中可以发现 学号是标识学生实体的唯一标识。那么该元组的超键就为学号。除此之外我们还可以把它跟其他属性组合起来，比如：(`学号`，`性别`)，(`学号`，`年龄`)
2. 候选键：根据例子可知，学号是一个可以唯一标识元组的唯一标识，因此学号是一个候选键，实际上，候选键是超键的子集，比如 （学号，年龄）是超键，但是它不是候选键。因为它还有了额外的属性。
3. 主键：简单的说，例子中的元组的候选键为学号，但是我们选定他作为该元组的唯一标识，那么学号就为主键。
4. 外键是相对于主键的，比如在学生记录里，主键为学号，在成绩单表中也有学号字段，因此学号为成绩单表的外键，为学生表的主键。

## MySQL存储引擎

1. **InnoDB**: 这可能是最常用的存储引擎，它提供了许多高级功能，包括事务支持、行级锁定、外键以及 MVCC (多版本并发控制)。InnoDB 引擎还具有一种叫做 "自适应哈希索引" 的功能，在某些工作负载下，这种索引可以提高性能。另外，InnoDB 还具有崩溃恢复功能，以及空间和时间较好的效率。
2. **MyISAM**: 在 InnoDB 之前，MyISAM 是 MySQL 的默认存储引擎。MyISAM 提供了全文搜索索引，这使得它在某些应用（如博客和文章发布系统）中非常有用。然而，MyISAM 不支持事务和行级锁定，它的表锁定机制在并发环境下可能会影响性能。另外，MyISAM 不具备崩溃恢复功能，这使得它在数据的完整性和持久性上可能存在问题。
3. **Memory (Heap)**: 这种存储引擎将数据存储在内存中，这可以提供非常快速的访问速度。然而，由于数据存储在内存中，所以如果服务器关闭或崩溃，数据将会丢失。另外，Memory 存储引擎不支持事务和行级锁定，它也使用表锁定机制。
4. **NDB (Network Database)**: 这是一个分布式存储引擎，用于创建高可用性和高性能的数据库集群。NDB 存储引擎支持事务和行级锁定，它主要用于大型和高并发的应用。
5. **Archive**: 这种存储引擎用于存储和压缩大量的数据，而不支持索引。它主要用于日志和审计等长期存储的场景。
6. **Federated**: 这种存储引擎允许你在一个 MySQL 服务器上访问另一个 MySQL 服务器中的数据。但是，由于性能和安全性的问题，Federated 存储引擎通常不推荐在生产环境中使用。
7. **Blackhole**: 这种存储引擎会接受但不存储数据，对其执行的所有 SELECT 操作均返回空结果。它主要用于复制和日志记录等特殊场景。

## MySQL性能优化

- 为搜索字段创建索引
- 避免使用 Select *，列出需要查询的字段
- 垂直分割分表
- 选择正确的存储引擎
- 避免索引失效等

## SQL注入

通过附加一些SQL命令，绕过密码检查

比如说登录查询是这样`SELECT * FROM users WHERE username = '输入的用户名' AND password = '输入的密码'`

但是攻击者用这样的语句`SELECT * FROM users WHERE username = '输入的用户名' AND password = '' OR '1'='1'`

这个WHERE子句的条件变成了“用户名匹配，并且密码为空，或者1等于1”。由于'1'='1'总是为真，所以整个OR条件为真，查询就会成功返回，绕过了密码验证。

**危害**主要包括：

1. 数据泄露：攻击者可以获取敏感信息，如用户数据、密码等。
2. 数据篡改：攻击者可以修改数据，比如改变价格、用户权限等。
3. 数据删除：攻击者可以删除整个数据库。
4. 服务器控制：在某些情况下，攻击者甚至可以获取到服务器的控制权。

解决方法：

**输入验证**：验证用户输入是否符合预期的格式，并且确保数据是安全的。例如，如果你期待一个电话号码，那么任何非数字的输入都应该被拒绝。

**使用Web应用防火墙**：Web应用防火墙可以帮助识别和阻止SQL注入攻击。

**定期更新和审查应用程序**：定期更新应用程序以及审查代码，是避免SQL注入等攻击的重要措施。

# 索引篇

## 索引概述

索引是一种专门的数据结构，类似于一本书的目录，其主要功能是加速数据的查找速度。默认情况下，MySQL使用InnoDB作为其存储引擎，其底层索引结构采用的是B+树，这保证了其高效的查询性能。

虽然索引能提升查询效率，但并不意味着在所有情况下都应创建索引。首先，索引的创建本身就是一个消耗资源的过程。当数据量较大时，建立索引需要耗费大量的时间。其次，索引本身占用了大量的内存空间。再者，每当添加新的数据时，可能需要重新调整索引，这在后期维护阶段可能会占用大量的时间。

此外，当数据量较小，或者MySQL估算出全表扫描的速度与使用索引查询的速度差别不大时，MySQL可能会选择不使用索引，因为此时建立索引并没有太大的必要。索引的创建和使用应当根据实际情况谨慎考虑。适当的索引可以大大提高查询效率，而不当的索引可能会导致资源的浪费和性能的下降。

## MySQL中有哪些索引？有什么特点？

- **普通索引**：仅加速查询
- **唯一索引**：加速查询 + 列值唯一（可以有null）
- **主键索引**：加速查询 + 列值唯一（不可以有null）+ 表中只有一个
- **组合索引**：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并
- **全文索引**：对文本的内容进行分词，进行搜索
- **索引合并**：使用多个单列索引组合搜索
- **覆盖索引**：select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖
- **聚簇索引**：表数据是和主键一起存储的，主键索引的叶结点存储行数据(包含了主键值)，二级索引的叶结点存储行的主键值。使用的是B+树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录(数据内容)

## 聚簇(主键)索引与二级(辅助)索引

- **聚簇索引（使用主键列建立的B+树，叶子节点存放完整的用户记录）**：

  - 使用记录主键值的大小进行记录和页的排序。页内记录按照主键大小排成一个单向链表；存放用户记录的页之间按照用户记录的主键大小排成双向链表；存放目录项的页在同一层级之间按照主键大小排成双向链表；


  - **叶子节点存储的是完整的用户记录**，包括隐藏列-->row_id（当表中未定义主键以及UNIQUE键时自动生成）、trx_id、roll_pointer。

- **二级索引（使用非主键列建立的B+树，叶子节点存放的是不完整的用户记录）**：

  - **其叶子节点保存的是主键值**，若想找到完整用户记录，需要回表，根据主键值去聚簇索引在查找一遍完整的用户记录。

  - 二级索引的非叶子节点的目录项记录由三部分组成：索引列的值，主键值，页号（主键值来保证B+树同一层内的非叶子节点的目录项记录除页号这个字段以外是唯一的）

**索引覆盖**：当**查询的数据是主键值**时，因为只**在二级索引就能查询到**，不用再去聚簇索引查，这个过程就叫作「索引覆盖」，也就是只需要查一个 B+ 树就能找到数据。

聚集索引和非聚集索引的区别在于， **通过聚集索引可以查到需要查找的数据**， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据。 聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。

## 聚簇索引，非聚簇索引，主键索引，联合索引对比

1. **聚簇索引（Clustered Index）**：在聚簇索引中，表中的行是按照索引键的顺序存储的。这意味着表中的数据行只能有一个顺序。因此，一张表只能有一个聚簇索引，但可以有多个非聚簇索引。当你查询一个范围的数据时，聚簇索引可以非常快，因为这些数据在磁盘上是连续的。
2. **非聚簇索引（Non-Clustered Index）**：非聚簇索引和聚簇索引的主要区别在于非聚簇索引不改变数据行的物理顺序，而是创建一个单独的对象来指向数据。这意味着你可以在同一张表上创建多个非聚簇索引，因为非聚簇索引是对数据的逻辑视图。
3. **主键索引（Primary Key Index）**：主键索引是在主键字段上自动创建的索引。主键索引是唯一的聚簇索引，主键索引不允许有空值，它确保了数据的唯一性和完整性。
4. **联合索引（Composite Index）**：联合索引是基于两个或更多列的索引。在创建联合索引时，列的顺序很重要，因为它影响了查询优化器如何使用索引。可以根据数据的访问模式和查询模式来确定列的顺序。

聚簇索引就像你根据书名字母顺序来排列书籍。每本书在书架上只有一个合适的位置（就像在数据库表中，只能有一个聚簇索引，决定了数据的物理存储顺序）。当你要找一本具体的书时，你知道它在书架的哪个部分，所以可以快速找到。同时，如果你想读一系列的书（例如，所有以 "A" 开头的书），它们都在一起，使得访问更为方便。

非聚簇索引就像你在另一个房间有一个按照书的作者姓名排列的书目列表。这个列表并没有实际的书，只有书名和书在书架上的位置。当你需要根据作者找书时，你可以在书目列表中找到作者，然后根据位置信息去书架上找到实际的书。这个过程就像非聚簇索引使用指向数据行的指针。注意，你可以有多个这样的书目列表，比如按照主题分类、按照出版日期分类等等（同样，一个数据库表可以有多个非聚簇索引）。

**聚簇索引和非聚簇索引最大的区别就是叶子节点存储的不同，聚簇索引叶子节点存储的就是数据，所以找到了索引就找到了数据，而非聚簇索引叶子节点存储的不是数据，所以还需要回表操作才能找到数据，聚簇索引表中的行是按索引键顺序存储的，因为只能有一个顺序，所以聚簇索引一个表只能有一个。**

## 主键索引和唯一索引区别

在关系型数据库中，主键索引（Primary Key Index）和唯一索引（Unique Index）是两种不同的索引类型，它们有以下区别：

1. **主键索引（Primary Key Index）**：
   - 主键是一种用于唯一标识表中每一行数据的列或列组合。
   - 主键索引是对主键列或列组合创建的索引，它确保了主键的唯一性和快速查找。
   - **主键索引要求主键列的值不能为空（即非NULL）。**
   - **每个表只能有一个主键，且主键值不能重复。**
2. **唯一索引（Unique Index）**：
   - 唯一索引是对表中的列或列组合创建的索引，它要求该列或列组合的值在整个索引中是唯一的。
   - **唯一索引可以包含NULL值，但只允许出现一个NULL值。**
   - **表可以有多个唯一索引，每个唯一索引都对应一个唯一性约束。**

基本上，主键索引和唯一索引都提供了唯一性约束，它们的区别在于**主键索引是表的主键的索引，而唯一索引是对非主键列或列组合创建的索引。**此外，主键索引要求主键列的值不能为空，而唯一索引允许包含NULL值（只能有一个NULL值）。

在实践中，主键索引通常用于标识表中的唯一行，并为表提供快速的查找功能。唯一索引可以用于确保其他列或列组合的唯一性。

## 为什么一个表只能有一个聚簇索引

当我们说一个表只能有一个聚簇索引时，我们的意思是**一个表的数据只能按照一种顺序在物理磁盘上存储**。如果一个表尝试有多个聚簇索引，这就意味着我们试图在同一时间以多种方式在物理磁盘上存储同一数据，这在实践中是不可能的。这就是为什么一个表只能有一个聚簇索引的原因。

## MySQL如何实现索引

MySQL通过B+树实现索引，B+树是一个多路平衡查找树，它将用户数据存储在叶子节点，叶子节点之间通过指针相互连接在一起。

## 为什么使用B+树不用B树或者二叉树或者hash索引

首先MySQL数据是存在磁盘的，操作磁盘要进行系统调用，所以希望较少的磁盘IO，**B+树的搜索复杂度是`O(logdN)`(d表示节点允许的最大子节点个数为d个)**，实际中d大于1000所以即使千万级别数据，树的高度也只是3-4层也就是3-4次IO，而**二叉树为 `O(logN)`，IO次数比B+树多**；

**B+树只在叶子节点存储数据**，而B树的非叶子节点也要存储数据，所以**相同磁盘IO次数下，B+树能查询更多节点**；

**B+树叶子节点采用的是双链表连接而B树没有这种构造，适合常见的基于范围的顺序查找**，**hash索引**不适合范围查询**更适合等值查询**。

**B树的每一个节点最多可以包括M个子节点**，**M称为B树的阶**，所以B树就是一个多叉树。假设M = 3，那么就是**一棵3阶的B树，特点就是每个节点最多有2个（M-1个）数据和最多有3个（M个）子节点**，超过这些要求的话，就会分裂节点。

**B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，而B树只能中序遍历所有节点，效率太低。**

B+树的叶子节点之间是用「双向链表」进行连接，这样的好处是既能向右遍历，也能向左遍历。

B+树点节点内容是数据页，数据页里存放了用户的记录以及各种信息，每个数据页默认大小是16KB。

## B+树插入和删除

B+树的插入和删除操作如下：

**插入操作**：

1. 首先，查找应插入的叶子节点的位置。从根节点开始，根据插入值与节点中关键字的比较结果，选择合适的子节点，逐层向下，直到找到应插入的叶子节点。
2. 如果叶子节点未满（每个节点的关键字数量小于预设的最大值），直接将新值插入到叶子节点中的适当位置。
3. 如果叶子节点已满，将新值插入后，将节点分裂为两个节点。选择中间的关键字上移到父节点。如果父节点也已满，则进行同样的分裂操作，直到分裂传播到一个未满的节点或根节点。如果根节点也满，生成新的根节点。

**删除操作**：

1. 首先，找到要删除的关键字在哪个叶子节点。
2. 如果这个叶子节点关键字数量大于最小值，直接删除即可。
3. 如果叶子节点关键字数量等于最小值，删除关键字后，就需要做一些额外的操作来保持B+树的性质：
   - 尝试从相邻节点借一个关键字。如果相邻节点的关键字数量大于最小值，从中借一个关键字，同时要更新父节点中的关键字。
   - 如果无法从相邻节点借关键字，就需要将这个节点与一个相邻节点合并，同时删除父节点中对应的关键字。如果因此导致父节点的关键字数量小于最小值，需要递归地进行借关键字或合并节点的操作。
4. 如果删除的是内部节点的关键字，需要找到叶子节点中的前驱或后继关键字替换删除的关键字，然后删除叶子节点中的前驱或后继关键字。

在进行这些操作时，需要保证B+树的两个重要性质：一是树的所有叶子节点在同一层；二是除根节点外，所有节点的关键字数量都在预设的最小值和最大值之间。

## 联合索引相关

联合索引的**最左匹配原则**，**在遇到范围查询（如 >、<）的时候，就会停止匹配**，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配。

**索引下推**：

在执行 `select * from table where a > 1 and b = 2` 语句的时候，只有a字段用到联合索引，找到满足条件的值之后还要判断b，采用**索引下推优化**，就可以在联合索引里判断不必回主键索引判断，**可以在存储引擎层进行联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，再返还给Server层，减少回表次数**。

**索引区分度**：

建立联合索引时，**要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到**。

区分度就是某个字段 column 不同值的个数「除以」表的总行数：区分度 = distinct(column) / count(*)

**联合索引进行排序**：

这里出一个题目，针对针对下面这条 SQL，你怎么通过索引来提高查询效率呢？

```sql
select * from order where status = 1 order by create_time asc
```

在查询时，如果只用到 status 的索引，但是这条语句还要对 create_time 排序，这时就要用文件排序 filesort，也就是在 SQL 执行计划中，Extra 列会出现 Using filesort。

所以，要**利用索引的有序性，在 status 和 create_time 列建立联合索引**，这样根据 status 筛选后的数据就是按照 create_time 排好序的，避免在文件排序，提高了查询效率。

意思应该是联合索引都是有序的。

## 什么时候需要/不需要建立索引？

- 需要建立索引：

  - 字段有唯一性限制的，比如商品编码；


    - 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。


    - 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就**不需要再去做一次排序**了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。


- 不需要建立索引：

  - `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的

  - 大量重复的数据的字段，不需要创建索引。

  - 表数据太少的时候，不需要创建索引。

  - 经常更新的字段不用创建索引。

## 为什么要用索引

- **通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。**
- **可以大大加快数据的检索速度，这也是创建索引的最主要的原因。**
- **帮助服务器避免排序**和临时表
- **将随机IO变为顺序IO。**
- 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义

## 索引优化

常见的有：**前缀索引优化**，**覆盖索引优化**，**主键索引自增**，**索引设置为NOT NULL**，**防止索引失效**

**前缀索引优化**：

- 使用前缀索引是**为了减小索引字段大小**，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。
- 局限性：order by就无法使用前缀索引；无法把前缀索引用作覆盖索引。

**覆盖索引优化**：

- 假设我们只需要查询商品的名称、价格，有什么方式可以避免回表呢？我们可以建立一个联合索引，即「商品ID、名称、价格」作为一个联合索引。如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。

**主键索引最好是自增的**：

- **如果使用自增主键**，每次**插入一条新记录，都是追加操作，不需要重新移动数据**，因此这种插入数据的方法效率非常高。
- **如果主键非自增**，插入数据时容易出现**页分裂**，页分裂还有**可能会造成大量的内存碎片**，导致索引结构不紧凑，从而影响查询效率。
- 另外，主键字段的长度不要太大，因为**主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小**。

**索引设置为NOT NULL**：

- 索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化
- NULL值没意义，且会占用至少1个字节空间来存储NULL值列表。

## 防止索引失效

首先看一下**常见的索引失效的场景**：

- 当我们**使用左或者左右模糊匹配的时候**，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；
- 当我们**在查询条件中对索引列做了表达式计算、函数、类型转换操作**，这些情况下都会造成索引失效；
  - **MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较**。

- 联合索引要能正确使用**需要遵循最左匹配原则**，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
- 在WHERE子句中，如果**在OR前的条件列是索引列，而在OR后的条件列不是索引列**，那么索引会失效。

## 索引执行计划(了解)

在索引语句前加explain

![](https://cdn.xiaolincoding.com//mysql/other/798ab1331d1d6dff026e262e788f1a28.png)

参数有：

- possible_keys 字段表示可能用到的索引；
- key 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引；
- key_len 表示索引的长度；
- rows 表示扫描的数据行数。
- type 表示数据扫描类型，我们需要重点看这个。

type 字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的**执行效率从低到高的顺序为**：

- All（全表扫描）；
- index（全索引扫描）；
- range（索引范围扫描）；
- ref（非唯一索引扫描）；
- eq_ref（唯一索引扫描）；
- const（结果只有一条的主键或唯一索引扫描）。

extra显示的结果：

- Using filesort ：当查询语句中包含 group by 操作，而且无法利用索引完成排序操作的时候， 这时不得不选择相应的排序算法进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。
- Using temporary：使了用临时表保存中间结果，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免这种问题的出现。
- Using index：所需数据只需在索引即可全部获得，不须要再到表中取数据，也就是使用了覆盖索引，避免了回表操作，效率不错。

## mysql使用like"%xxx"索引一定会失效吗

如果**数据库表中的字段只有主键+二级索引**，那么即使**使用了左模糊匹配**，也不会走全表扫描（type=all），而是**走全扫描二级索引树(type=index)**，也就是发生了**索引覆盖**。类似的，**联合索引**要遵循最左匹配才能走索引，但是如果数据库表中的字段都是索引的话，即使查询过程中，没有遵循最左匹配原则，也是走全扫描二级索引树(type=index)。

## B+树什么时候走索引扫描（用到B+树索引）

1、全值匹配（搜索条件中的列与索引列一致，搜索条件的列的顺序不重要）。

2、匹配左边的列，搜索条件的列可用不包含联合索引的全部列，只要包含从最左边连续的列就可以。

3、匹配列前缀。

```mysql
//table建立了name的索引
SELECT * FROM table WHERE name LIKE 'As%';//索引扫描
SELECT * FROM table WHERE name LIKE '%As';//全表扫描（聚簇索引）
```

4、匹配范围值。建立联合索引后，如果对多个列进行范围查询，只有对索引最左边的列进行范围查询才会走索引查询。

```mysql
//创建表时建立联合索引 KEY idx_name_number (name, number);
SELECT * FROM table WHERE name > 'AS' AND name < 'Bs' AND number > 10;
//因为通过name查询到的记录可能不是按照number列降序排序的，因此name做索引扫描，number用不到B+树索引
```

5、精准匹配联合索引左边的列，右边的列范围查询。走索引扫描

6、使用索引进行排序（ORDER BY）。ORDER BY子句后边的列的顺序必须按照索引列的顺序给出，否则不能使用B+树索引。

不可以使用索引进行排序的情况：DESC混用；WHERE子句中出现非排序使用到的索引列；排序列包含非同一个索引的列；排序列使用了复杂的表达式

```mysql
SELECT * FROM table ORDER BY name, number DESC LIMIT 10;//DESC混用
SELECT * FROM table WHERE country = 'China' ORDER BY name LIMIT 10;//WHERE子句中出现非排序使用到的索引列
SELECT * FROM table ORDER BY name, country LIMIT 10;//排序列包含非同一个索引的列,name与country并不属于同一联合索引
SELECT * FROM table ORDER BY UPPER(name) LIMIT 10;//排序列使用了复杂的表达式，使用函数进行修饰就无法使用索引
```

7、使用索引用于分组，分组列的顺序也需要和索引列的顺序一致，也可以只使用索引列中连续最左边的列进行分组。

```mysql
SELECT name, birthday, phone_number, COUNT(*) FROM table GROUP BY name, birthday, phone_number
```

## 回表相关

使用二级索引查询完整的用户记录会造成回表，其特点是：使用两个B+树索引，一个**二级索引，使用顺序I/O**，一个**聚簇索引（回表）使用随机I/O**；

一般情况下，顺序I/O比随机I/O性能高很多，**需要回表的记录越多 ，使用二级索引的性能就越低**

**全表扫描就是扫描聚簇索引，索引扫描一般采用二级索引+ 回表**，通过优化器据根据回表的记录数，需要回表的记录数越多，就越倾向于使用全表扫描，反之倾向于使用 二级索引 + 回表的方式。

**覆盖索引**：在查询列表里只包含索引列，就是在二级索引就能查询到数据，省去了回表操作带来的性能损耗。

## count()函数相关

**性能：count(*) = count(1) > count(主键字段) > count(字段)；**

count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式。**count计算时会自动省去NULL值**。

该函数作用是**统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个**。

**count(主键字段)如何执行的**：

- server层会循环遍历索引，向InnoDB存储引擎读取一条该字段的记录，如果该参数不为NULL，会将server层维护的count变量+1，直到符合查询的所有记录都被读完，就退出循环，将count变量发给客户端。

**为什么需要循环遍历**：

- 因为mysql支持事务的，同一时刻的多个查询，由于MVCC的原因，应该返回多少行是变化的，所以无法像 MyISAM一样，只维护一个row_count变量。

**count(1)读取到记录返回给server层就行了，它不会读取记录中的字段的值，所以他比count(主键字段)效率高**

**count(*)其实等于count(0)**

count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描（**因为相同数量的二级索引记录可以比聚簇索引记录占用更少的存储空间，所以二级索引树比聚簇索引树小，这样遍历二级索引的 I/O 成本比遍历聚簇索引的 I/O 成本小**）。所以尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。且不要用count(字段)来统计记录个数，采用的是全表扫描，效率最差。

**如何优化count(*)**：

1. 使用 show table status 或者 explain 命令来表进行估算。
2. 建立额外表保存计数值。

# 事务篇

**事务是一种机制，可以保证数据库在操作时数据的一致性和可靠性**

## 事务4个特性ACID

- **原子性（Atomicity）**：**一个事务中的所有操作，要么全部完成，要么全部不完成**，不会结束在中间某个环节，而且**事务在执行过程中发生错误，会被回滚到事务开始前的状态**，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。

- **一致性（Consistency）**：是指**事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态**。

- **隔离性（Isolation）**：**数据库允许多个并发事务同时对其数据进行读写和修改的能力**，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。

- **持久性（Durability）**：**事务处理结束后，对数据的修改就是永久的**，即便系统故障也不会丢失。

InnoDB引擎通过redo log（重做日志）来保证持久性；通过undo log（回滚日志）来保证原子性；通过MVCC（多版本并发控制）或锁机制保证隔离性；通过持久性+原子性+隔离性来保证一致性。

## 并行事务相关问题

在同时处理多个事务的时候，就可能出现**脏读（dirty read）**、**不可重复读（non-repeatable read）**、**幻读（phantom read）**的问题。

- **脏读**：**如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。**
- **不可重复读**：**在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。**
- **幻读**：**在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的数量不一样，就意味着发生了「幻读」现象。**

## 事务隔离级别

- **读未提交（read uncommitted）**，指一个事务**还没提交时**，它做的变更就能被其他事务看到；
- **读提交（read committed）**，指一个事务**提交之后**，它做的变更才能被其他事务看到；
- **可重复读（repeatable read）**，指一个事务执行过程中看到的数据，**一直跟这个事务启动时看到的数据是一致的**，⚠️MySQL InnoDB引擎的**默认隔离级别**；
- **串行化（serializable）**；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行；

读未提交（发生脏读 、不可重复读和幻读）、读已提交（发生不可重复读和幻读）、可重复读（发生幻读）、串行化（没问题）

⚠️读已提交、可重复读这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，**读已提交**在**每一次进行普通SELECT操作前都会生成一个ReadView**，而**可重复读**只在**第一次进行普通SELECT操作前生成一个ReadView**，之后的查询操作都**重复使用**这个ReadView就好了。

⚠️**MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了）**

- 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
  - 实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，**后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的**，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。
- 针对**当前读**（select ... for update 等语句，每次执行的时候都是读取最新的数据），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。

开启事务的命令：**begin/start transaction ，执行该命令后，并不代表事务真的启动了，只有执行增删查改操作后才正在启动**。

​					  		**start transaction with consistent snapshot，执行后，马上开启事务。**

## MVCC相关

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/readview%E7%BB%93%E6%9E%84.drawio.png)

ReadView有四个重要的字段：

- creator_trx_id ：指的是**创建该Read View的事务的事务id**。
- m_ids ：指的是在创建Read View时，当前数据库中活跃事务（**启动但未提交的事务**）的**事务id列表**。
- min_trx_id ：指的是在创建Read View时，当前数据库中「活跃事务」中**事务id最小的事务**。
- max_trx_id ：创建Read View时**当前数据库中应该分配给下一个新事务的id值**，也就是**全局事务中最大的事务id值 + 1**；

MVCC通过事务的Read View里的字段（m_id_x、min_trx_idx、max_trx_id）和记录的两个隐藏列（trx_id与roll_pointer）比较来判断记录对事务是否可见：

- trx_id < min_trx_id，表示这个版本的记录是在创建 Read View **前**已经提交的事务生成的，所以该版本的记录对当前事务**可见**。
- trx_id > max_trx_id，说明该版本的记录是在创建 Read View **后**才启动的事务生成的，所以该版本的记录对当前事务**不可见**。
- min_trx_id <= trx_id <= max_trx_id时，要判断trx_id是否在m_ids活跃事务列表内，如果在说明生成该版本记录的事务未提交，该版本记录对当前事务**不可见**。若不在说明已提交，**可见**。

## 发生幻读的特殊情况

在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/%E5%B9%BB%E8%AF%BB%E5%8F%91%E7%94%9F.drawio.png)

# 锁篇

MySQL里根据加锁的范围可以分为**全局锁**、**表级锁**和**行级锁**。

## 全局锁

加锁 `flush tables with read lock`，加锁后整个数据可以处于**只读**状态，解锁`unlock tables`。

**应用场景**：
全局锁主要应用于做**全库逻辑备份**，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

**缺点**：

加上全局锁，整个数据库都是只读状态。那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。

**解决方法**：

如果数据库的引擎支持的事务支持**可重复读的隔离级别**，那么在备份数据库之前先开启事务，会先创建 Read View，然后**整个事务执行期间都在用这个 Read View**，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。即使其他事务更新了表的数据，也不会影响备份数据库的Read View，这就是隔离性，这样备份期间备份的数据一直是在开启事务时的数据。

## 表级锁

表级锁有**表锁**，**元数据锁（MDL）**，**意向锁**，**AUTO-INC锁**。

**表锁**:

表级别的共享锁，也就是读锁:`lock tables t_student read`，表级别的独占锁，也就是写锁:`lock tables t_stuent write`

**意向锁**：

当执行插入、更新、删除操作，需要先对表加上「意向独占锁」，然后对该记录加独占锁。

意向共享锁和意向独占锁**是表级锁**，不会和**行级的**共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables ... read）和独占表锁（lock tables ... write）发生冲突。也就是**读读共享**，**读写互斥**，**写写互斥**。**意向锁的目的是为了快速判断表里是否有记录被加锁**。如果没有意向锁，我想加独占表锁就得遍历看看表里有没有记录被加了独占锁。

**ATUO-INC锁**：

ATUO-INC锁是一种特殊的表锁机制，实现主键自增的，**在执行插入语句时**就在表级别加一个AUTO-INC锁，然后为每条待插入记录的AUTO_INCREMENT修饰的列分配递增的值，**在该语句执行结束后**，再把AUTO-INC锁释放掉。这样其他事务如果向该表插入语句就会被阻塞，从而保证了被AUTO_INCREMENT修饰的字段是自增的。但是，大量数据插入时会影响性能，所以提供了一种**轻量级的锁**实现自增。**区别就是，轻量级的锁在赋值完递增的值之后就会被释放而不是等插入语句结束后才释放**。

## 行级锁

普通的select语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的。

对读取的记录加**共享锁**(**S锁**)：`select ... lock in share mode`；对读取的记录加**独占锁**(**X锁**)：`select ... for update`。这两条语句**必须在同一个事务中**，**因为当事务提交了**，**锁就会被释放**，所以在使用这两条语句的时候，要加上begin、start transaction或者set autocommit = 0。

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E9%94%81/x%E9%94%81%E5%92%8Cs%E9%94%81.png)

行级锁有

- **Record Lock**，记录锁，也就是仅仅**把一条记录锁上**；
- **Gap Lock**，间隙锁，**锁定一个范围**，但是**不包含记录本身**；
- **Next-Key Lock**：Record Lock + Gap Lock的组合，**锁定一个范围**，**并且锁定记录本身**。

**间隙锁之间是兼容的**，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为**间隙锁的目的是防止插入幻影记录而提出的**。

**插入意向锁**：

插入意向锁名字虽然有意向锁，但是它并**不是意向锁，它是一种特殊的间隙锁，属于行级别锁**。

一个事务在插入一条记录的时候，**需要判断插入位置是否已被其他事务加了间隙锁**（next-key lock 也包含间隙锁）。**如果有**的话，插入操作就会发生**阻塞**，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个**插入意向锁**，表明有事务想在某个区间插入新记录，但是现在处于等待状态。

## MySQL如何加行级锁

加锁都是**select...for update**语句；

加锁的**对象是索引**，加锁的**基本单位是next-key lock**，它是由记录锁和间隙锁组合而成的，**next-key lock是前开后闭区间**，而**间隙锁是前开后开区间**。

⚠️**在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock就会退化成记录锁或间隙锁**。

⚠️**使用唯一索引(主键索引)等值查询**：

- 当**记录存在**时，对该记录的next-key lock退化成**X型记录锁**。其它事务不能对该记录进行更新与删改，因为更新或删改会给记录加X型记录锁，而 X 锁和 X 锁之间是互斥关系。

  - 为什么此时加记录锁就可以避免幻读？**唯一索引具有唯一性，其他事物插入相同的索引会冲突**，就比如主键ID=1，我再插入ID=1的就会有冲突；加了记录锁，**其他事物不能删除该记录**；因此避免幻读。

- 当**记录不存在**时，对该记录的next-key lock退化成**间隙锁**。

  - 为什么唯一索引等值查询记录不存在时，在索引树找到第一条大于该查询记录的记录后，要将该记录的索引中的next-key lock会退化成「间隙锁」？因为**在唯一索引等值查询并且查询记录不存在的场景下，仅靠间隙锁就能避免幻读的问题。**
  - 比如我ID是1,5,6，我查2，那么在1-5就产生了间隙锁，**范围右边界是第一个大于他的值，也就是LOCK_DATA，左边界是右边界上一个值。**

  - 不需要在第一条大于该记录的记录加记录锁，因为本次查询的不是该记录，只要保证前后两次查询记录的结果集一样就能避免幻读，该记录即使被删除了，也没有什么影响。


  - 为什么不可以针对不存在的记录加记录锁？**锁是加在索引上的，而这个场景下查询的记录是不存在的，自然就没办法锁住这条不存在的记录**。因此退化成间隙锁。

⚠️**使用唯一索引范围查询**：

⚠️当唯一索引**进行范围查询时**，会**对每一个扫描到的索引加next-key锁**，然后如果遇到下面这些情况，会退化成记录锁或者间隙锁：

- 针对**大于等于的范围查询**，由于存在等值查询条件，当等值查询的条件在表中时，该记录的索引的**next-key锁退化为记录锁**；对大于，管在不在都**不会退化**。

- 针对**小于或小于等于的范围查询**，需要**看条件值的记录是否存在于表中**：

  - 条件值的记录**不在表中**，不管小于或等于，**扫描到终止范围查询的记录时**，该记录索引的next-key锁会**退化为间隙锁**

  - 条件记录**在表中**，如果是**小于**条件，**扫描到终止范围查询记录的索引的next-key锁会退化为间隙锁**；如果是**小于等于**，该记录索引的next-key**不会退化**。

⚠️**非唯一索引（二级索引）等值查询**：

用非唯一索引进行等值查询的时候，**因为存在两个索引，一个是主键索引，一个是非唯一索引（二级索引），所以在加锁时，同时会对这两个索引都加锁，但是对主键索引加锁的时候，只有满足查询条件的记录才会对它们的主键索引加锁**。

针对非唯一索引等值查询时，查询的记录存不存在，加锁的规则也会不同：

- 当查询的记录**「存在」**时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是**非唯一索引等值查询的过程是一个扫描的过程**，**直到扫描到第一个不符合条件的二级索引记录就停止扫描**，然后在扫描的过程中，**对扫描到的二级索引记录加的是next-key锁**，而**对于第一个不符合条件的二级索引记录**，该二级索引的**next-key锁会退化成间隙锁**。**同时，在符合查询条件的记录的主键索引上加记录锁**。
- 当查询的记录**「不存在」**时，**扫描到第一条不符合条件的二级索引记录**，该二级索引的**next-key锁会退化成间隙锁**。因为**不存在满足查询条件的记录，所以不会对主键索引加锁**。

⚠️**非唯一索引范围查询**：

- 非唯一索引和主键索引的范围查询的加锁也有所不同，不同之处在于**非唯一索引范围查询，索引的next-key lock不会有退化为间隙锁和记录锁的情况**，也就是非唯一索引进行范围查询时，对二级索引记录加锁**都是加 next-key 锁**。

⚠️**没有加索引的查询**：

- 如果锁定读查询语句（update与delete语句查询条件不加索引），没有使用索引列作为查询条件，或者查询语句没有走索引查询，**导致扫描是全表扫描**。那么，**每一条记录的索引上都会加next-key锁**，这样就相当于锁住的全表，这时如果其他事务对该表进行增、删、改操作的时候，都会被阻塞。

- 在线上在**执行update、delete、select ... for update等具有加锁性质的语句**，**一定要检查语句是否走了索引**，如果是全表扫描的话，会对每一个索引加next-key锁，相当于把整个表锁住了，这是挺严重的问题。


## for update没用索引锁全表

⚠️**如果对没有建立索引的列查询，那就会走全表扫描**，比如我的ID是主键索引，再没有其他的了，如果你select...where age...for update就会走全表扫描，就会锁住整个表，每个记录都被加上了next key Lock。

**如何避免？**

将`sql_safe_updates` 参数设置为 1，开启安全更新模式。可以预防update操作时where条件没有带上索引列。如果在where条件中加了索引，优化器还是走全表扫描，用`force index([index_name])` 可以告诉优化器使用哪个索引。

update必须满足下面条件之一才能执行成功

- 使用where且条件中必须有索引列
- 使用limit
- 同时使用where和limit，此时where条件中可以没有索引列

delete语句必须同时使用where和limit，此时where条件中可以没有索引列，才可执行成功。

## 二级索引的间隙锁如何插入

当有一个事务持有二级索引的间隙锁 (22, 39) 时，什么情况下，可以让其他事务的插入 age = 22 或者 age = 39 记录的语句成功？又是什么情况下，插入 age = 22 或者 age = 39 记录时的语句会被阻塞？

**插入语句在插入一条记录之前，需要先定位到该记录在 B+树 的位置，如果插入的位置的下一条记录的索引上有间隙锁，才会发生阻塞**。二级索引的B+树先根据索引列排序，索引列相同再通过主键进行排序。

当有一个事务持有二级索引的间隙锁 (22, 39) 时，插入 age = 22 或者 age = 39 记录的语句是否可以执行成功，**关键还要考虑插入记录的主键值**，**因为「二级索引值（age列）+主键值（id列）」才可以确定插入的位置**，确定了插入位置后，就要**看插入的位置的下一条记录是否有间隙锁**，如果**有间隙锁，就会发生阻塞**，如果**没有间隙锁，则可以插入成功**。

## 死锁

死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。

在**数据库层面**，通过打破循环等待条件来解除死锁：

- **设置事务等待锁的超时时间。**当一个事务的等待时间超过该值后，就对这个事务进行**回滚**，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 **`innodb_lock_wait_timeout`** 是用来设置超时时间的，默认值时 50 秒。
- **开启主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 **`innodb_deadlock_detect`** 设置为 on，表示开启这个逻辑，默认就开启。

从业务的角度避免死锁：对插入的字段设置为唯一索引列，利用它的唯一性

**例子**：

1. 事务A锁定了资源X。
2. 事务B锁定了资源Y。
3. 事务A尝试锁定资源Y，但不能，因为它被事务B锁定。
4. 同时，事务B尝试锁定资源X，但也不能，因为它被事务A锁定。

**为避免死锁，可以采用一些策略：**

1. 按照一致的顺序获取锁。例如，如果所有事务都首先锁定资源X，然后锁定资源Y，那么上面描述的死锁情况就不会发生。
2. 使用超时机制。如果事务尝试获取锁，但在指定时间内无法获得，则该事务放弃并重试。
3. 避免长时间持有锁。当不再需要某个资源时，尽快释放它。

# 日志篇

## undo log(回滚日志)

- 会**在事务没有提交之前**，会先记录更新前的数据到undo log日志文件里面
- **实现事务回滚**，**保证事务的原子性**。事务处理出现错误会用户执行了ROLLBACK语句，MySQL利用undo log中的历史数据将数据恢复到事务开始前的状态。
- **实现多版本并发控制（MVCC）**。MVCC通过ReadView(m_ids、min_trx_id、max_trx_id、creator_trx_id)+undo log(roll pointer和trx_id)实现的。

## redo log(重做日志)

redo log是物理日志，**记录了某个数据页做了什么修改**。用在事务提交后发生了崩溃。

- **实现事务的持久性，让MySQL有crash-safe的能力**。事务提交时，只要先将redo log持久化到磁盘就行，不需要等待将缓存在Buffer Pool里的脏页数据持久化到磁盘。系统崩溃时，虽然脏页数据没有持久化，但是redo log已经刷盘（持久化），MySQL重启后，看根据redo log内容将数据恢复到最新状态。
- 将MySQL的写操作**从磁盘的随机写变成了顺序写**，提升语句的执行能力。**写入redo log的方式使用了追加操作**。因为MySQL的写操作并不是立刻更新到磁盘，而是先记录在日志上，然后在合适时间再更新到磁盘。

redo log记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；undo log记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；

## redo log什么时候刷新到磁盘

产生的redo log**不会直接写入磁盘**，因为这样会产生大量的IO操作。redo log有自己的缓存--**redo log buffer**，每产生一条redo log，会将其写入redo log buffer中，后续刷盘。

主要有下面几个时机：

- MySQL正常关闭时；
- 当redo log buffer中记录大于其内存空间的一半时；
- InnoDB的后台线程每隔1秒将redo log持久化到磁盘；
- 每次事务提交时都将缓存在redo log buffer里的redo log直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制）。
  - 0：事务提交时不会主动触发写入磁盘操作；
  - 1：每次事务提交时，都将redo log buffer中的redo log直接持久化到磁盘，保证重启后数据不丢失；
  - 2：将redo log buffer中的redo log写到redo log文件（属于page cache），但不意味着写入磁盘

**当innodb_flush_log_at_trx_commit为0或2时，什么时候将redo log刷盘**：

InnoDB 的后台线程每隔 1 秒：

- 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 `write()` 写到操作系统的 Page Cache，然后调用 `fsync()` 持久化到磁盘。**所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失**;
- 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。**所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失**。

**innodb_flush_log_at_trx_commit三个参数的应用场景**：

- 数据安全性：1>2>0;
- 数据写入性能：0>2>1;

因此在对**数据安全要求较高**时，将其设置为**1**；可以**容许数据库崩溃时丢失1s数据**的场景中，将innodb_fulsh_log_at_trx_commit设置为**0**；安全性与写入性能**折中**参数为2，**只要操作系统不宕机，即使数据库崩溃了，数据也不会丢失**。

**为什么事务提交时暂时不将修改页面刷盘，用redo log来刷盘**：

1、redo log占用空间非常小。2、redo log日志是顺序写入磁盘，是顺序IO。

只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。只需要把修改了哪些东西记录一下就好。---redo log

## binlog(归档日志)

binlog文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如SELECT和SHOW 操作。

**redo log和binlog区别**：

- 适用对象不同。**binlog**是MySQL的**Server层实现的日志**，所有存储引擎都可以使用；**redo log**是**InnoDB存储引擎实现的日志**。
- 文件格式不同。binlog有3种格式类型，redo log是物理日志。
- 写入方式不同。**binlog是追加写**，写满一个文件就创建一个新文件继续写，**不会覆盖以前的日志**，保存的是全量的日志；**redo log是循环写**，日志空间大小固定，**全部写满就从头开始写**，保存未被刷盘的脏页日志。
- 用途不同。binlog用于**备份恢复和主从复制**；redo log用于**掉电等故障恢复**。

**主从复制**：

MySQL的主从复制依赖于binlog ，也就是**记录MySQL上的所有变化并以二进制形式保存在磁盘上**。**复制的过程就是将binlog中的数据从主库传输到从库上**。这个过程一般是**异步**的，也就是主库上执行事务操作的线程不会等待复制binlog的线程同步完成。

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/how_update/%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6%E8%BF%87%E7%A8%8B.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

**主从复制过程**：

- MySQL主库收到客户端提交事务的请求后，会**先写入binlog**，**再提交事务**，**更新存储引擎中的数据**，事务提交完成后，返回给客户端“操作成功”的响应。
- 从库会创建一个专门的I/O线程，连接主库的log dump线程，来接收主库的binlog日志，再把binlog信息写入relay log的中继日志里，再返回给主库“复制成功”的响应。
- 从库会创建一个用于回放binlog的线程，去读relay log中继日志，然后回放binlog更新存储引擎中的数据，最终实现主从的数据一致性。

在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。

**从库是不是越多越好**：

当然不是，因为从库数量增加，从库连接上来的 I/O 线程也比较多，**主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽**。

**binlog刷盘时机**：

事务**执行过程中**，**会把binlog日志写到binlog cache**（Server层的cache），事务**提交时**，**会把binlog cache里的完整事务写到binlog文件**（位于page cache）中，**并清空binlog cache**。注意binlog文件位于page cache，并没有刷盘。

MySQL提供一个 **sync_binlog** 参数来控制数据库的 binlog 刷到磁盘上的频率：

- sync_binlog=0时（默认），每次提交事务只将binlog cache里的完整事务写入binlog文件，不主动触发刷盘，后续交由操作系统决定何时将数据持久化到磁盘；**此时性能最好，但风险最大，一旦主机发生异常重启，还没有持久化到磁盘的数据就会丢失**。
- sync_binlog=1时，每次提交事务都会将binlog cache写入binlog文件，然后马上刷盘。**是最安全但是性能损耗最大的设置。即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。**
- sync_binlog=N(N>1)时，每次提交事务都会将binlog cache的完整事务写入binlog文件，积累N个事务后刷盘。

## 执行一条update语句期间发生了什么

1、客户端通过连接器建立连接，连接器判断用户身份；

2、update语句不需要经过查询缓存，因为一旦有更新语句，会把整个表的查询缓存清空。

3、解析器通过词法分析识别关键字，构建语法树，语法分析，判断输入语句是否符合MySQL语法；

4、预处理器判断表与字段是否存在；

5、优化器确定最优的执行计划；

6、执行器执行具体计划，找到该行，然后更新；具体更新操作如下

- 执行器负责具体执行，调用存储引擎的接口，通过主键索引获取id=1的一行记录；
  - 如果该行记录所在页在buffer pool中，直接返回给执行器更新；
  - 如果不再buffer pool中，将数据页从磁盘读到buffer pool，返回记录给执行器；
- 执行器得到主键索引的记录后，会查看更新前后的记录是否一致；
  - 一样就不进行后续更新流程；
  - 不一样就把更新前、后记录都发送给InnoDB层，让InnoDB真正执行更新的操作。
- 开启事务，InnoDB层更新事务前，先记录相应的undo log，把被更新的旧值记下来，undo log会写入buffer pool中的Undo页，在内存修改该Undo页面后，需要记录对应的redo log。
- InnoDB层开始更新记录，先更新内存，同时标记为脏页，将记录写入redo log，会保存到**redo log buffer**中，更新完成。为减少磁盘IO，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。即WAL技术，MySQL的写操作不时立即写到磁盘，先写redo log日志，再在合适时机将修改的行数据写到磁盘上。
- 至此，一条记录更新完成了；
- 完成后，开始记录该语句对应的binlog，将记录binlog保存到**binlog cache**，事务提交时将其写入位于page Cache的binlog文件中，sync_binlog=1时，事务提交时，写入binlog文件后刷盘。
- 事务提交，两阶段提交（**避免出现两份日志之间的逻辑不一致的问题，是分布式一致性协议，保证多个逻辑要么全部成功，要么全部失败**）。在持久化（刷盘） redo log和binlog这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。这是因为 **redo log影响主库的数据**，**binlog影响从库的数据**，所以redo log和binlog必须保持一致才能保证主从数据一致。事务提交的两阶段就是**将redo log的写入拆成了两个步骤：prepare和commit，中间再穿插写入binlog**：（**对于“双1”配置**）
  - **prepare阶段**：将内部XA事务的ID写入redo log，同时将redo log对应的事务状态设置为prepare，将redo log刷盘（innodb_flush_log_at_trx_commit = 1 的作用）。
  - **commit阶段**：将内部XA事务ID写入binlog，然后将binlog刷盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将redo log设置为commit，该状态不需要刷盘，只需写到文件系统的page cache中，因为只要binlog写磁盘成功，就算redo log的状态还是prepare也没有关系，一样会被认为事务已经执行成功；

## 在两阶段提交的不同时刻，异常重启会出现什么现象

在 MySQL 重启后会按**顺序扫描 redo log 文件**，碰到**处于 prepare 状态的 redo log**，就**拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID**：

- **prepare阶段之后，commit阶段之前异常重启：binlog中没有当前内部XA事务的ID，说明redo log刷盘了，binlog没有刷盘，执行事务回滚**。
- **在commit阶段binlog刷盘成功之后，将redo log状态改为commit之前异常重启：binlog中有内部XA事务的ID，说明redo log和binlog都刷盘了，提交事务**。

因此，**对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID**，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。**保证主库和备库的数据一致性**。

⚠️**事务没提交的时候，redo log 也是可能被持久化到磁盘的**。此时如果mysql崩溃了，还没有提交事务的redo log刷盘了，mysql重启后会触发回滚操作，因为事务未提交时，binlog还没有刷盘。**所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘**。

## 两阶段提交的问题

- **磁盘IO次数高**。对于“双1”配置，每次事务提交都会进行两次fsync刷盘，一次redo log，另一次binlog刷盘。
- **锁竞争激烈**。两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「**多事务**」的情况下，却**不能保证**两者的**提交顺序一致**，因此，在两阶段提交的流程基础上，还**需要加一个锁**来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。

## 为什么锁竞争激烈

早期的 MySQL 版本中，通过使用 **prepare_commit_mutex 锁**来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。**完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳**。

**MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**。引入组提交机制后，prepare阶段不变，只针对commit阶段，将commit阶段拆分为三个过程：

- **flush阶段**：多个事务按进入的顺序将 binlog 从binlog cache 写入一个binlog文件（位于page cache不刷盘）；
- **sync阶段**：对binlog文件做fsync刷盘操作（多个事务的 binlog 合并一次刷盘）；
- **commit阶段**：调用引擎的提交事务接口，将 redo log 状态设置为 commit。

上面的**每个阶段都有一个队列，每个阶段有锁进行保护**，因此保证了事务写入的顺序。对每个阶段引入了队列后，**锁就只针对每个队列进行保护**，不再锁住提交事务的整个过程，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。

## 两阶段提交优化

MySQL 5.6 没有 redo log 组提交，每个事务各自执行prepare阶段，将redo log刷盘；MySQL 5.7 有 redo log 组提交，在 prepare 阶段**不再让事务各自执行 redo log 刷盘操作**，而是**推迟到组提交的 flush 阶段**，也就是说 **prepare 阶段融合在了 flush 阶段**。

这个优化是将 redo log 的刷盘延迟到了 flush 阶段之中，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。**针对的是“双 1” 配置**（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）：

- **flush阶段**（**用于支撑 redo log 的组提交**）：首先时prepare阶段，多个事务按顺序将redo log buffer中的redo log调用write写入一个redo log文件，再调用fsync一次性刷盘；完成prepare阶段后，将该组事务的binlog（储存在binlog cache）依次调用write写入一个binlog文件，此时不刷盘。
- **sync阶段**（**用于支持 binlog 的组提交**）：通过两个参数，binlog_group_commit_sync_delay= N（等待N毫秒后再调用fsync），binlog_group_commit_sync_delay= N（该组事务数达到N个，则不管有没有等待N毫秒，执教调用fsync刷盘），**组合更多事务的 binlog，然后再一起刷盘**。（如果在这一步完成后数据库崩溃，由于 binlog 中已经有了事务记录，MySQL会在重启后通过 redo log 刷盘的数据继续进行事务的提交。）
- **commit阶段**：调用引擎的提交事务接口，将 redo log 状态设置为 commit。作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率。

# Buffer Pool

Buffer Pool主要是为了减少磁盘IO操作，当MySQL从磁盘中读取数据时，它将这些数据页缓存在Buffer Pool中。如果后续的查询请求需要相同的数据页，MySQL可以直接从Buffer Pool中获取数据，而不需要再次访问磁盘。

## Buffer Pool缓存什么

![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/bufferpool%E5%86%85%E5%AE%B9.drawio.png?image_process=watermark,text_5YWs5LyX5Y-377ya5bCP5p6XY29kaW5n,type_ZnpsdHpoaw,x_10,y_10,g_se,size_20,color_0000CD,t_70,fill_0)

**InnoDB会为Buffer Pool申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool中的页就叫做缓存页**，为了更好的管理这些在Buffer Pool中的缓存页，InnoDB为每一个缓存页都创建了一个**控制块**，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。控制块和缓存页中间灰色是**碎片空间**，每个控制块和缓存页都是一一对应的，在填充足够多的控制块和缓存页的组合后， Buffer Pool 剩余的空间可能产生不够填充一组控制块和缓存页，这部分空间不能被使用，也被称为碎片。



![](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/innodb/%E7%BC%93%E5%AD%98%E9%A1%B5.drawio.png)

## 为什么需要Buffer pool

- 减少磁盘I/O的次数；
- 读取数据时，若命中则直接读取返回，未命中则去磁盘中读取，并缓存；
- 修改数据时，如果命中缓存，将其标记为脏页，后续选择合适的实际将脏页刷到磁盘。（与文件系统中内核的page cache的原理很相似）

## Buffer pool如何管理空闲页与脏页

为了更好管理Buffer Pool中的缓存页，InnoDB为每一个缓存页创建一个控制块，控制块信息包括**缓存页的表空间**、**页号**、**缓存页地址**、**链表节点**等等。控制块指向每一个缓存页。

**Free链表**（空闲链表）：将空闲缓存页的控制块作为链表的节点。不可避免会产生碎片空间。Free链表上除了有控制块之外，还有一个**头节点**，该节点**包含链表头控制块的地址和尾控制块的地址以及当前链表中节点的数量等信息**。

**Flush链表**（脏页链表）：和Free链表类似，链表的节点是脏页的控制块。

⚠️**通过LRU链表管理脏页+干净页**

## 如何提高Buffer Pool的缓存命中率

**针对预读的页面可能不进行后续访问情况的优化**： 将LRU 链表（里面既有脏页也有干净页，将最近且经常查询的数据缓存在其中）划分成 young 和 old 区域，当磁盘上的某个页面在初次加载到Buffer Pool中的某个缓存页时，该缓存页对应的控制块会被放到old区域的头部。如果后续对其不进行访问，则逐渐从old区域逐出，不会影响 young 区域中被使用比较频繁的缓存页。

**针对Buffer Pool污染**，全表扫描时，**短时间内访问大量使用频率非常低的页面情况的优化**：**记录处于old区域的缓存页进行第一次访问时的访问时间**，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old区域移动到young区域的头部，否则将它移动到young区域的头部。该间隔有系统变量`innodb_old_blocks_time` 控制。

## 脏页什么时候刷盘？

InnoDB的更新操作采用的是WAL(Write Ahead Log)技术，即先写redo log日志，再刷盘，通过redo log日志让MySQL拥有了崩溃恢复能力。

刷盘时机：

- redo log日志满了，会主动触发脏页刷新到磁盘。
- Buffer Pool空间不足时，需要将一部分数据页淘汰掉，如果是脏页，则需要现将脏页刷盘。
- MySQL认为空闲时，后台线程会定期将适量的脏页刷盘。
- MySQL正常关闭时，会把所有脏页刷盘。

# MySQL语句(CSDN)

**四个表如下图**：

| ![](../Typora用到图片/image-20230605203102794.png) | ![image-20230605203135651](../Typora用到图片/image-20230605203135651.png) | ![image-20230605203224940](../Typora用到图片/image-20230605203224940.png) |
| -------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |

## 第1道题(GROUP BY&HAVING)

**查询平均成绩大于60分的学生的学号和平均成绩**：

AVG求平均值

```sql
SELECT s_id, AVG(s_score) avg_score 
FROM Score
GROUP BY s_id
HAVING avg_score > 60;
```

<img src="../../%25E6%25A1%258C%25E9%259D%25A2/%25E5%25BE%2585%25E5%2590%2588%25E5%25B9%25B6/%25E5%2588%25AB%25E4%25BA%25BA%25E7%259A%2584%25E9%259D%25A2%25E7%25BB%258F/Typora%25E7%2594%25A8%25E5%2588%25B0%25E5%259B%25BE%25E7%2589%2587/image-20230605203349904.png" alt="image-20230605203349904" />

**注意事项**：

- GROUP BY语句用于分组，**根据指定的列对结果集进行分组并应用聚合函数计算每个组的汇总信息**。它允许你按照不同的维度对数据进行汇总、分析和筛选。
- 用了GROUP BY尽量配合使用HAVING，不能用WHERE因为avg_score是表中不存在的列

## 第2、3道题(连接相关)

**查询课程编号为“01”的课程比“02”的课程成绩高的所有学生的学号和成绩**：

用到连接的语法，就是from ... join ... on...这个语法就是连接，from一个表，join一个表，然后on是条件

```mysql
SELECT sc1.s_id, sc1.s_score s_score1, sc2.s_score s_score2  # 注意有两个成绩
FROM Score sc1
JOIN Score sc2 
ON sc1.s_id = sc2.s_id 
AND sc1.c_id = '01'  #  因为是INNER JOIN 下面的条件可以不写在WHERE中
AND sc2.c_id = '02'
AND sc1.s_score > sc2.s_score;
```

⚠️在MySQL中没有显示指定连接类型时就是**默认用的内连接**，`JOIN ... ON`语句，连接的类型（如`INNER JOIN`、`LEFT JOIN`、`RIGHT JOIN`等）

**区别**：

- 内连接返回**两个表中符合连接条件的行**，**只返回两个表中都存在行**。如果某行在一个表中存在但在另一个表中不存在匹配，那么它将被排除在结果集之外。
- **左连接**返回**左表中的所有行以及符合连接条件的右表中的匹配行**。如果右表中没有匹配行，则用NULL值**填充右侧列**。
- **右连接**返回**右表中的所有行以及符合连接条件的左表中的匹配行**。如果左表中没有匹配行，则用NULL值**填充左侧列**。

**所有成绩小于60分的学生信息**：

可以用连接先查出小于60分的学号，作为一个表，然后在连接这个表和信息

max求最大值

```mysql
SELECT st.s_id, st.s_name, st.s_birth, st.s_sex
FROM Student st
JOIN (
		SELECT s_id, MAX(s_score) min_score  # 可以对上边界来进行限制，来满足【所有】这个条件
		FROM Score
		GROUP BY s_id
		HAVING min_score < 60) t 
ON st.s_id = t.s_id
```

⚠️**聚合函数通常搭配GROUP BY分组和HAVING筛选**

# MySQL语句(nowcoder)

## 降序和限制数量

[查找最晚入职员工的所有信息](https://www.nowcoder.com/practice/218ae58dfdcd4af195fff264e062138f?tpId=82&tqId=29753&rp=1&ru=/exam/company&qru=/exam/company&sourceUrl=%2Fexam%2Fcompany&difficulty=undefined&judgeStatus=undefined&tags=&title=)

需要注意的是，**MySQL默认升序(从小到大)排列**，所以要降序排列且限制输出数量可以用`select * from employees order by hire_date desc limit 1;`

**desc是降序关键字，limit限制数量**

## 指定顺序中第几个(倒数第三)

[查找入职员工时间排名倒数第三的员工所有信息](https://www.nowcoder.com/practice/ec1ca44c62c14ceb990c3c40def1ec6c?tpId=82&tqId=29754&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

这一题可能有多个入职最晚的，所以要先找到入职最晚的时间，然后用wher查询。

**DISTINCT**关键字是去除重复的，返回唯一的值；

**offset常常配合limit使用，**`limit 10 offset 10;`**代表从第11行取10个，也可以写作**`limit 10, 10;`**，第一个10是偏移量，第二个10是取多少**

```mysql
select * from employees 
where hire_data = (
	select distinct hire_date 
    from employess 
    order by hire_date desc 
    limit 1 offset 2
);
```

## 连接相关

[查找当前薪水详情以及部门编号](https://www.nowcoder.com/practice/c63c5b54d86e4c6d880e4834bfd70c3b?tpId=82&tqId=29755&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

直接内连接搞定

```mysql
select sa.emp_no, sa.salary, sa.from_date, sa.to_date, de.dept_no
from salaries sa
join dept_manager de
on sa.emp_no = de.emp_no
order by sa.emp_no;
```

[查找所有已经分配部门的员工的last_name和first](https://www.nowcoder.com/practice/6d35b1cd593545ab985a68cd86f28671?tpId=82&tqId=29756&rp=1&ru=/exam/company&qru=/exam/company&sourceUrl=%2Fexam%2Fcompany&difficulty=undefined&judgeStatus=undefined&tags=&title=)

直接内连接搞定

```mysql
select em.last_name, em.first_name, de.dept_no
from employees as em
join dept_emp as de
on em.emp_no = de.emp_no;
```

[查找所有员工的last_name和first_name以及对应部门的dept_no](https://www.nowcoder.com/practice/dbfafafb2ee2482aa390645abd4463bf?tpId=82&tqId=29757&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

直接左连接搞定，左连接填充右表的列，右连接填充左表的列

```mysql
select em.last_name, em.first_name, de.dept_no
from employees as em
left join dept_emp as de
on em.emp_no = de.emp_no;
```

## 聚合函数( count() )

[查找薪水记录超过15条的员工号emp_no以及其对应的记录次数](https://www.nowcoder.com/practice/6d4a4cff1d58495182f536c548fee1ae?tpId=82&tqId=29759&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

直接聚合函数，count+分组和HAVING就行

```mysql
select emp_no, count(emp_no) as t
from salaries
group by emp_no
having t > 15;
```

## 去重+倒序

[找出所有员工当前薪水salary情况](https://www.nowcoder.com/practice/ae51e6d057c94f6d891735a48d1c2397?tpId=82&tqId=29760&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

```mysql
select distinct salary
from salaries
order by salary desc;
```

**DISTINCT**去重的，**DESC**逆序的

## 在一个表不在另一个表

最好用where ... not in ...

```mysql
select emp_no
from employees
where emp_no not in (select emp_no from dept_manager);
```

## 不能同时在两个表

直接连接之后，用where ... != ...语句把在两个表都有的去掉就行了

```mysql
select emp.emp_no, ma.emp_no as manager
from dept_emp as emp
join dept_manager as ma
on emp.dept_no = ma.dept_no
where emp.emp_no != ma.emp_no;
```

## 多层连接

[获取每个部门中当前员工薪水最高的相关信息](https://www.nowcoder.com/practice/4a052e3e1df5435880d4353eb18a91c6?tpId=82&tqId=29764&rp=1&ru=/exam/company&qru=/exam/company&sourceUrl=%2Fexam%2Fcompany&difficulty=undefined&judgeStatus=undefined&tags=&title=)

要先小连接，产生两个临时表，再连接这两个临时表：先找出每个部门的最高薪水，再把部门，员工号，薪水连接起来，再从这两个表得出答案

```mysql
select t1.dept_no, t2.emp_no, t1.salary as maxsalary
from 
(
  select d.dept_no, max(s.salary) as salary
  from dept_emp as d
  join salaries as s
  on d.emp_no = s.emp_no
  group by d.dept_no
) t1
join
(
  select d.emp_no, d.dept_no, s.salary
  from dept_emp as d
  join salaries as s
  on d.emp_no = s.emp_no
) t2
on t1.salary = t2.salary and t1.dept_no = t2.dept_no
order by t1.dept_no;
```

## 奇偶性+不等于

[查找employees表emp_no与last_name的员工信息](https://www.nowcoder.com/practice/a32669eb1d1740e785f105fa22741d5c?tpId=82&tqId=29767&rp=1&ru=/exam/company&qru=/exam/company&sourceUrl=%2Fexam%2Fcompany&difficulty=undefined&judgeStatus=undefined&tags=&title=)

```mysql
SELECT *
FROM employees
WHERE emp_no % 2 = 1
AND last_name != 'Mary'
ORDER BY hire_date DESC;
```

## 分组求平均值

[统计出当前各个title类型对应的员工当前薪水对应的平均工资](https://www.nowcoder.com/practice/c8652e9e5a354b879e2a244200f1eaae?tpId=82&tqId=29768&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

```mysql
select t.title, avg(s.salary) as avg
from titles as t
join salaries as s
on t.emp_no = s.emp_no
group by t.title
order by avg;
```

## 倒序+去重

[获取当前薪水第二多的员工的emp_no以及其对应的薪水](https://www.nowcoder.com/practice/8d2c290cc4e24403b98ca82ce45d04db?tpId=82&tqId=29769&rp=1&ru=/exam/company&qru=/exam/company&sourceUrl=%2Fexam%2Fcompany&difficulty=undefined&judgeStatus=undefined&tags=&title=)

可能有多个重复的第二工资高的，所以先找到工资第二高的，再用where

**记住MySQL排序是默认升序，也就是从上到下是从小到大，如果想从上到下是从大到小要用desc**

```mysql
select emp_no, salary
from salaries
where salary =(
    select distinct salary
    from salaries
    order by salary desc
    limit 1 offset 1
)
order by emp_no;
```

## 获取的第二名的信息

[获取当前薪水第二多的员工的emp_no以及其对应的薪水](https://www.nowcoder.com/practice/c1472daba75d4635b7f8540b837cc719?tpId=82&tqId=29770&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

不能用order by，所以这一题用两个max，第一个max找到最大的，然后再从小于最大的里面再找max就是第二大的

```mysql
select em.emp_no, sa.salary, em.last_name, em.first_name
from employees as em
join salaries as sa
on em.emp_no = sa.emp_no
where sa.salary = (
    select max(s2.salary)
    from salaries s2
    where s2.salary < (select max(salary) from salaries)
);
```

## 多层连接

[查找所有员工的last_name和first_name以及对应的dept_name](https://www.nowcoder.com/practice/5a7975fabe1146329cee4f670c27ad55?tpId=82&tqId=29771&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

直接多层连接，三表连接就行了

```mysql
select e.last_name,e.first_name,d.dept_name
from employees e 
left join dept_emp de 
on e.emp_no=de.emp_no
left join departments d 
on de.dept_no=d.dept_no
```

## 多个连接

涨幅就用现在工资减以前工资，现在在职的就是`s.to_date = '9999-01-01'`，入职的时候就是`e.hire_date = s.from_date`

```mysql
select b.emp_no, (b.salary - a.salary) as growth
from(
    select e.emp_no, s.salary
    from employees as e
    join salaries as s
    on e.emp_no = s.emp_no and e.hire_date = s.from_date
) as a
join(
    select e.emp_no, s.salary
    from employees as e
    join salaries as s
    on e.emp_no = s.emp_no and s.to_date = '9999-01-01'
) as b
on a.emp_no = b.emp_no
order by growth;
```

## 连接以后再分组

[统计各个部门的工资记录数](https://www.nowcoder.com/practice/6a62b6c0a7324350a6d9959fa7c21db3?tpId=82&tqId=29774&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

连接以后再分组

```mysql
SELECT d.dept_no, d.dept_name, count(s.salary) AS SUM
FROM departments d JOIN dept_emp de ON d.dept_no = de.dept_no
JOIN salaries s ON de.emp_no = s.emp_no
GROUP BY d.dept_no
ORDER BY d.dept_no;
```

## 窗口函数(排序函数)

[对所有员工的薪水按照salary降序进行1-N的排名](https://www.nowcoder.com/practice/b9068bfe5df74276bd015b9729eec4bf?tpId=82&tqId=29775&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3DSQL%E7%AF%87%26topicId%3D82&difficulty=undefined&judgeStatus=undefined&tags=&title=)

下面介绍三种用于进行排序的专用窗口函数：

1、RANK()

  在计算排序时，若存在相同位次，会跳过之后的位次。

  例如，有3条排在第1位时，排序为：1，1，1，4······

2、DENSE_RANK()

  这就是题目中所用到的函数，在计算排序时，若存在相同位次，不会跳过之后的位次。

  例如，有3条排在第1位时，排序为：1，1，1，2······

3、ROW_NUMBER()

  这个函数赋予唯一的连续位次。

  例如，有3条排在第1位时，排序为：1，2，3，4······

窗口函数用法：

窗口函数 OVER ([PARTITION BY <列清单> ] ORDER BY <排序用列清单>）

*其中[ ]中的内容可以忽略

```mysql
select emp_no, salary,
dense_rank() over (order by salary desc) as s_rank
from salaries;
```

因为主键是emp_no，主键自增所以不用管了

## 选出不在某个表的其余记录

[获取所有非manager员工当前的薪水情况](https://www.nowcoder.com/practice/8fe212a6c71b42de9c15c56ce354bebe?tpId=82&tqId=29776&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3D%E7%AE%97%E6%B3%95%E7%AF%87%26topicId%3D295&difficulty=undefined&judgeStatus=undefined&tags=&title=)

直接where搞定

```mysql
SELECT
    b.dept_no,
    a.emp_no,
    d.salary 
FROM
    employees a,
    dept_emp b,
    dept_manager c,
    salaries d 
WHERE
    a.emp_no = b.emp_no 
    AND b.dept_no = c.dept_no 
    AND b.emp_no != c.emp_no 
    AND a.emp_no = d.emp_no
```

## 一表多用+从多个表选记录

[获取员工其当前的薪水比其manager当前薪水还高的相关信息](https://www.nowcoder.com/practice/f858d74a030e48da8e0f69e21be63bef?tpId=82&tqId=29777&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3D%E7%AE%97%E6%B3%95%E7%AF%87%26topicId%3D295&difficulty=undefined&judgeStatus=undefined&tags=&title=)

```mysql
select de.emp_no,
       dm.emp_no as manager_no,
       s1.salary as emp_salary,
       s2.salary as manager_salary
from dept_emp de,dept_manager dm,salaries s1,salaries s2
where de.dept_no=dm.dept_no
and de.emp_no=s1.emp_no
and dm.emp_no=s2.emp_no
and s1.salary>s2.salary
```

## 从多个表选+多个分组

[汇总各个部门当前员工的title类型的分配数目](https://www.nowcoder.com/practice/4bcb6a7d3e39423291d2f7bdbbff87f8?tpId=82&tqId=29778&rp=1&ru=/exam/oj&qru=/exam/oj&sourceUrl=%2Fexam%2Foj%3Fpage%3D1%26tab%3D%E7%AE%97%E6%B3%95%E7%AF%87%26topicId%3D295&difficulty=undefined&judgeStatus=undefined&tags=&title=)

```mysql
select d.dept_no,
       d.dept_name,
       t.title,
       count(t.title)as count
from departments d,dept_emp de,titles t
where de.emp_no=t.emp_no
and de.dept_no=d.dept_no
group by d.dept_no,t.title
order by d.dept_no;
```





# =======================================

## Mysql 基础====

### SQL、Mysql、关系型数据库介绍下

**MySQL 是一种关系型数据库，主要用于持久化存储我们的系统中的一些数据比如用户信息。**

**SQL** 是一种结构化查询语言(Structured Query Language)，专门用来与数据库打交道，目的是提供一种从数据库中读写数据的简单有效的方法。

几乎所有的主流关系数据库都支持 SQL ，适用性非常强。并且，一些非关系型数据库也兼容 SQL 或者使用的是类似于 SQL 的查询语言。

**关系型数据库**（RDB，Relational Database）就是一种建立在关系模型的基础上的数据库。关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。

关系型数据库中，我们的数据都被存放在了各种表中（比如用户表），表中的每一行就存放着一条数据（比如一个用户的信息）。

### 关系型和非关系型数据库的区别你了解多少？

- 关系型数据库的优点
  - 容易理解。因为它采用了关系模型来组织数据。
  - 可以保持数据的一致性。
  - 数据更新的开销比较小。
  - 支持复杂查询（带where子句的查询）
- 非关系型数据库的优点
  - 不需要经过SQL层的解析，读写效率高。
  - 基于键值对，数据的扩展性很好。
  - 可以支持多种类型数据的存储，如图片，文档等等。

### 什么是非关系型数据库？

非关系型数据库也叫NOSQL，采用键值对的形式进行存储。

它的读写性能很高，易于扩展，可分为内存性数据库以及文档型数据库，比如 Redis，Mongodb，HBase等等。

适合使用非关系型数据库的场景：

- 日志系统
- 地理位置存储
- 数据量巨大
- 高可用

### SQL 与 MySQL 有什么区别

SQL 和 MySQL 是 DBMS 中最令人困惑的两个术语，二者之间存在本质上的区别。

- SQL 是一种 结构化查询语言，用于在数据库上执行各种操作，但 MySQL 是一个 关系数据库管理系统（RDBMS），使用 SQL 执行所有数据库操作。
- SQL 用于访问，更新和操作数据库中的数据，用户使用时需要学习该语言，然后编写查询，而 MySQL 是一个软件，会为用户提供一个界面，只需单击一些按钮即可用于执行各种数据库操作。
- 由于 MySQL 是一个软件，所以它会定期获得各种更新，但在 SQL 中，命令总是相同的。

### 请说下你对 MySQL 架构的了解？

- 先看下 MySQL 的基本架构图：

从下图你可以很清晰的看到客户端的一条 SQL 语句在 MySQL 内部是如何执行的。

![img](https://oss.javaguide.cn/javaguide/13526879-3037b144ed09eb88.png)

大体来说，MySQL 可以分为 Server 层和存储引擎两部分。

MySQL 主要由下面几部分构成：

- **连接器：** 身份认证和权限相关(登录 MySQL 的时候)。
- **查询缓存：** 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。
- **分析器：** 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。
- **优化器：** 按照 MySQL 认为最优的方案去执行。
- **执行器：** 执行语句，然后从存储引擎返回数据。 执行语句之前会先判断是否有权限，如果没有权限的话，就会报错。
- **插件式存储引擎**：主要负责数据的存储和读取，采用的是插件式架构，支持 InnoDB、MyISAM、Memory 等多种存储引擎

==================================

Server 层包括：连接器、查询缓存、分析器、优化器、执行器等，涵盖了 MySQL 的大多数核心服务功能，以及所有的内置函数（如：日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如：存储过程、触发器、视图等等。

存储引擎层负责：数据的存储和提取。其架构是插件式的，支持 InnoDB、MyISAM 等多个存储引擎。从 MySQL5.5.5 版本开始默认的是InnoDB，但是在建表时可以通过 engine = MyISAM 来指定存储引擎。不同存储引擎的表数据存取方式不同，支持的功能也不同。

从上图中可以看出，不同的存储引擎共用一个 Server 层，也就是从连接器到执行器的部分。

### 一条 SQL 语句在数据库框架中的执行流程？

1. 应用程序把查询 SQL 语句发送给服务器端执行；
2. 查询缓存，如果查询缓存是打开的，服务器在接收到查询请求后，并不会直接去数据库查询，而是在数据库的查询缓存中找是否有相对应的查询数据，如果存在，则直接返回给客户端。只有缓存不存在时，才会进行下面的操作；
3. 查询优化处理，生成执行计划。这个阶段主要包括解析 SQL、预处理、优化 SQL 执行计划；
4. MySQL 根据相应的执行计划完成整个查询；
5. 将查询结果返回给客户端。

=========================

> 执行一条 SQL 查询语句，期间发生了什么？
>
> - 连接器：建立连接，管理连接、校验用户身份；
> - 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；
> - 解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；
> - 执行 SQL：执行 SQL 共有三个阶段：
>   - 预处理阶段：检查表或字段是否存在；将 `select *` 中的 `*` 符号扩展为表上的所有列。
>   - 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；
>   - 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；

- MySQL 的架构共分为两层：**Server 层和存储引擎层**，

  - **Server 层负责建立连接、分析和执行 SQL**。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。
  - **存储引擎层负责数据的存储和提取**。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。

- 第一步：连接器（短连接、长连接）

  - **连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的；完成 TCP 连接的建立后，连接器就要开始验证用户名和密码，如果用户名或密码不对，就收到一个"Access denied for user"的错误，然后客户端程序结束执行；如果用户密码都没有问题，连接器就会获取该用户的权限，然后保存起来，后续该用户在此连接里的任何操作，都会基于连接开始时读到的权限进行权限逻辑的判断**
  - 执行 `show processlist` 命令进行查看MySQL 服务被多少个客户端连接；定义了空闲连接的最大空闲时长，由 `wait_timeout` 参数控制的，默认值是 8 小时（28880秒），如果空闲连接超过了这个时间，连接器就会自动将它断开；MySQL 服务支持的最大连接数由 max_connections 参数控制
  - **短连接、长连接：**MySQL 的连接也跟 HTTP 一样，有短连接和长连接的概念，使用长连接的好处就是可以减少建立连接和断开连接的过程，所以一般是推荐使用长连接；但是使用长连接后可能会占用内存增多，因为 MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。如果长连接累计很多，将导致 MySQL 服务占用内存太大，有可能会被系统强制杀掉，这样会发生 MySQL 服务异常重启的现象，有两种解决方式：**定期断开长连接**；**客户端主动重置连接**

- 第二步：查询缓存；以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果；如果查询的语句命中查询缓存，那么就会直接返回 value 给客户端。**对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空**，MySQL 8.0 版本直接将查询缓存删掉了

- 第三步：解析 SQL；解析器会做词法分析（构建出 SQL 语法树）和**语法分析**，输入的 SQL 语句语法不对，就会在解析器这个阶段报错；但是表不存在或者字段不存在，并不是在解析器里做的，而是在执行SQL的预处理阶段做的

- 第四步：执行 SQL；每条`SELECT` 查询语句流程主要可以分为下面这三个阶段：prepare 阶段，预处理阶段；optimize 阶段，优化阶段；execute 阶段，执行阶段

  - 预处理阶段检查 SQL 查询语句中的表或者字段是否存在；将 `select *` 中的 `*` 符号，扩展为表上的所有列
  - **优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引
  - 确定了执行方案，接下来 MySQL 就真正开始执行语句了，这个工作是由「执行器」完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的

### SQL查到缓存命中，直接返回客户端，那数据发生更改是怎样？命中缓存出来的是什么？

1、SQL 语句查询的结果，直接跳过解析环节。

2、如果数据发生了更改，缓存的结果集将会失效，数据库会重新执行查询操作并更新缓存。因此，查询缓存只适用于对数据不频繁进行修改的查询，对于频繁修改数据的查询不适合使用查询缓存。查询缓存的限制和性能问题，MySQL 8.0 版本直接将查询缓存删掉了，转查询优化。

3、直接从缓存中返回结果给客户端，而无需执行实际的查询操作。这种情况查询的结果是**之前执行该查询语句时缓存的结果，不会访问数据库中的实际数据**。

4、**补充：数据更改：**如果在查询语句执行之后，数据库中相关的数据发生了更改（例如插入、更新或删除数据），那么缓存中的结果将会失效。当下一次执行相同的查询语句时，数据库系统会重新执行查询，而不会再返回缓存的结果。因此，查询缓存只能用于查询不频繁修改的数据。

5、**补充：查询优化有哪些？**

1. 索引优化：使用合适的索引可以加速数据的查找和过滤，从而提高查询性能。数据库系统会根据查询条件选择合适的索引，避免全表扫描，减少IO操作。
2. 查询重写：数据库系统可能会对查询进行重写，将复杂的查询转换为等价的简单查询，以减少计算复杂度。
3. 查询计划优化：数据库系统会生成多个可能的查询计划，并通过成本估算选择最优的执行计划，使得查询操作的执行代价最小。
4. 缓存优化：数据库可能会使用查询缓存，将查询结果缓存起来，减少重复查询的开销。
5. 表设计优化：合理的表设计可以减少关联查询的复杂性，提高查询效率。
6. 分区优化：对于大型数据表，可以采用分区技术将数据划分为多个子集，减少查询的范围，提高查询性能。
7. 并行查询优化：数据库系统可以使用并行查询技术，将查询操作分成多个子任务并行执行，提高查询效率。



在执行查询语句时，会将该查询语句以及**查询的参数**作为**键（key）**进行哈希，并检查这个哈希值是否在缓存中存在。如果存在，则表示该查询语句**曾经执行过，并且缓存了结果**。这个缓存的结果通常被称为**查询缓存结果集**。查询缓存的结果集是根据查询语句的内容和参数来确定的，而不是根据查询的表数据。

当下次有**相同的查询语句被执行**，并且查询参数与之前缓存的查询语句完全一致时，数据库系统会**直接从缓存中获取之前存储的结果集**，并**返回给客户端**。这样可以避免实际的查询操作，提高查询的性能。

注意：**查询缓存的结果集**是根据**查询语句的内容和参数**来确定的，而不是根据**查询的表数据**。所以，如果数据发生了更改，缓存的结果集将会失效，数据库会重新执行查询操作并更新缓存。因此，查询缓存只适用于对数据不频繁进行修改的查询，对于频繁修改数据的查询不适合使用查询缓存。

以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果；如果查询的语句命中查询缓存，那么就会直接返回 value 给客户端。**对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空**

### MySQL 一行记录是怎么存储的？

 MySQL 的数据都是保存在磁盘的，那具体是保存在哪个文件呢？MySQL 存储的行为是由存储引擎实现的，MySQL 支持多种存储引擎，不同的存储引擎保存的文件自然也不同。每创建一个 database（数据库） 都会在 /var/lib/mysql/ 目录里面创建一个以 database 为名的目录，然后保存表结构和表数据的文件都会存放在这个目录里。**一张数据库表的数据是保存在「 表名字.ibd 」的文件里的，这个文件也称为独占表空间文件**。t_order 的**表结构**会保存在这个文件。表空间由段（segment）、区（extent）、页（page）、行（row）组成，InnoDB 的数据是按「页」为单位来读写的，默认每个页的大小为 16KB，InnoDB 存储引擎是用 B+ 树来组织数据。

表空间是由各个段（segment）组成的，段是由多个区（extent）组成的。段一般分为数据段、索引段和回滚段等。

- 索引段：存放 B + 树的非叶子节点的区的集合；
- 数据段：存放 B + 树的叶子节点的区的集合；
- 回滚段：存放的是回滚数据的区的集合，之前讲[事务隔离 (opens new window)](https://xiaolincoding.com/mysql/transaction/mvcc.html)的时候就介绍到了 MVCC 利用了回滚段实现了多版本查询数据。

### 数据库的三范式是什么？

1. 第一范式：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项；
2. 第二范式：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性；
3. 第三范式：任何非主属性不依赖于其它非主属性。

解释：

1NF：原子性。 字段不可再分,否则就不是关系数据库;　　

2NF：唯一性 。一个表只说明一个事物； 　　3NF：每列都与主键有直接关系，不存在传递依赖。

### 什么是超键？什么是主键？

**超 键**：在关系中，能唯一标识元组的属性集称为关系模式的超键。一个属性可以作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。

**候选键**：是最小超键，即没有冗余元素的超键。

**主 键**：数据库表中对储存数据对象予以 **唯一和完整标识的数据列或属性的组合**。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（NULL）。

**外 键**：在一个表中存在的另一个表的主键称此表的外键，外键可以有重复的, 可以是空值。外键是用来和其他表建立联系用的。

**举例**：

| 学号     | 姓名   | 性别 | 年龄 | 系别   | 专业     |
| -------- | ------ | ---- | ---- | ------ | -------- |
| 20020612 | 李辉   | 男   | 20   | 计算机 | 软件开发 |
| 20060613 | 张明   | 男   | 18   | 计算机 | 软件开发 |
| 20060614 | 王小玉 | 女   | 19   | 物理   | 力学     |
| 20060615 | 李淑华 | 女   | 17   | 生物   | 动物学   |
| 20060616 | 赵静   | 男   | 21   | 化学   | 食品化学 |
| 20060617 | 赵静   | 女   | 20   | 生物   | 植物学   |

1. 超键：于是我们从例子中可以发现 学号是标识学生实体的唯一标识。那么该元组的超键就为学号。除此之外我们还可以把它跟其他属性组合起来，比如：(`学号`，`性别`)，(`学号`，`年龄`)
2. 候选键：根据例子可知，学号是一个可以唯一标识元组的唯一标识，因此学号是一个候选键，实际上，候选键是超键的子集，比如 （学号，年龄）是超键，但是它不是候选键。因为它还有了额外的属性。
3. 主键：简单的说，例子中的元组的候选键为学号，但是我们选定他作为该元组的唯一标识，那么学号就为主键。
4. 外键是相对于主键的，比如在学生记录里，主键为学号，在成绩单表中也有学号字段，因此学号为成绩单表的外键，为学生表的主键。

**主键为候选键的子集，候选键为超键的子集，而外键的确定是相对于主键的**

### UNION 与 UNION ALL 的区别

UNION 用于把来自多个 SELECT 语句的结果组合到一个结果集合中，MySQL 会把结果集中 重复的记录删掉，而使用 UNION ALL，MySQL 会把所有的记录返回，且效率高于 UNION 。

### DROP、DELETE 与 TRUNCATE 的区别

三种都可以表示删除，其中的细微区别之处如下：

|              | DROP                                                  | DELETE                                    | TRUNCATE                        |
| ------------ | ----------------------------------------------------- | ----------------------------------------- | ------------------------------- |
| SQL 语句类型 | DDL                                                   | DML                                       | DDL                             |
| 回滚         | 不可回滚                                              | 可回滚                                    | 不可回滚                        |
| 删除内容     | 从数据库中 删除表，所有的数据行，索引和权限也会被删除 | 表结构还在，删除表的 全部或者一部分数据行 | 表结构还在，删除表中的 所有数据 |
| 删除速度     | 删除速度最快                                          | 删除速度慢，需要逐行删除                  | 删除速度快                      |

因此，在不再需要一张表的时候，采用 DROP；在想删除部分数据行时候，用 DELETE；在保留表而删除所有数据的时候用 TRUNCATE。

###  什么是分库？水平切分、垂直切分

**分库** 就是将数据库中的数据分散到不同的数据库上，可以垂直分库，也可以水平分库。

- **水平切分**

水平切分是将同一个表中的记录拆分到多个结构相同的表中。当一个表的数据不断增多时，水平切分是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。

- **垂直切分**

垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。例如：将原来的电商数据库垂直切分成商品数据库、用户数据库等。

#### 什么是分表？**垂直分表** 、水平分表

**分表** 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。

**垂直分表** 是对数据表列的拆分，把一张列比较多的表拆分为多张表。

举个例子：我们可以将用户信息表中的一些列单独抽出来作为一个表。

**水平分表** 是对数据表行的拆分，把一张行比较多的表拆分为多张表，可以解决单一表数据量过大的问题。

#### 什么情况下需要分库分表？

遇到下面几种场景可以考虑分库分表：

- 单表的数据达到千万级别以上，数据库读写速度比较缓慢。
- 数据库中的数据占用的空间越来越大，备份时间越来越长。
- 应用的并发量太大

#### 为什么要分库分表

数据库中的数据量不一定是可控的，随着时间和业务的发展，库中的表会越来越多，表中的数据量也会越来越大，相应地数据操作，例如 **增删改查的开销** 也会越来越大；另外，若不进行分布式部署，而一台服务器的 **资源** （CPU、磁盘、内存、IO 等）是有限的，最终数据库所能承载的数据量、数据处理能力都将遭遇瓶颈。所以，从 **性能** 和 **可用性** 角度考虑，会进行数据库拆分处理，具体地说，把原本存储于一个库的数据分块存储到多个库上，把原本存储于一个表的数据分块存储到多个表上，即 分库分表。

#### 为什么要进行分库和分表呢？都放在一个库、表中不可吗？

分库与分表的目的在于，减小数据库的单库单表负担，提高查询性能，缩短查询时间。

**通过分表**，可以减少数据库的单表负担，将压力分散到不同的表上，同时因为不同的表上的数据量少了，起到提高查询性能，缩短查询时间的作用，此外，可以很大的缓解表锁的问题。 分表策略可以归纳为垂直拆分和水平拆分： **水平分表**：取模分表就属于随机分表，而时间维度分表则属于连续分表。 如何设计好垂直拆分，我的建议：将不常用的字段单独拆分到另外一张扩展表. 将大文本的字段单独拆分到另外一张扩展表, 将不经常修改的字段放在同一张表中，将经常改变的字段放在另一张表中。 对于海量用户场景，可以考虑取模分表，数据相对比较均匀，不容易出现热点和并发访问的瓶颈。

**库内分表**，仅仅是解决了单表数据过大的问题，但并没有把单表的数据分散到不同的物理机上，因此并不能减轻 MySQL 服务器的压力，仍然存在同一个物理机上的资源竞争和瓶颈，包括 CPU、内存、磁盘 IO、网络带宽等。

**分库与分表带来的分布式困境与应对之策** 数据迁移与扩容问题----一般做法是通过程序先读出数据，然后按照指定的分表策略再将数据写入到各个分表中。 分页与排序问题----需要在不同的分表中将数据进行排序并返回，并将不同分表返回的结果集进行汇总和再次排序，最后再返回给用户

#### 分片算法有哪些？

分片算法主要解决了数据被水平分片之后，数据究竟该存放在哪个表的问题。

- **哈希分片**：求指定 key（比如 id） 的哈希，然后根据哈希值确定数据应被放置在哪个表中。哈希分片比较适合随机读写的场景，不太适合经常需要范围查询的场景。
- **范围分片**：按照特性的范围区间（比如时间区间、ID 区间）来分配数据，比如 将 `id` 为 `1~299999` 的记录分到第一个库， `300000~599999` 的分到第二个库。范围分片适合需要经常进行范围查找的场景，不太适合随机读写的场景（数据未被分散，容易出现热点数据的问题）。
- **地理位置分片**：很多 NewSQL 数据库都支持地理位置分片算法，也就是根据地理位置（如城市、地域）来分配数据。
- **融合算法**：灵活组合多种分片算法，比如将哈希分片和范围分片组合。

#### 分库分表存在哪些问题

**join 操作**：同一个数据库中的表分布在了不同的数据库中，导致无法使用 join 操作。这样就导致我们需要手动进行数据的封装，比如你在一个数据库中查询到一个数据之后，再根据这个数据去另外一个数据库中找对应的数据。

**事务问题**：同一个数据库中的表分布在了不同的数据库中，如果单个操作涉及到多个数据库，那么数据库自带的事务就无法满足我们的要求了。

**分布式 id**：分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？这个时候，我们就需要为我们的系统引入分布式 id 了。

#### 分库分表后，数据怎么迁移呢？

我们对老库的更新操作（增删改），同时也要写入新库（双写）。如果操作的数据不存在于新库的话，需要插入到新库中。 这样就能保证，咱们新库里的数据是最新的。

在迁移过程，双写只会让被更新操作过的老库中的数据同步到新库，我们还需要自己写脚本将老库中的数据和新库的数据做比对。如果新库中没有，那咱们就把数据插入到新库。如果新库有，旧库没有，就把新库对应的数据删除（冗余数据清理）。

重复上一步的操作，直到老库和新库的数据一致为止。

### 读写分离

#### 什么是读写分离？

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。

**读写分离主要是为了将对数据库的读写操作分散到不同的数据库节点上。** 这样的话，就能够小幅提升写性能，大幅提升读性能。

我简单画了一张图来帮助不太清楚读写分离的小伙伴理解。

![读写分离示意图](https://oss.javaguide.cn/github/javaguide/high-performance/read-and-write-separation-and-library-subtable/read-and-write-separation.png)读写分离示意图

一般情况下，我们都会选择一主多从，也就是一台主数据库负责写，其他的从数据库负责读。主库和从库之间会进行数据同步，以保证从库中数据的准确性。这样的架构实现起来比较简单，并且也符合系统的写少读多的特点。

#### 读写分离会带来什么问题？如何解决？

读写分离对于提升数据库的并发非常有效，但是，同时也会引来一个问题：主库和从库的数据存在延迟，比如你写完主库之后，主库的数据同步到从库是需要时间的，这个时间差就导致了主库和从库的数据不一致性问题。这也就是我们经常说的 **主从同步延迟** 。

主从同步延迟问题的解决，没有特别好的一种方案（可能是我太菜了，欢迎评论区补充）。你可以根据自己的业务场景，参考一下下面几种解决办法。

**1.强制将读请求路由到主库处理。**

既然你从库的数据过期了，那我就直接从主库读取嘛！这种方案虽然会增加主库的压力，但是，实现起来比较简单，也是我了解到的使用最多的一种方式。

比如 `Sharding-JDBC` 就是采用的这种方案。通过使用 Sharding-JDBC 的 `HintManager` 分片键值管理器，我们可以强制使用主库。

```java
HintManager hintManager = HintManager.getInstance();
hintManager.setMasterRouteOnly();
// 继续JDBC操作
```

对于这种方案，你可以将那些必须获取最新数据的读请求都交给主库处理。

**2.延迟读取。**

还有一些朋友肯定会想既然主从同步存在延迟，那我就在延迟之后读取啊，比如主从同步延迟 0.5s,那我就 1s 之后再读取数据。这样多方便啊！方便是方便，但是也很扯淡。

不过，如果你是这样设计业务流程就会好很多：对于一些对数据比较敏感的场景，你可以在完成写请求之后，避免立即进行请求操作。比如你支付成功之后，跳转到一个支付成功的页面，当你点击返回之后才返回自己的账户。

#### 如何实现读写分离？

不论是使用哪一种读写分离具体的实现方案，想要实现读写分离一般包含如下几步：

1. 部署多台数据库，选择其中的一台作为主数据库，其他的一台或者多台作为从数据库。
2. 保证主数据库和从数据库之间的数据是实时同步的，这个过程也就是我们常说的**主从复制**。
3. 系统将写请求交给主数据库处理，读请求交给从数据库处理。

落实到项目本身的话，常用的方式有两种：

**1. 代理方式**

![代理方式实现读写分离](https://oss.javaguide.cn/github/javaguide/high-performance/read-and-write-separation-and-library-subtable/read-and-write-separation-proxy.png)代理方式实现读写分离

我们可以在应用和数据中间加了一个代理层。应用程序所有的数据请求都交给代理层处理，代理层负责分离读写请求，将它们路由到对应的数据库中。

提供类似功能的中间件有 **MySQL Router**（官方）、**Atlas**（基于 MySQL Proxy）、**MaxScale**、**MyCat**。

**2. 组件方式**

在这种方式中，我们可以通过引入第三方组件来帮助我们读写请求。

这也是我比较推荐的一种方式。这种方式目前在各种互联网公司中用的最多的，相关的实际的案例也非常多。如果你要采用这种方式的话，推荐使用 `sharding-jdbc` ，直接引入 jar 包即可使用，非常方便。同时，也节省了很多运维的成本。

#### **读写分离能提高性能的原因在于：**

1. 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
2. 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
3. 增加冗余，提高可用性。

#### 主从复制原理是什么？

MySQL binlog(binary log 即二进制日志文件) 主要记录了 MySQL 数据库中数据的所有变化(数据库执行的所有 DDL 和 DML 语句)。因此，我们根据主库的 MySQL binlog 日志就能够将主库的数据同步到从库中。

1. 主库将数据库中数据的变化写入到 binlog
2. 从库连接主库
3. 从库会创建一个 I/O 线程向主库请求更新的 binlog
4. 主库会创建一个 binlog dump 线程来发送 binlog ，从库中的 I/O 线程负责接收
5. 从库的 I/O 线程将接收的 binlog 写入到 relay log 中。
6. 从库的 SQL 线程读取 relay log 同步数据本地（也就是再执行一遍 SQL ）。



#### MySQL 读写分离的实现方案

MySQL 读写分离的实现方式主要基于 主从复制，通过 路由的方式 使应用对数据库的写请求只在 Master 上进行，读请求在 Slave 上进行。

具体地，有以下四种实现方案：

**方案一：基于 MySQL proxy 代理**

在应用和数据库之间增加 代理层，代理层接收应用对数据库的请求，根据不同请求类型（即是读 read 还是写 write）转发到不同的实例，在实现读写分离的同时可以实现负载均衡。MySQL 的代理最常见的是 mysql-proxy、cobar、mycat、Atlas 等。

**方案二：基于应用内路由**

基于应用内路由的方式即为在应用程序中实现，针对不同的请求类型去不同的实例执行 SQL。

具体实现可基于 spring 的 aop：用 aop 来拦截 spring 项目的 dao 层方法，根据方法名称就可以判断要执行的类型，进而动态切换主从数据源。

**方案三：基于 MySQL-Connector-Java 的 JDBC 驱动方式**

Java 程序通过在连接 MySQL 的 JDBC 中配置主库与从库等地址，JDBC 会自动将读请求发送给从库，将写请求发送给主库，此外， MySQL 的 JDBC 驱动还能够实现多个从库的负载均衡。

**方案四：基于 sharding-jdbc 的方式**

sharding-sphere 是强大的读写分离、分表分库中间件，sharding-jdbc 是 sharding-sphere 的核心模块。

### MySQL 问题排查都有哪些手段？

1. 使用 show processlist 命令查看当前所有连接信息；
2. 使用 Explain 命令查询 SQL 语句执行计划；
3. 开启慢查询日志，查看慢查询的 SQL。





### 自增值不连续的 4 个场景：

1. 自增初始值和自增步长设置不为 1
2. 唯一键冲突
3. 事务回滚
4. 批量插入（如 `insert...select` 语句）

### Innodb为什么要用自增id作为主键？

如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE（optimize table）来重建表并优化填充页面

### InnoDB 和 MyISAM 的比较？适用场景

1. 事务：MyISAM不支持事务，InnoDB支持事务；
2. 全文索引：MyISAM 支持全文索引，InnoDB 5.6 之前不支持全文索引；
3. 关于 count(*)：MyISAM会直接存储总行数，InnoDB 则不会，需要按行扫描。意思就是对于 select count(*) from table; 如果数据量大，MyISAM 会瞬间返回，而 InnoDB 则会一行行扫描；
4. 外键：MyISAM 不支持外键，InnoDB 支持外键；
5. 锁：MyISAM 只支持表锁，InnoDB 可以支持行锁。

**适用场景**： MyISAM适合： 插入不频繁，查询非常频繁，如果执行大量的SELECT，MyISAM是更好的选择， 没有事务。 InnoDB适合： 可靠性要求比较高，或者要求事务； 表更新和查询都相当的频繁， 大量的INSERT或UPDATE

### SQL中的NOW()和CURRENT_DATE()两个函数有什么区别？

NOW（）命令用于显示当前年份，月份，日期，小时，分钟和秒。 CURRENT_DATE（）仅显示当前年份，月份和日期。

### SQL语法中内连接、自连接、外连接（左、右、全）、交叉连接的区别分别是什么？

内连接：只有两个元素表相匹配的才能在结果集中显示。 外连接： 左外连接: 左边为驱动表，驱动表的数据全部显示，匹配表的不匹配的不会显示。 右外连接:右边为驱动表，驱动表的数据全部显示，匹配表的不匹配的不会显示。 全外连接：连接的表中不匹配的数据全部会显示出来。 交叉连接： 笛卡尔效应，显示的结果是链接表数的乘积。

### 哪些数据库结构优化的手段？

- **范式优化**： 比如消除冗余（节省空间。。）
- **反范式优化**：比如适当加冗余等（减少join）
- **限定数据的范围**： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。
- **读/写分离**： 经典的数据库拆分方案，主库负责写，从库负责读；
- **拆分表**：分区将数据在物理上分隔开，不同分区的数据可以制定保存在处于不同磁盘上的数据文件里。这样，当对这个表进行查询时，只需要在表分区中进行扫描，而不必进行全表扫描，明显缩短了查询时间，另外处于不同磁盘的分区也将对这个表的数据传输分散在不同的磁盘I/O，一个精心设置的分区可以将数据传输对磁盘I/O竞争均匀地分散开。对数据量大的时时表可采取此方法。可按月自动建表分区

### COMPACT 行格式长什么样？

一条完整的记录分为「记录的额外信息」和「记录的真实数据」两个部分，记录的额外信息包含 3 个部分：变长字段长度列表、NULL 值列表、记录头信息。

记录真实数据部分除了我们定义的字段，还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer

**为什么「变长字段长度列表」的信息要按照逆序存放？**

「变长字段长度列表」中的信息之所以要逆序存放，是因为这样可以**使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率**。同样的道理， NULL 值列表的信息也需要逆序存放。

每个数据库表的行格式都有「变长字段字节数列表」吗？

**当数据表没有变长字段的时候，比如全部都是 int 类型的字段，这时候表里的行格式就不会有「变长字段长度列表」了**，因为没必要，不如去掉以节省空间。

所以「变长字段长度列表」只出现在数据表有变长字段的时候。

##### 每个数据库表的行格式都有「NULL 值列表」吗？

当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了。

「NULL 值列表」是固定 1 字节空间吗？如果这样的话，一条记录有 9 个字段值都是 NULL，这时候怎么表示？

「NULL 值列表」的空间不是固定 1 字节的。

当一条记录有 9 个字段值都是 NULL，那么就会创建 2 字节空间的「NULL 值列表」，以此类推

### 磁盘查询时就会有大量的随机I/O，随机 I/O 是非常慢，怎么解决？

在表中数据量大的时候，为某个索引分配空间的时候就不再按照页为单位分配了，而是按照区（extent）为单位分配。每个区的大小为 1MB，对于 16KB 的页来说，连续的 64 个页会被划为一个区，这样就使得链表中相邻的页的物理位置也相邻，就能使用顺序 I/O 了

### MySQL 的 NULL 值是怎么存放的？

MySQL 的 Compact 行格式中会用「NULL值列表」来标记值为 NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。

NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。

### 行溢出后，MySQL 是怎么处理的？

如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。

Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。

Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。

# =====================

## Mysql字段类型====

### char 和 varchar 的区别？

**char(n) ：**固定长度类型，比如：订阅 char(10)，当你输入”abc”三个字符的时候，它们占的空间还是 10 个字节，其他 7 个是空字节。char 优点：效率高；缺点：占用空间；适用场景：存储密码的 md5 值，固定长度的，使用 char 非常合适。

**varchar(n) ：**可变长度，存储的值是每个值占用的字节再加上一个用来记录其长度的字节的长度。

所以，从空间上考虑 varcahr 比较合适；从效率上考虑 char 比较合适，二者使用需要权衡。

### varchar(10) 和 varchar(20) 的区别？

varchar(10) 中 10 的涵义最多存放 10 个字符，varchar(10) 和 varchar(20) 存储 hello 所占空间一样，但后者在排序时会消耗更多内存，因为 order by col 采用 fixed_length 计算 col 长度

### varchar(100)和 varchar(10)的区别是什么？

varchar(100)和 varchar(10)都是变长类型，表示能存储最多 100 个字符和 10 个字符。因此，varchar (100) 可以满足更大范围的字符存储需求，有更好的业务拓展性。而 varchar(10)存储超过 10 个字符时，就需要修改表结构才可以。

虽说 varchar(100)和 varchar(10)能存储的字符范围不同，但二者存储相同的字符串，所占用磁盘的存储空间其实是一样的，这也是很多人容易误解的一点。

不过，varchar(100)会消耗更多的内存。这是因为 varchar 类型在内存中操作时，通常会分配固定大小的内存块来保存值，即使用字符类型中定义的长度。例如在进行排序的时候，varcahr(100)是按照 100 这个长度来进行的，也就会消耗更多内存

### decimal 和 float/double 的区别是什么？

decimal 和 float 的区别是：**decimal 是定点数，float/double 是浮点数。decimal 可以存储精确的小数值，float/double 只能存储近似的小数值。**

decimal 用于存储有精度要求的小数比如与金钱相关的数据，可以避免浮点数带来的精度损失。

### datetime 和 timestamp 的区别是什么？

DateTime 类型没有时区信息，Timestamp 和时区有关。

Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。

- DateTime：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59
- Timestamp：1970-01-01 00:00:01 ~ 2037-12-31 23:59:59

### NULL 和 '' 的区别是什么？

`NULL` 跟 `''`(空字符串)是两个完全不一样的值，区别如下：

- `NULL` 代表一个不确定的值,就算是两个 `NULL`,它俩也不一定相等。例如，`SELECT NULL=NULL`的结果为 false，但是在我们使用`DISTINCT`,`GROUP BY`,`ORDER BY`时,`NULL`又被认为是相等的。
- `''`的长度是 0，是不占用空间的，而`NULL` 是需要占用空间的。
- `NULL` 会影响聚合函数的结果。例如，`SUM`、`AVG`、`MIN`、`MAX` 等聚合函数会忽略 `NULL` 值。 `COUNT` 的处理方式取决于参数的类型。如果参数是 `*`(`COUNT(*)`)，则会统计所有的记录数，包括 `NULL` 值；如果参数是某个字段名(`COUNT(列名)`)，则会忽略 `NULL` 值，只统计非空值的个数。
- 查询 `NULL` 值时，必须使用 `IS NULL` 或 `IS NOT NULLl` 来判断，而不能使用 =、!=、 <、> 之类的比较运算符。而`''`是可以使用这些比较运算符的。

看了上面的介绍之后，相信你对另外一个高频面试题：“为什么MySQL不建议使用 `NULL` 作为列默认值？

### 说一说Drop、Delete与Truncate的共同点和区别

**第一种回答**

Drop、Delete、Truncate都表示删除，但是三者有一些差别：

**Delete**用来删除表的全部或者一部分数据行，执行delete之后，用户需要提交(commmit)或者回滚(rollback)来执行删除或者撤销删除，会触发这个表上所有的delete触发器。

**Truncate**删除表中的所有数据，这个操作不能回滚，也不会触发这个表上的触发器，TRUNCATE比delete更快，占用的空间更小。

**Drop**命令从数据库中删除表，所有的数据行，索引和权限也会被删除，所有的DML触发器也不会被触发，这个命令也不能回滚。

因此，在不再需要一张表的时候，用Drop；在想删除部分数据行时候，用Delete；在保留表而删除所有数据的时候用Truncate

### MySQL 怎么知道 varchar(n) 实际占用数据的大小？

MySQL 的 Compact 行格式中会用「变长字段长度列表」存储变长字段实际占用的数据大小。

### varchar(n) 中 n 最大取值为多少？

一行记录最大能存储 65535 字节的数据，但是这个是包含「变长字段字节数列表所占用的字节数」和「NULL值列表所占用的字节数」。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去这两个列表所占用的字节数。会用 1 字节来表示「NULL 值列表」。

如果一张表只有一个 varchar(n) 字段，且允许为 NULL，字符集为 ascii。varchar(n) 中 n 最大取值为 65532。

计算公式：65535 - 变长字段字节数列表所占用的字节数 - NULL值列表所占用的字节数 = 65535 - 2 - 1 = 65532。

如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 <= 65535。

#### 「变长字段长度列表」所占用的字节数是多少？

「变长字段长度列表」所占用的字节数 = 所有「变长字段长度」占用的字节数之和。

- 条件一：如果变长字段允许存储的最大字节数小于等于 255 字节，就会用 1 字节表示「变长字段长度」；
- 条件二：如果变长字段允许存储的最大字节数大于 255 字节，就会用 2 字节表示「变长字段长度」；

我们这里字段类型是 varchar(65535) ，字符集是 ascii，所以代表着变长字段允许存储的最大字节数是 65535，符合条件二，所以会用 2 字节来表示「变长字段长度」。

因为我们这个案例是只有 1 个变长字段，所以「变长字段长度列表」= 1 个「变长字段长度」占用的字节数，也就是 2 字节。

因为我们在算 varchar(n) 中 n 最大值时，需要减去 「变长字段长度列表」和 「NULL 值列表」所占用的字节数的。所以，在数据库表只有一个 varchar(n) 字段且字符集是 ascii 的情况下，varchar(n) 中 n 最大值 = 65535 - 2 - 1 = 65532。

# =====================

## Mysql 索引=====

### 谈谈你对索引的理解？

索引的出现是为了提高数据的查询效率，就像书的目录一样。一本500页的书，如果你想快速找到其中的某一个知识点，在不借助目录的情况下，那我估计你可得找一会儿。同样，对于数据库的表而言，索引其实就是它的“目录”。

同样索引也会带来很多负面影响：创建索引和维护索引需要耗费时间，这个时间随着数据量的增加而增加；索引需要占用物理空间，不光是表需要占用数据空间，每个索引也需要占用物理空间；当对表进行增、删、改、的时候索引也要动态维护，这样就降低了数据的维护速度。

### 为什么使用索引？

- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。
- 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。
- 帮助服务器避免排序和临时表
- 将随机IO变为顺序IO。
- 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。

### 什么时候需要建立数据库索引呢？

在最频繁使用的、用以缩小查询范围的字段,需要排序的字段上建立索引。 不宜： 1）对于查询中很少涉及的列或者重复值比较多的列 2）对于一些特殊的数据类型，不宜建立索引，比如文本字段（text）等

### 什么时候需要 / 不需要创建索引？

- 索引也是有缺点的，比如：

  - 需要占用物理空间，数量越大，占用空间越大；

  - 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增大；

  - 会降低表的增删改的效率，因为每次增删改索引，B+ 树为了维护索引有序性，都需要进行动态维护

- **什么时候适用索引？**
  - 字段有唯一性限制的，比如商品编码；
  - 经常用于 `WHERE` 查询条件的字段，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
  - 经常用于 `GROUP BY` 和 `ORDER BY` 的字段，这样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的

- **什么时候不需要创建索引？**
  - `WHERE` 条件，`GROUP BY`，`ORDER BY` 里用不到的字段，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。
  - 字段中存在大量重复数据，不需要创建索引，MySQL 有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描
  - 表数据太少的时候，不需要创建索引；
  - 经常更新的字段不用创建索引，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的

### 创建索引时需要注意什么？

非空字段：应该指定列为NOT NULL，除非你想存储NULL。在 MySQL 中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值；

取值离散大的字段：（变量各个取值之间的差异程度）的列放到联合索引的前面，可以通过count()函数查看字段的差异值，返回值越大说明字段的唯一值越多字段的离散程度高；

索引字段越小越好：数据库的数据存储以页为单位一页存储的数据越多一次IO操作获取的数据越大效率越高。 唯一、不为空、经常被查询的字段 的字段适合建索引

### 什么时候索引会失效？

- 使用左或者左右模糊匹配，也就是‘like %xx'或者’like %xx%'
- 在查询条件中对索引列做了计算、函数、类型转换操作
- 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效
- WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效
- 补：常见扫描类型的**执行效率从低到高的顺序为**：
  - All（全表扫描）；
  - index（全索引扫描）；
  - range（索引范围扫描）；
  - ref（非唯一索引扫描）；
  - eq_ref（唯一索引扫描）；**多表联查**
  - const（结果只有一条的主键或唯一索引扫描）。

### 索引失效有哪些？

- 1、当我们使用**左或者左右模糊匹配**的时候，也就是 `like %xx` 或者 `like %xx%`这两种方式都会造成索引失效；
  - **因为索引 B+ 树是按照「索引值」有序排列存储的，只能根据前缀进行比较。**
- 2、当我们在查询条件中对索引**列使用函数，**就会导致索引失效。
  - 因为索引保存的是索引字段的原始值，而不是经过函数计算后的值，自然就没办法走索引了
- 当我们在查询条件中对**索引列进行表达式计算**，也是无法走索引的。原因跟对索引使用函数差不多。
- MySQL 在遇到字符串和数字比较的时候，**会自动把字符串转为数字，然后再进行比较**。如果字符串是索引列，而条件语句中的输入参数是数字的话，那么索引列会发生隐式类型转换，由于**隐式类型转换是通过 CAST 函数实现**的，等同于对索引列使用了函数，所以就会导致索引失效。
- 3、联合索引要能正确使用需要遵循**最左匹配原则**，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
  - MySQL 5.6 之后，有一个**索引下推功能**，可以在**存储引擎层**进行索引遍历过程中，对**索引中包含的字段先做判断**，直接**过滤掉不满足条件的记录**，再**返还给 Server 层，从而减少回表次数**
  - **索引下推原理**：截断的字段不会在 Server 层进行条件判断，而是会被下推到**「存储引擎层」进行条件判断**，然后过滤出符合条件的数据后再返回给 Server 层。由于在引擎层就过滤掉大量的数据，无需再回表读取数据来进行判断，减少回表次数，从而提升了性能。
- 4、在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。
  - 因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要**有条件列不是索引列，就会进行全表扫描。**

### 用 like “%x“，索引一定会失效吗

- 假设现在数据库表只有两个字段，一个是**主键索引 id**，另外一个是**二级索引 name**，则`select * from t_user where name like "%xx";`用上了二级索引，而且从 Extra 里的 Using index 说明用上了**覆盖索引**
  - 这张表的字段**没有「非索引」字段**，所以 `select *` 相当于 `select id,name`，然后**这个查询的数据都在二级索引的 B+ 树，因为二级索引的 B+ 树的叶子节点包含「索引值==>主键值」，所以查二级索引的 B+ 树就能查到全部结果了，这个就是覆盖索引。**
  - 执行计划里的 type 是 `index`，这代表着是通过**全扫描二级索引的 B+ 树的方式查询到数据的**，也就是遍历了整颗索引树。
  - 执行计划中 type 如果是 `range`，表示**对索引列进行范围查询**，也就是利用了索引树的有序性的特点，通过查询比较的方式，快速定位到了数据行。所以，**type=range 的查询效率会比 type=index 的高一些**

#### 选全扫描二级索引树，而不扫描聚簇索引树？

- 因为二级索引树的记录东西**很少**，就只有**「索引列+主键值」**，而聚簇索引记录的东西会更多，比如聚簇索引中的叶子节点则记录了主键值、事务 id、用于事务id和 MVCC 的回滚指针id以及所有的剩余列。
- 再加上，这个 **select * 不用执行回表操作**。
- 所以， **MySQL 优化器**认为直接遍历二级索引树要比遍历聚簇索引树的**成本要小的多**，因此 MySQL 选择了「**全扫描二级索引树**」的方式查询数据。

#### 数据表加了非索引字段，怎么变成走的是全表扫描呢？

- **加了其他字段**后，`select * from t_user where name like "%xx";` 要查询的数据就不能只在二级索引树里找了，得需要回表操作才能完成查询的工作，再加上是**左模糊匹配，无法利用索引树的有序性来快速定位数据**，所以得在二级索引树逐一遍历，获取主键值后，再到聚簇索引树检索到对应的数据行，这样实在太累了。
- 所以，优化器认为上面这样的查询过程的成本实在太高了，所以直接选择**全表扫描的方式来查询数据**。

- **答：**使用左模糊匹配（like "%xx"）并不一定会走全表扫描，关键还是**看数据表中的字段**。如果数据库表中的**字段只有主键+二级索引**，那么即使使用了左模糊匹配，也不会**走全表扫描**（type=all），而是走**全扫描二级索引树**(type=index)。
  - **联合索引要遵循最左匹配才能走索引**，但是如果数据库表中的字段都是索引的话，即使查询过程中，**没有遵循最左匹配原则，也是走全扫描二级索引树**(type=index)

### InnoDB 是如何存储数据的？

- 记录是按照**行来存储**的，但是数据库的**读取并不以「行」为单位**，否则一次读取（也就是一次 I/O 操作）只能处理一行数据，效率会非常低。因此，**InnoDB 的数据是按「数据页」为单位来读写的**，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。
  - 数据库的 I/O 操作的最小单位是页，**InnoDB 数据页的默认大小是 16KB**，意味着数据库每次读写都是以 16KB 为单位的，一次最少从磁盘中读取 16K 的内容到内存中，一次最少把内存中的 16K 内容刷新到磁盘中。

- 数据页包括七个部分，分别是**文件头、页头、最大最小记录、用户记录、空闲空间、页目录、文件尾**；在 File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表，采用链表的结构是让数据页之间不需要是物理上的连续的，而是**逻辑上的连续**。

- **数据页中的记录按照「主键==key」顺序组成单向链表**，单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。因此，数据页中有一个**页目录**，起到记录的**索引**作用

- 每组的地址偏移量也被称之为槽（slot），**每个槽相当于指针指向了不同组的最后一个记录**
  - **页目录就是由多个槽组成的，槽相当于分组记录的索引**。然后，因为记录是按照「主键值」从小到大排序的，所以**通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽（哪个记录分组），定位到槽后，再遍历槽内的所有记录，找到对应的记录**，无需从最小记录==数据开始遍历整个页中的记录链表

### B+ 树是如何进行查询数据记录的？

- 当需要存储大量的记录时，就需要多个数据页，这时就需要考虑如何建立合适的索引，才能方便定位记录所在的页；为了解决这个问题，**InnoDB 采用了 B+ 树作为索引**。磁盘的 I/O 操作次数对索引的使用效率至关重要，因此在构造索引的时候，更倾向于采用“矮胖”的 B+ 树数据结构，这样所需要进行的**磁盘 I/O 次数更少**，而且 B+ 树 更适合进行**关键字的范围查询**
- InnoDB 里的 B+ 树中的**每个节点都是一个数据页**
- B+ 树的特点：
  - 只有叶子节点（最底层的节点）才存放了数据，非叶子节点（其他上层节）仅用来存放目录项作为索引。
  - 非叶子节点分为不同层次，通过分层来降低每一层的搜索量；
  - 所有节点按照**索引键key**大小排序，构成一个双向链表，便于范围查询；
- **答：**在定位记录所在哪一个页时，也是通过**二分法快速定位到包含该记录的页**。定位到该页后，又会在该页内进行**二分法快速定位**记录所在的分组（槽号），最后在**分组内**进行遍历查找

### 索引的优缺点

**优点**：

- 使用索引可以大大加快 数据的检索速度（大大减少检索的数据量）, 这也是创建索引的最主要的原因。
- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。

**缺点**：

- 创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。
- 索引需要使用物理文件存储，也会耗费一定空间。

但是，**使用索引一定能提高查询性能吗?**

大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升

**建立索引的原则：**

1. 在最频繁使用的、用以缩小查询范围的字段上建立索引；
2. 在频繁使用的、需要排序的字段上建立索引。

**不适合建立索引的情况：**

1. 对于查询中很少涉及的列或者重复值比较多的列，不宜建立索引；
2. 对于一些特殊的数据类型，不宜建立索引，比如：文本字段（text）等。

### 索引底层数据结构选型

索引的数据结构和具体存储引擎的实现有关,，在MySQL中使用较多的索引有 Hash 索引、B+树索引等。而我们经常使用的 InnoDB 存储引擎的默认索引实现为 B+ 树索引。

#### Hash 表

哈希表是键值对的集合，通过键(key)即可快速取出对应的值(value)，因此哈希表可以快速检索数据（接近 O（1））。

**为何能够通过 key 快速取出 value 呢？** 原因在于 **哈希算法**（也叫散列算法）。通过哈希算法，我们可以快速找到 key 对应的 index，找到了 index 也就找到了对应的 value。

```java
hash = hashfunc(key)
index = hash % array_size
```

但是！哈希算法有个 **Hash 冲突** 问题，也就是说多个不同的 key 最后得到的 index 相同。通常情况下，我们常用的解决办法是 **链地址法**。链地址法就是将哈希冲突数据存放在链表中。就比如 JDK1.8 之前 `HashMap` 就是通过链地址法来解决哈希冲突的。不过，JDK1.8 以后`HashMap`为了减少链表过长的时候搜索时间过长引入了红黑树。

为了减少 Hash 冲突的发生，一个好的哈希函数应该“均匀地”将数据分布在整个可能的哈希值集合中。

既然哈希表这么快，**为什么 MySQL 没有使用其作为索引的数据结构呢？** 主要是因为 Hash 索引不支持顺序和范围查询。假如我们要对表中的数据进行排序或者进行范围查询，那 Hash 索引可就不行了。并且，每次 IO 只能取一个。

试想一种情况:

```java
SELECT * FROM tb1 WHERE id < 500;
```

在这种范围查询中，优势非常大，直接遍历比 500 小的叶子节点就够了。而 Hash 索引是根据 hash 算法来定位的，难不成还要把 1 - 499 的数据，每个都进行一次 hash 计算来定位吗?这就是 Hash 最大的缺点了。

#### 二叉查找树(BST)

二叉查找树（Binary Search Tree）是一种基于二叉树的数据结构，它具有以下特点：

1. 左子树所有节点的值均小于根节点的值。
2. 右子树所有节点的值均大于根节点的值。
3. 左右子树也分别为二叉查找树。

当二叉查找树是平衡的时候，也就是树的每个节点的左右子树深度相差不超过 1 的时候，查询的时间复杂度为 O(log2(N))，具有比较高的效率。然而，当二叉查找树不平衡时，例如在最坏情况下（有序插入节点），树会退化成线性链表（也被称为斜树），导致查询效率急剧下降，时间复杂退化为 O（N）。

也就是说，**二叉查找树的性能非常依赖于它的平衡程度，这就导致其不适合作为 MySQL 底层索引的数据结构。**

为了解决这个问题，并提高查询效率，人们发明了多种在二叉查找树基础上的改进型数据结构，如平衡二叉树、B-Tree、B+Tree 等。

#### AVL 树

AVL 树是计算机科学中最早被发明的自平衡二叉查找树，它的名称来自于发明者 G.M. Adelson-Velsky 和 E.M. Landis 的名字缩写。AVL 树的特点是保证任何节点的左右子树高度之差不超过 1，因此也被称为高度平衡二叉树，它的查找、插入和删除在平均和最坏情况下的时间复杂度都是 O(logn)。

AVL 树采用了旋转操作来保持平衡。主要有四种旋转操作：LL 旋转、RR 旋转、LR 旋转和 RL 旋转。其中 LL 旋转和 RR 旋转分别用于处理左左和右右失衡，而 LR 旋转和 RL 旋转则用于处理左右和右左失衡。

由于 AVL 树需要频繁地进行旋转操作来保持平衡，因此会有较大的计算开销进而降低了查询性能。并且， 在使用 AVL 树时，每个树节点仅存储一个数据，而每次进行磁盘 IO 时只能读取一个节点的数据，如果需要查询的数据分布在多个节点上，那么就需要进行多次磁盘 IO。 **磁盘 IO 是一项耗时的操作，在设计数据库索引时，我们需要优先考虑如何最大限度地减少磁盘 IO 操作的次数。**

实际应用中，AVL 树使用的并不多。

#### 红黑树

红黑树是一种自平衡二叉查找树，通过在插入和删除节点时进行颜色变换和旋转操作，使得树始终保持平衡状态，它具有以下特点：

1. 每个节点非红即黑；
2. 根节点总是黑色的；
3. 每个叶子节点都是黑色的空节点（NIL 节点）；
4. 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）；
5. 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）。

和 AVL 树不同的是，红黑树并不追求严格的平衡，而是大致的平衡。正因如此，红黑树的查询效率稍有下降，因为红黑树的平衡性相对较弱，可能会导致树的高度较高，这可能会导致一些数据需要进行多次磁盘 IO 操作才能查询到，这也是 MySQL 没有选择红黑树的主要原因。也正因如此，红黑树的插入和删除操作效率大大提高了，因为红黑树在插入和删除节点时只需进行 O(1) 次数的旋转和变色操作，即可保持基本平衡状态，而不需要像 AVL 树一样进行 O(logn) 次数的旋转操作。

**红黑树的应用还是比较广泛的，TreeMap、TreeSet 以及 JDK1.8 的 HashMap 底层都用到了红黑树。对于数据在内存中的这种情况来说，红黑树的表现是非常优异的。**

#### B 树& B+树

B 树也称 B-树,全称为 **多路平衡查找树** ，B+ 树是 B 树的一种变体。B 树和 B+树中的 B 是 `Balanced` （平衡）的意思。

目前大部分数据库系统及文件系统都采用 B-Tree 或其变种 B+Tree 作为索引结构。

##### **B 树& B+树两者有何异同呢？**

- B 树的所有节点既存放键(key) 也存放数据(data)，而 B+树只有叶子节点存放 key 和 data，其他内节点只存放 key。
- B 树的叶子节点都是独立的;B+树的叶子节点有一条引用链指向与它相邻的叶子节点。
- B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。
- 在 B 树中进行范围查询时，首先找到要查找的下限，然后对 B 树进行中序遍历，直到找到查找的上限；而 B+树的范围查询，只需要对链表进行遍历即可。

综上，B+树与 B 树相比，具备更少的 IO 次数、更稳定的查询效率和更适于范围查询这些优势。

#### 什么是B树？

为了解决降低树的高度的问题，后面就出来了 B 树，它不再限制一个节点就只能有 2 个子节点，而是允许 M 个子节点 (M>2)，从而降低树的高度。B 树的每一个节点最多可以包括 M 个子节点，M 称为 B 树的阶，所以 B 树就是一个多叉树。

- 但是 B 树的**每个节点都包含数据（索引+记录）**，而用户的记录数据的大小很有可能远远超过了索引数据，这就需要花费更多的磁盘 I/O 操作次数来读到「有用的索引数据」。
- 而且，在**查询**位于底层的某个节点（比如 A 记录）过程中，「非 A 记录节点」里的记录数据会从磁盘加载到内存，但是这些记录数据是没用的，我们只是想读取这些节点的索引数据来做比较查询，而「非 A 记录节点」里的记录数据对我们是没用的，这样不仅增多磁盘 I/O 操作次数，也占用内存资源。
- 另外，如果**使用 B 树来做范围查询**的话，需要使用中序遍历，这会涉及多个节点的磁盘 I/O 问题，从而导致整体速度下降。

#### B+树和B树的结构区别？

- 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
- 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；
- 非叶子节点的索引也会同时存在在子节点中，并且是在子节点中**所有索引的最大**（或最小）。
- 非叶子节点中有**多少个子节点**，就有**多少个索引**；

#### B+ 和 B 树的性能区别？

- B 树进行单个索引查询时，最快可以在 O(1) 的时间代价内就查到，而从平均时间代价来看，会比 B+ 树稍快一些。但是 B 树的查询波动会比较大，因为每个节点即存索引又存记录，所以有时候访问到了非叶子节点就可以找到索引，而有时需要访问到叶子节点才能找到索引；**B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少**
- B+ 树有**大量的冗余节点**，这样使得删除一个节点的时候，可以直接从叶子节点中删除，甚至可以不动非叶子节点，这样**删除非常快**；**B 树**则不同，B 树没有冗余节点，删除节点的时候非常复杂，比如删除根节点中的数据，可能涉及**复杂的树的变形** ；**B+ 树的插入**也是一样，有冗余节点，插入可能存在节点的分裂（如果节点饱和），但是最多**只涉及树的一条路径**。而且 B+ 树会自动平衡，不需要像更多复杂的算法，类似**红黑树的旋转操作**等。
- **B+ 树所有叶子节点间还有一个链表进行连接，这种设计对范围查找非常有帮助**，而 **B 树**没有将所有叶子节点用链表串联起来的结构，因此只能通过**树的遍历**来完成**范围查询**，这会涉及多个节点的磁盘 I/O 操作，**范围查询效率不如 B+ 树。**

- Innodb 使用的 B+ 树有一些特别的点，比如：
  - B+ 树的叶子节点之间是用**「双向链表」**进行连接，这样的好处是既能向右遍历，也能向**左遍历**。
  - **B+ 树点**节点内容是**数据页**，数据页里存放了用户的记录以及各种信息，每个数据页默认大小是 **16 KB**。

### MySQL中有四种索引类型，可以简单说说吗？

- **FULLTEXT** ：即为全文索引，目前只有MyISAM引擎支持。其可以在CREATE TABLE ，ALTER TABLE ，CREATE INDEX 使用，不过目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引，需要注意的是MySQL5.6以后支持全文索引了，5.6之前是不支持的。
- **HASH** ：由于HASH的唯一（几乎100%的唯一）及类似键值对的形式，很适合作为索引。 HASH索引可以一次定位，不需要像树形索引那样逐层查找,因此具有极高的效率。但是，这种高效是有条件的，即只在“=”和“in”条件下高效，对于范围查询、排序及组合索引仍然效率不高。
- **BTREE** ：BTREE索引就是一种将索引值按一定的算法，存入一个树形的数据结构中（二叉树），每次查询都是从树的入口root开始，依次遍历node，获取leaf。这是MySQL里默认和最常用的索引类型。
- **RTREE** ：RTREE在MySQL很少使用，仅支持geometry数据类型，支持该类型的存储引擎只有MyISAM、BDb、InnoDb、NDb、Archive几种。 相对于BTREE，RTREE的优势在于范围查找。

###  InnoDB 存储引擎选用 B+ 树而不是 B 树？

B+tree的磁盘读写代价更低，B+tree的查询效率更加稳定 数据库索引采用B+树而不是B树的主要原因：B+树只要遍历叶子节点就可以实现整棵树的遍历，而且在数据库中基于范围的查询是非常频繁的，而B树只能中序遍历所有节点，效率太低。

**B+树的特点**

- 所有关键字都出现在叶子结点的链表中(稠密索引)，且链表中的关键字恰好是有序的;
- 不可能在非叶子结点命中;
- 非叶子结点相当于是叶子结点的索引(稀疏索引)，叶子结点相当于是存储(关键字)数据的数据层

### 索引用B+树而不用hash表和B树？

- 利用Hash需要把数据全部**加载到内存中**，如果数据量大，是一件很**消耗内存**的事，而采用B+树，是基于**按照节点分段加载，由此减少内存消耗**。
- 和业务场景有段，**对于唯一查找**（查找一个值），Hash确实更快，**但数据库中经常查询多条数据**，这时候由于B+数据的有序性，与叶子节点又有链表相连，他的查询效率会比Hash快的多。
- b+树的**非叶子节点不保存数据**，**只保存子树的临界值**（最大或者最小），所以同样大小的节点，**b+树相对于b树能够有更多的分支，使得这棵树更加矮胖，查询时做的IO操作次数也更少**

### Hash比B+树更快，为什么MySQL用B+树来存储索引呢？

MySQL中存储索引用到的数据结构是B+树，B+树的查询时间跟树的高度有关，是log(n)，如果用hash存储，那么查询时间是O(1)。

采用Hash来存储确实要更快，但是采用B+树来存储索引的原因主要有以下两点：

一、**从内存角度上说**，数据库中的索引一般是在磁盘上，数据量大的情况可能无法一次性装入内存，B+树的设计可以允许数据分批加载。

二、**从业务场景上说**，如果只选择一个数据那确实是hash更快，但是数据库中经常会选中多条，这时候由于B+树索引有序，并且又有链表相连，它的查询效率比hash就快很多了

### MyISAM和InnoDB实现B树索引方式的区别是什么？

- MyISAM，B+Tree叶节点的data域存放的是数据记录的地址，在索引检索的时候，首先按照B+Tree搜索算法搜索索引，如果指定的key存在，则取出其data域的值，然后以data域的值为地址读取相应的数据记录，这被称为“非聚簇索引”

- InnoDB，其数据文件本身就是索引文件，相比MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按B+Tree组织的一个索引结构，树的节点data域保存了完整的数据记录，这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引，这被称为“聚簇索引”或者聚集索引，而其余的索引都作为辅助索引，辅助索引的data域存储相应记录主键的值而不是地址，这也是和MyISAM不同的地方。

  在根据主索引搜索时，直接找到key所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。因此，在设计表的时候，不建议使用过长的字段为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。

### 谈谈你对 B+ 树的理解？

1. B+ 树是基于 B 树和叶子节点顺序访问指针进行实现，它具有 B 树的平衡性，并且通过顺序访问指针来提高区间查询的性能。
2. 在 B+ 树中，一个节点中的 key 从左到右非递减排列，如果某个指针的左右相邻 key 分别是 key i 和 key i+1，且不为 null，则该指针指向节点的所有 key 大于等于 key i 且小于等于 key i+1。
3. 进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。
4. 插入、删除操作会破坏平衡树的平衡性，因此在插入删除操作之后，需要对树进行一个分裂、合并、旋转等操作来维护平衡性。

### MySQL中有哪些索引？有什么特点？

- **普通索引**：仅加速查询
- **唯一索引**：加速查询 + 列值唯一（可以有null）
- **主键索引**：加速查询 + 列值唯一（不可以有null）+ 表中只有一个
- **组合索引**：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并
- **全文索引**：对文本的内容进行分词，进行搜索
- **索引合并**：使用多个单列索引组合搜索
- **覆盖索引**：select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖
- **聚簇索引**：表数据是和主键一起存储的，主键索引的叶结点存储行数据(包含了主键值)，二级索引的叶结点存储行的主键值。使用的是B+树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录(数据内容)

### 索引类型总结

数据结构划分：

- BTree 索引：MySQL 里默认和最常用的索引类型。只有叶子节点存储 value，非叶子节点只有指针和 key。存储引擎 MyISAM 和 InnoDB 实现 BTree 索引都是使用 B+Tree，但二者实现方式不一样（前面已经介绍了）。
- 哈希索引：类似键值对的形式，一次即可定位。
- RTree 索引：一般不会使用，仅支持 geometry 数据类型，优势在于范围查找，效率较低，通常使用搜索引擎如 ElasticSearch 代替。
- 全文索引：对文本的内容进行分词，进行搜索。目前只有 `CHAR`、`VARCHAR` ，`TEXT` 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替。

按照底层存储方式角度划分：

- 聚簇索引（聚集索引）：索引结构和数据一起存放的索引，InnoDB 中的主键索引就属于聚簇索引。
- 非聚簇索引（非聚集索引）：索引结构和数据分开存放的索引，二级索引(辅助索引)就属于非聚簇索引。MySQL 的 MyISAM 引擎，不管主键还是非主键，使用的都是非聚簇索引。

按照应用维度划分：

- 主键索引：加速查询 + 列值唯一（不可以有 NULL）+ 表中只有一个。
- 普通索引：仅加速查询。
- 唯一索引：加速查询 + 列值唯一（可以有 NULL）。
- 覆盖索引：一个索引包含（或者说覆盖）所有需要查询的字段的值。
- 联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。
- 全文索引：对文本的内容进行分词，进行搜索。目前只有 `CHAR`、`VARCHAR` ，`TEXT` 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替。

MySQL 8.x 中实现的索引新特性：

- 隐藏索引：也称为不可见索引，不会被优化器使用，但是仍然需要维护，通常会软删除和灰度发布的场景中使用。主键不能设置为隐藏（包括显式设置或隐式设置）。
- 降序索引：之前的版本就支持通过 desc 来指定索引为降序，但实际上创建的仍然是常规的升序索引。直到 MySQL 8.x 版本才开始真正支持降序索引。另外，在 MySQL 8.x 版本中，不再对 GROUP BY 语句进行隐式排序。
- 函数索引：从 MySQL 8.0.13 版本开始支持在索引中使用函数或者表达式的值，也就是在索引中可以包含函数或者表达式。

#### 主键索引(Primary Key)

数据表的主键列使用的就是主键索引。

一张数据表有只能有一个主键，并且主键不能为 null，不能重复。

在 MySQL 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引且不允许存在 null 值的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键。

#### 二级索引

**二级索引（Secondary Index）又称为辅助索引，是因为二级索引的叶子节点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置。**

唯一索引，普通索引，前缀索引等索引属于二级索引。

1. **唯一索引(Unique Key)**：唯一索引也是一种约束。**唯一索引的属性列不能出现重复的数据，但是允许数据为 NULL，一张表允许创建多个唯一索引。** 建立唯一索引的目的大部分时候都是为了该属性列的数据的唯一性，而不是为了查询效率。
2. **普通索引(Index)**：**普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和 NULL。**
3. **前缀索引(Prefix)**：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小， 因为只取前几个字符。
4. **全文索引(Full Text)**：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。Mysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。

### 聚簇索引与非聚簇索引

#### 聚簇索引（聚集索引）

**聚簇索引（Clustered Index）即索引结构和数据一起存放的索引，并不是一种单独的索引类型。InnoDB 中的主键索引就属于聚簇索引。**

在 MySQL 中，InnoDB 引擎的表的 `.ibd`文件就包含了该表的索引和数据，对于 InnoDB 引擎表来说，该表的索引(B+树)的每个非叶子节点存储索引，叶子节点存储索引和索引对应的数据。

#### 聚簇索引的优缺点

**优点**：

- **查询速度非常快**：聚簇索引的查询速度非常的快，因为整个 B+树本身就是一颗多叉平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。相比于非聚簇索引， 聚簇索引少了一次读取数据的 IO 操作。
- **对排序查找和范围查找优化**：聚簇索引对于主键的排序查找和范围查找速度非常快。

**缺点**：

- **依赖于有序的数据**：因为 B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序，如果数据是整型还好，否则类似于字符串或 UUID 这种又长又难比较的数据，插入或查找的速度肯定比较慢。
- **更新代价大**：如果对索引列的数据被修改时，那么对应的索引也将会被修改，而且聚簇索引的叶子节点还存放着数据，修改代价肯定是较大的，所以对于主键索引来说，主键一般都是不可被修改的。

#### 非聚簇索引（非聚集索引）

#### 非聚簇索引介绍

**非聚簇索引(Non-Clustered Index)即索引结构和数据分开存放的索引，并不是一种单独的索引类型。二级索引(辅助索引)就属于非聚簇索引。MySQL 的 MyISAM 引擎，不管主键还是非主键，使用的都是非聚簇索引。**

非聚簇索引的叶子节点并不一定存放数据的指针，因为二级索引的叶子节点就存放的是主键，根据主键再回表查数据。

#### 非聚簇索引的优缺点

**优点**：

更新代价比聚簇索引要小 。非聚簇索引的更新代价就没有聚簇索引那么大了，非聚簇索引的叶子节点是不存放数据的

**缺点**：

- **依赖于有序的数据**：跟聚簇索引一样，非聚簇索引也依赖于有序的数据
- **可能会二次查询(回表)**：这应该是非聚簇索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询。

#### **聚簇索引和非聚簇索引的区别：**

聚集索引和非聚集索引的区别在于， 通过聚集索引可以查到需要查找的数据， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据。 聚集索引和非聚集索引的根本区别是表记录的排列顺序和与索引的排列顺序是否一致。

聚集索引（Innodb）的叶节点就是数据节点，而非聚集索引(MyisAM)的叶节点仍然是索引节点，只不过其包含一个指向对应数据块的指针。

###  覆盖索引和联合索引

#### 覆盖索引

如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为 **覆盖索引（Covering Index）** 。我们知道在 InnoDB 存储引擎中，如果不是主键索引，叶子节点存储的是主键+列值。最终还是要“回表”，也就是要通过主键再查找一次，这样就会比较慢。而覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！

**覆盖索引即需要查询的字段正好是索引的字段，那么直接根据该索引，就可以查到数据了，而无需回表查询。**

> 如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。

===================

如果一个索引包含了满足查询语句中字段与条件的数据就叫做覆盖索引。具有以下优点：

1. 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。
2. 一些存储引擎（例如：MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。
3. 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。

#### 联合索引

使用表中的多个字段创建索引，就是 **联合索引**，也叫 **组合索引** 或 **复合索引**。

以 `score` 和 `name` 两个字段建立联合索引：

```sql
ALTER TABLE `cus_order` ADD INDEX id_score_name(score, name);
```

#### 最左前缀匹配原则

最左前缀匹配原则指的是，在使用联合索引时，**MySQL** 会根据联合索引中的字段顺序，从左到右依次到查询条件中去匹配，如果查询条件中存在与联合索引中最左侧字段相匹配的字段，则就会使用该字段过滤一批数据，直至联合索引中全部字段匹配完成，或者在执行过程中遇到范围查询（如 **`>`**、**`<`**）才会停止匹配。对于 **`>=`**、**`<=`**、**`BETWEEN`**、**`like`** 前缀匹配的范围查询，并不会停止匹配。所以，我们在使用联合索引时，可以将区分度高的字段放在最左边，这也可以过滤更多数据。

=================

MySQL 使用联合索引时，需要满足最左前缀原则。下面举例对其进行说明：

```java
1. 一个 2 列的索引 (name, age)，对 (name)、(name, age) 上建立了索引；
2. 一个 3 列的索引 (name, age, sex)，对 (name)、(name, age)、(name, age, sex) 上建立了索引。
```

1、 B+ 树的数据项是复合的数据结构，比如：(name, age, sex) 的时候，B+ 树是按照从左到右的顺序来建立搜索树的，比如：当(小明, 22, 男)这样的数据来检索的时候，B+ 树会优先比较 name 来确定下一步的所搜方向，如果 name 相同再依次比较 age 和 sex，最后得到检索的数据。但当 (22, 男) 这样没有 name 的数据来的时候，B+ 树就不知道第一步该查哪个节点，因为建立搜索树的时候 name 就是第一个比较因子，必须要先根据 name 来搜索才能知道下一步去哪里查询。

2、 当 (小明, 男) 这样的数据来检索时，B+ 树可以用 name 来指定搜索方向，但下一个字段 age 的缺失，所以只能把名字等于小明的数据都找到，然后再匹配性别是男的数据了， 这个是非常重要的性质，即索引的最左匹配特性。

**关于最左前缀的补充：**

1. 最左前缀匹配原则会一直向右匹配直到遇到范围查询（>、<、between、like）就停止匹配，比如：a = 1 and b = 2 and c > 3 and d = 4 如果建立 (a, b, c, d) 顺序的索引，d 是用不到索引的。如果建立 (a, b, d, c) 的索引则都可以用到，a、b、d 的顺序可以任意调整。
2. = 和 in 可以乱序，比如：a = 1 and b = 2 and c = 3 建立 (a, b ,c) 索引可以任意顺序，MySQL 的优化器会优化成索引可以识别的形式。

### 主键索引的 B+Tree 和二级索引的 B+Tree 区别？

- 主键索引的 B+Tree 的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；

- 二级索引的 B+Tree 的叶子节点存放的是主键值，而不是实际数据。

- 先检二级索引中的 B+Tree 的索引值（商品编码，product_no），找到对应的叶子节点，然后获取主键值，然后再通过主键索引中的 B+Tree 树查询到对应的叶子节点，然后获取整行数据。**这个过程叫「回表」，也就是说要查两个 B+Tree 才能查到数据**。

- 当查询的数据是能在二级索引的 B+Tree 的叶子节点里查询到，这时就不用再查主键索引查。

- ```sql
  select id from product where product_no = '0002';
  ```

  **这种在二级索引的 B+Tree 就能查询到结果的过程就叫作「覆盖索引」，也就是只需要查一个 B+Tree 就能找到数据**。![回表](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/%E7%B4%A2%E5%BC%95/%E5%9B%9E%E8%A1%A8.drawio.png)

- ```sql
  select * from product where id= 5;主键查询
  select * from product where product_no = '0002';二级索引
  ```

### 索引下推

**索引下推（Index Condition Pushdown）** 是 **MySQL 5.6** 版本中提供的一项索引优化功能，可以在非聚簇索引遍历过程中，对索引中包含的字段先做判断，过滤掉不符合条件的记录，减少回表次数。

### 使用索引的注意事项

- 在经常需要搜索的列上，可以加快搜索的速度；
- 在经常使用在where子句中的列上面创建索引，加快条件的判断速度。
- **将打算加索引的列设置为NOT NULL，否则将导致引擎放弃使用索引而进行全表扫描**
- 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间
- 避免where子句中对字段施加函数，这会造成无法命中索引
- 在中到大型表索引都是非常有效的，但是特大型表的维护开销会很大，不适合建索引，建立用逻辑索引
- 在经常用到连续的列上，这些列主要是由一些外键，可以加快连接的速度
- 与业务无关时多使用逻辑主键，也就是自增主键在使用InnoDB时使用与业务无关的自增主键作为主键，即使用逻辑主键，而不要使用业务主键。
- 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗
- 在使用limit offset查询缓存时，可以借助索引来提高性能。

### 正确使用索引的一些建议

MySQL 索引通常是被用于提高 WHERE 条件的数据行匹配时的搜索速度，在索引的使用过程中，存在一些使用细节和注意事项。

函数，运算，否定操作符，连接条件，多个单列索引，最左前缀原则，范围查询，不会包含有NULL值的列，like 语句不要在列上使用函数和进行运算

#### **不要在列上使用函数，这将导致索引失效而进行全表扫描。**

为了使用索引，防止执行全表扫描，可以进行改造。不要在列上进行运算，这也将导致索引失效而进行全表扫描。

#### **尽量避免使用 != 或 not in或 <> 等否定操作符**

应该尽量避免在 where 子句中使用 != 或 not in 或 <> 操作符，因为这几个操作符都会导致索引失效而进行全表扫描。尽量避免使用 or 来连接条件 应该尽量避免在 where 子句中使用 or 来连接条件，因为这会导致索引失效而进行全表扫描。

#### **复合索引的最左前缀原则**

复合索引遵守“最左前缀”原则，即在查询条件中使用了复合索引的第一个字段，索引才会被使用。因此，在复合索引中索引列的顺序至关重要。如果不是按照索引的最左列开始查找，则无法使用索引。 假设，有一个场景只需要针对资讯的月份进行查询，那么，SQL 语句可以写成：

```text
select * from news where news_month = 1
    
```

此时，无法使用 news_year_month_idx(news_year, news_month) 索引，因为遵守“最左前缀”原则，在查询条件中没有使用复合索引的第一个字段，索引是不会被使用的。

#### **多个单列索引并不是最佳选择**

MySQL 只能使用一个索引，会从多个索引中选择一个限制最为严格的索引，因此，为多个列创建单列索引，并不能提高 MySQL 的查询性能。 假设，有两个单列索引，分别为 news_year_idx(news_year) 和 news_month_idx(news_month)。现在，有一个场景需要针对资讯的年份和月份进行查询，那么，SQL 语句可以写成：

#### 选择合适的字段创建索引

- **不为 NULL 的字段**：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。
- **被频繁查询的字段**：我们创建索引的字段应该是查询操作非常频繁的字段。
- **被作为条件查询的字段**：被作为 WHERE 条件查询的字段，应该被考虑建立索引。
- **频繁需要排序的字段**：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。
- **被经常频繁用于连接的字段**：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率。

#### 被频繁更新的字段应该慎重建立索引

虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。

#### 限制每张表上的索引数量

索引并不是越多越好，建议单张表索引不超过 5 个！索引可以提高效率同样可以降低效率。

索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。

因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。

#### 尽可能的考虑建立联合索引而不是单列索引

因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。

#### 注意避免冗余索引

冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引。如（name,city ）和（name ）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。

#### 字符串类型的字段使用前缀索引代替普通索引

前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。

#### 避免索引失效

索引失效也是慢查询的主要原因之一，常见的导致索引失效的情况有下面这些：

- 使用 `SELECT *` 进行查询; `SELECT *` 不会直接导致索引失效（如果不走索引大概率是因为 where 查询范围过大导致的），但它可能会带来一些其他的性能问题比如造成网络传输和数据处理的浪费、无法使用索引覆盖;
- 创建了组合索引，但查询条件未遵守最左匹配原则;
- 在索引列上进行计算、函数、类型转换等操作;
- 以 `%` 开头的 LIKE 查询比如 `like '%abc'`;
- 查询条件中使用 or，且 or 的前后条件中有一个列没有索引，涉及的索引都不会被使用到;
- 发生[隐式转换]();
- ......

#### 删除长期未使用的索引

删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗。

MySQL 5.7 可以通过查询 `sys` 库的 `schema_unused_indexes` 视图来查询哪些索引从未被使用。

#### 知道如何分析语句是否走索引查询

我们可以使用 `EXPLAIN` 命令来分析 SQL 的 **执行计划** ，这样就知道语句是否命中索引了。执行计划是指一条 SQL 语句在经过 MySQL 查询优化器的优化会后，具体的执行方式。

`EXPLAIN` 并不会真的去执行相关的语句，而是通过 **查询优化器** 对语句进行分析，找出最优的查询方案，并显示对应的信息。

`EXPLAIN` 的输出格式如下：

```sql
mysql> EXPLAIN SELECT `score`,`name` FROM `cus_order` ORDER BY `score` DESC;
+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+
| id | select_type | table     | partitions | type | possible_keys | key  | key_len | ref  | rows   | filtered | Extra          |
+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+
|  1 | SIMPLE      | cus_order | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 997572 |   100.00 | Using filesort |
+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+
1 row in set, 1 warning (0.00 sec)
```

各个字段的含义如下：

| **列名**      | **含义**                                     |
| ------------- | -------------------------------------------- |
| id            | SELECT 查询的序列标识符                      |
| select_type   | SELECT 关键字对应的查询类型                  |
| table         | 用到的表名                                   |
| partitions    | 匹配的分区，对于未分区的表，值为 NULL        |
| type          | 表的访问方法                                 |
| possible_keys | 可能用到的索引                               |
| key           | 实际用到的索引                               |
| key_len       | 所选索引的长度                               |
| ref           | 当使用索引等值查询时，与索引作比较的列或常量 |
| rows          | 预计要读取的行数                             |
| filtered      | 按表条件过滤后，留存的记录数的百分比         |
| Extra         | 附加信息                                     |

### **like 语句的索引失效问题**

like 的方式进行查询，在 like “value%” 可以使用索引，但是对于 like “%value%” 这样的方式，执行全表查询，这在数据量小的表，不存在性能问题，但是对于海量数据，全表扫描是非常可怕的事情。所以，根据业务需求，考虑使用 ElasticSearch 或 Solr 是个不错的方案。

### 既然索引有那么多优点，为什么不对表总的每一列创建一个索引呢？

- 当对表中的数据进行增加、删除和修改的时候，**索引也要动态的维护**，这样就降低了数据的维护速度。
- **索引需要占物理空间**，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立簇索引，那么需要的空间就会更大。
- **创建索引和维护索引要耗费时间**，这种时间随着数据量的增加而增加

### 添加索引的原则

索引虽好，但也不是无限制使用的，以下为添加索引时需要遵循的几项建议性原则：

- 在 查询中很少使用 或者参考的列不要创建索引。由于这些列很少使用到，增加索引反而会降低系统的维护速度和增大空间需求。
- 只有很少数据值的列 也不应该增加索引。由于这些列的取值很少，区分度太低，例如人事表中的性别，在查询时，需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。
- 定义为 text、image 和 bit 数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。
- 当 修改性能远远大于检索性能 时，不应该创建索引。这时因为，二者是相互矛盾的，当增加索引时，会提高检索性能，但是会降低修改性能。
- 定义有 外键 的数据列一定要创建索引。

### 谈谈你对哈希索引的理解？

哈希索引能以 O(1) 时间进行查找，但是失去了有序性。无法用于排序与分组、只支持精确查找，无法用于部分查找和范围查找。

InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+ 树索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如：快速的哈希查找。

### MySQL索引主要使用的两种数据结构是什么？

- **哈希索引**，对于哈希索引来说，底层的数据结构肯定是哈希表，因此**在绝大多数需求为单条记录查询**的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择BTree索引

- **BTree索引**，Mysql的BTree索引使用的是B树中的B+Tree，BTREE索引就是一种将索引值按一定的算法，存入一个树形的数据结构中（二叉树），每次查询都是从树的入口root开始，依次遍历node，获取leaf。

  但对于主要的两种存储引擎（MyISAM和InnoDB）的实现方式是不同的。

### 怎么知道创建的索引有没有被使用到？或者说怎么才可以知道这条语句运行很慢的原因?

使用 Explain 命令来查看语句的执行计划，MySQL 在执行某个语句之前，会将该语句过一遍查询优化器，之后会拿到对语句的分析，也就是执行计划，其中包含了许多信息。可以通过其中和索引有关的信息来分析是否命中了索引，例如：possilbe_key、key、key_len 等字段，分别说明了此语句可能会使用的索引、实际使用的索引以及使用的索引长度。

### 什么情况下索引会失效？即查询不走索引？

下面列举几种不走索引的 SQL 语句：

1、索引列参与表达式计算：

```mysql
SELECT 'sname' FROM 'stu' WHERE 'age' + 10 = 30;
```

2、 函数运算：

```mysql
SELECT 'sname' FROM 'stu' WHERE LEFT('date',4) < 1990; 
```

3、%词语%–模糊查询：

```mysql
SELECT * FROM 'manong' WHERE `uname` LIKE '码农%' -- 走索引 

SELECT * FROM 'manong' WHERE `uname` LIKE '%码农%' -- 不走索引 
```

4、 字符串与数字比较不走索引：

“`mysql
CREATE TABLE 'a' ('a' char(10));
EXPLAIN SELECT * FROM 'a' WHERE 'a'="1" — 走索引
EXPLAIN SELECT * FROM 'a'WHERE 'a'=1 — 不走索引，同样也是使用了函数运算
“`

5、 查询条件中有 or ，即使其中有条件带索引也不会使用。换言之，就是要求使用的所有字段，都必须建立索引：

```mysql
select * from dept where dname='xxx' or loc='xx' or deptno = 45;
```

6、正则表达式不使用索引。

7、 MySQL 内部优化器会对 SQL 语句进行优化，如果优化器估计使用全表扫描要比使用索引快，则不使用索引。

### 增加B+树的路数可以降低树的高度，那么无限增加树的路数是不是可以有最优的查找效率？

不可以。因为这样会形成一个有序数组，文件系统和数据库的索引都是存在硬盘上的，并且如果数据量大的话，不一定能一次性加载到内存中。有序数组没法一次性加载进内存，这时候B+树的多路存储威力就出来了，可以每次加载B+树的一个结点，然后一步步往下找，

### 查询性能的优化方法？

**减少请求的数据量**

1. 只返回必要的列：最好不要使用 SELECT * 语句。
2. 只返回必要的行：使用 LIMIT 语句来限制返回的数据。
3. 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。

**减少服务器端扫描的行数**

1. 最有效的方式是使用索引来覆盖查询。

### 索引如何提高查询速度的

将无序的数据变成相对有序的数据

### 主从同步的延迟原因及解决办法？

**主从同步的延迟的原因：**

假如一个服务器开放 Ｎ 个连接给客户端，这样有会有大并发的更新操作, 但是从服务器的里面读取 binlog 的线程仅有一个， 当某个 SQL 在从服务器上执行的时间稍长或者由于某个 SQL 要进行锁表就会导致主服务器的 SQL 大量积压，未被同步到从服务器里。这就导致了主从不一致， 也就是主从延迟。

**主从同步延迟的解决办法：**

实际上主从同步延迟根本没有什么一招制敌的办法， 因为所有的 SQL 必须都要在从服务器里面执行一遍，但是主服务器如果不断的有更新操作源源不断的写入，那么一旦有延迟产生，那么延迟加重的可能性就会原来越大。当然我们可以做一些缓解的措施。

1. 我们知道因为主服务器要负责更新操作， 它对安全性的要求比从服务器高，所有有些设置可以修改，比如sync_binlog=1，innodb_flush_log_at_trx_commit = 1 之类的设置，而 slave 则不需要这么高的数据安全，完全可以将 sync_binlog 设置为 0 或者关闭 binlog、innodb_flushlog、innodb_flush_log_at_trx_commit 也 可以设置为 0 来提高 SQL 的执行效率。
2. 增加从服务器，这个目的还是分散读的压力， 从而降低服务器负载。



### count(*) 和 count(1) 有什么区别？哪个性能最好？

- count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是**统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个**。

  - 假设 count() 函数的参数是字段名，如下：

    ```sql
    select count(name) from t_order;
    ```

    这条语句是统计「 t_order 表中，name 字段不为 NULL 的记录」有多少个。也就是说，**如果某一条记录中的 name 字段的值为 NULL，则就不会被统计进去**。

  - 假设 count() 函数的参数是数字 1 这个表达式，如下：

    ```sql
    select count(1) from t_order;
    ```

    这条语句是统计「 t_order 表中，1 这个表达式不为 NULL 的记录」有多少个。1 这个表达式就是单纯数字，它永远都不是 NULL，所以上面这条语句，其实是**在统计 t_order 表中有多少个记录**。

#### count(主键字段) 执行过程是怎样的？

- 在**通过 count 函数统计有多少个记录**时，MySQL 的 server 层会维护一个名叫 count 的变量。server 层会循环向 InnoDB存储引擎 读取一条记录，如果 count 函数指定的参数不为 NULL，那么就会将变量 count 加 1，直到符合查询的全部记录被读完，就退出循环。最后将 count 变量的值发送给客户端。
- InnoDB 是通过 B+ 树来保存记录的，根据索引的类型又分为聚簇索引和二级索引；
  - 如果表里只有主键索引，没有二级索引时，那么，InnoDB 循环遍历**聚簇索引**，将读取到的记录返回给 server 层，然后读取记录中的 id 值，就会 id 值判断是否为 NULL，如果不为 NULL，就将 count 变量加 1。
  - 如果**表里有二级索引时**，InnoDB 循环遍历的对象就不是聚簇索引，而是二级索引。相同数量的二级索引记录可以比聚簇索引记录占用更少的存储空间，所以**二级索引树比聚簇索引树小**，这样遍历二级索引的 I/O 成本比遍历聚簇索引的 **I/O 成本小，因此「优化器」优先选择的是二级索引**

#### count(1) 执行过程是怎样的？

- 如果表里只有主键索引，没有二级索引时。InnoDB 循环遍历聚簇索引（主键索引），将读取到的记录返回给 server 层，**但是不会读取记录中的任何字段的值**，因为 count 函数的**参数是 1，不是字段**，所以不需要读取记录中的字段值。参数 1 很明显并不是 NULL，因此 server 层每从 InnoDB 读取**到一条记录，就将 count 变量加 1**。
- count(1) 相比 count(主键字段) 少一个步骤，就是不需要读取记录中的字段值，所以通常会说 **count(1) 执行效率会比 count(主键字段) 高一点**。
- 如果表里有二级索引时，InnoDB 循环遍历的对象就二级索引

#### count(*) 执行过程是怎样的？

- 看到 `*` 这个字符的时候，是不是大家觉得是读取记录中的所有字段值？对于 `selete *` 这条语句来说是这个意思，但是在 count(*) 中并不是这个意思。
- **count(`*`) 其实等于 count(`0`)**，也就是说，当你使用 count(`*`) 时，MySQL 会将 `*` 参数转化为参数 0 来处理。所以，**count(\*) 执行过程跟 count(1) 执行过程基本一样的**，性能没有什么差异。
- 而且 MySQL 会对 count(*) 和 count(1) 有个**优化**，如果有多个二级索引的时候，**优化器会使用key_len 最小的二级索引进行扫描**。只有当没有二级索引的时候，才会采用主键索引来进行统计。

#### 如何优化 count(*)？

- 如果你的业务对于**统计个数不需要很精确**，比如搜索引擎在搜索关键词的时候，给出的搜索结果条数是一个大概值。可以使用 **show table status 或者 explain 命令**来表进行**估算**。执行 explain 命令效率是很高的，因为它并不会真正的去查询，下图中的 **rows 字段值**就是 explain 命令对表 t_order 记录的估算值。
- 如果是**想精确的获取表的记录总数**，我们可以将这个**计数值**保存到**单独的一张计数表**中。当我们在数据表插入一条记录的同时，将计数表中的计数字段 + 1。也就是说，在新 增和删除操作时，我们需要额外维护这个计数表。

#### count(普通字段) 执行过程是怎样的？

- count(字段) 的执行效率相比前面的 count(1)、 count(*)、 count(主键字段) **执行效率是最差**的。

- ```sql
  // name不是索引，普通字段
  select count(name) from t_order;
  ```

  对于这个查询来说，会采用**全表扫描**的方式来计数，所以它的执行效率是比较差的。

- 如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在**数据表上建立二级索引**，这样优化器会自动采用 **key_len 最小**的二级索引进行扫描，相比于**扫描主键索引**效率会高一些。再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用**全表扫描的方式来统计**。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引

#### 为什么 count 函数需要通过遍历的方式来统计记录个数？

- 在 **MyISAM 存储引擎**里，执行 count 函数的方式是不一样的，通常在**没有**任何**查询条件**下的 count(*)，**MyISAM 的查询速度要明显快于 InnoDB**。使用 MyISAM 引擎时，执行 count 函数只需要 O(1 )复杂度，这是因为每张 MyISAM 的数据表都有一个 **meta 信息有存储了row_count值**，由**表级锁保证一致性**，所以直接读取 row_count 值就是 count 函数的执行结果。
- 而 **InnoDB 存储引擎是支持事务**的，同一个时刻的多个查询，由于**多版本并发控制（MVCC）**的原因，InnoDB 表“应该返回多少行”也是不确定的，所以无法像 MyISAM一样，只维护一个 row_count 变量。
- 当带上 **where 条件语句**之后，MyISAM 跟 InnoDB 就没有区别了，它们都需要**扫描表**来进行记录个数的统计

# ==========================

## Mysql事务=======

### 何谓数据库事务？

大多数情况下，我们在谈论事务的时候，如果没有特指**分布式事务**，往往指的就是**数据库事务**。

数据库事务在我们日常开发中接触的最多了。如果你的项目属于单体架构的话，你接触到的往往就是数据库事务了。

**那数据库事务有什么作用呢？**

简单来说，数据库事务可以保证多个对数据库的操作（也就是 SQL 语句）构成一个逻辑上的整体。构成这个逻辑上的整体的这些数据库操作遵循：**要么全部执行成功,要么全部不执行** 。

### 有哪些事务状态

- 活跃状态：事务的第一个状态，任何正在执行的事务都处于此状态，所做的 更改 存储在 主内存的缓冲区 中。
- 部分提交状态：执行上次操作后，事务进入部分提交状态。之所以是部分提交，是因为所做的更改仍然在主内存的缓冲区中。
- 失败状态：如果某个检查在活动状态下失败，在活动状态或部分提交状态发生一些错误，并且事务无法进一步执行，则事务进入失败状态。
- 中止状态：如果任何事务已达到失败状态，则恢复管理器将数据库回滚到开始执行的原始状态。
- 提交状态：如果所有操作成功执行，则来自 部分提交状态 的事务进入提交状态。无法从此状态回滚，它是一个新的 一致状态。

### 事务的特性ACID？

1. 原子性：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；
2. 一致性：执行事务前后，数据库从一个一致性状态转换到另一个一致性状态。
3. 隔离性：并发访问数据库时，一个用户的事物不被其他事务所干扰，各并发事务之间数据库是独立的；
4. 持久性：一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库 发生故障也不应该对其有任何影响。

#### 四大特性在得不到保障的情况下会怎样？

**如果无法保证原子性会怎么样？**

OK，就会出现数据不一致的情形，A账户减去50元，而B账户增加50元操作失败。系统将无故丢失50元~

**如果无法保证一致性会怎么样？**

- 例一:A账户有200元，转账300元出去，此时A账户余额为-100元。你自然就发现了此时数据是不一致的，为什么呢？因为你定义了一个状态，余额这列必须大于0。
- 例二:A账户200元，转账50元给B账户，A账户的钱扣了，但是B账户因为各种意外，余额并没有增加。你也知道此时数据是不一致的，为什么呢？因为你定义了一个状态，要求A+B的余额必须不变。

**如果无法保证持久性会怎么样？**

在MySQL中，为了解决CPU和磁盘速度不一致问题，MySQL是将磁盘上的数据加载到内存，对内存进行操作，然后再回写磁盘。好，假设此时宕机了，在内存中修改的数据全部丢失了，持久性就无法保证。

设想一下，系统提示你转账成功。但是你发现金额没有发生任何改变，此时数据出现了不合法的数据状态，我们将这种状态认为是**数据不一致**的情形。

**如果无法保证隔离性会怎么样**？

假设A账户有200元，B账户0元。A账户往B账户转账两次，金额为50元，分别在两个事务中执行。如果无法保证隔离性，A可能就会出现扣款两次的情形，而B只加款一次，凭空消失了50元，依然出现了数据不一致的情形！

#### 如何保证事务的四个特性？

- 持久性是通过 redo log （重做日志）来保证的；
- 原子性是通过 undo log（回滚日志） 来保证的；
- 隔离性是通过 **MVCC（多版本并发控制） 或锁机制**来保证的；
- 一致性则是通过持久性+原子性+隔离性来保证；

#### 数据库如何保证一致性？

分为两个层面来说。

- **从数据库层面**，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。**数据库必须要实现AID三大特性，才有可能实现一致性**。例如，原子性无法保证，显然一致性也无法保证。
- **从应用层面**，通过代码判断数据库数据是否有效，然后决定回滚还是提交数据！

#### 数据库如何保证原子性？

主要是利用 Innodb 的**undo log**。 **undo log**名为回滚日志，是实现原子性的关键，当事务回滚时能够撤销所有已经成功执行的 SQL语句，他需要记录你要回滚的相应日志信息。 例如

- 当你delete一条数据的时候，就需要记录这条数据的信息，回滚的时候，insert这条旧数据
- 当你update一条数据的时候，就需要记录之前的旧值，回滚的时候，根据旧值执行update操作
- 当年insert一条数据的时候，就需要这条记录的主键，回滚的时候，根据主键执行delete操作

**undo log**记录了这些回滚需要的信息，当事务执行失败或调用了**rollback**，导致事务需要回滚，便可以利用**undo log**中的信息将数据回滚到修改之前的样子。

#### 数据库如何保证持久性？

主要是利用Innodb的**redo log**。重写日志， 正如之前说的，MySQL是先把磁盘上的数据加载到内存中，在内存中对数据进行修改，再写回到磁盘上。如果此时突然宕机，内存中的数据就会丢失。 怎么解决这个问题？ 简单啊，事务提交前直接把数据写入磁盘就行啊。 这么做有什么问题？

- 只修改一个页面里的一个字节，就要将整个页面刷入磁盘，太浪费资源了。毕竟一个页面16kb大小，你只改其中一点点东西，就要将16kb的内容刷入磁盘，听着也不合理。
- 毕竟一个事务里的SQL可能牵涉到多个数据页的修改，而这些数据页可能不是相邻的，也就是属于随机IO。显然操作随机IO，速度会比较慢。

于是，决定采用**redo log**解决上面的问题。当做数据修改的时候，不仅在内存中操作，还会在**redo log**中记录这次操作。当事务提交的时候，会将**redo log**日志进行刷盘(**redo log**一部分在内存中，一部分在磁盘上)。当数据库宕机重启的时候，会将redo log中的内容恢复到数据库中，再根据**undo log**和**binlog**内容决定回滚数据还是提交数据。

**采用redo log的好处？**

其实好处就是将**redo log**进行刷盘比对数据页刷盘效率高，具体表现如下：

- **redo log**体积小，毕竟只记录了哪一页修改了啥，因此体积小，刷盘快。
- **redo log**是一直往末尾进行追加，属于顺序IO。效率显然比随机IO来的快。

### 事务隔离级别？

1. READ_UNCOMMITTED（未提交读）: 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读；
2. READ_COMMITTED（提交读）: 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生；
3. REPEATABLE_READ（可重复读）: 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生；
4. SERIALIZABLE（串行化）: 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。

### 并发事务问题：脏读、不可重复读和幻读？

- **脏读：**

表示一个事务能够读取另一个事务中还未提交的数据。比如：某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。

- **不可重复读 ：**

是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两 次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不 可重复读。例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果 只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题

**不可重复读的重点是修改** :同样的条件 , 你读取过的数据 , 再次读取出来发现值不一样了

- **幻读：**

指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。

**幻读的重点在于新增或者删除**：同样的条件 , 第 1 次和第 2 次读出来的记录数不一样

### 不可重复读和幻读有什么区别？

**不可重复读的重点是修改，幻读的重点在于新增或者删除。**

- 不可重复读的重点是内容修改或者记录减少比如多次读取一条记录发现其中某些记录的值被修改；
- 幻读的重点在于记录新增比如多次执行同一条查询语句（DQL）时，发现查到的记录增加了。

幻读其实可以看作是不可重复读的一种特殊情况，单独把区分幻读的原因主要是解决幻读和不可重复读的方案不一样。

举个例子：执行 `delete` 和 `update` 操作的时候，可以直接对记录加锁，保证事务安全。而执行 `insert` 操作的时候，由于记录锁（Record Lock）只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁（Gap Lock）。也就是说执行 `insert` 操作的时候需要依赖 Next-Key Lock（Record Lock+Gap Lock） 进行加锁来保证不出现幻读。

### MySQL  记录锁+间隙锁可以防止删除操作而导致的幻读吗？

- 幻读的定义：当同一个**查询**在不同的时间产生不同的结果集时，事务中就会出现所谓的幻象问题；

- MySQL 可重复读隔离级别可以解决幻读问题，查询数据的操作有两种方式，所以解决的方式是不同的：

  - 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
  - 针对**当前读**（select ... for update 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题

- 通过 `select * from performance_schema.data_locks\G;` 查看事务执行 SQL 过程中加了什么锁：

  - 表锁（`LOCK_TYPE: TABLE`）：X 类型的意向锁；
  - 行锁（`LOCK_TYPE: RECORD`）：X 类型的 next-key 锁
    - 如果 LOCK_MODE 为 `X`，说明是 next-key 锁；
    - 如果 LOCK_MODE 为 `X, REC_NOT_GAP`，说明是记录锁；
    - 如果 LOCK_MODE 为 `X, GAP`，说明是间隙锁；
  - 如果 LOCK_MODE 是 next-key 锁或者间隙锁，那么 **LOCK_DATA 就表示锁的范围最右值**，而锁范围的最左值为 LOCK_DATA 的上一条记录的值

- 实验分析，在可重复读隔离级别下，针对无索引的查询语句，**相当于把整个表给锁住了，其他事务在对该表进行增、删、改操作的时候都会被阻塞**；因为这条查询语句会进行**全表扫描，锁是在遍历索引的时候加上的，并不是针对输出的结果加锁**；因此，在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁

- 如果对查找字段建立了索引，则**因为表中有两个索引，分别是主键索引和 age 索引，所以会分别对这两个索引加锁**；查询语句是索引查询，并不会全表扫描，因此**不会把整张表给锁住**

  <img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZfZJqiauZPNWqz1jmVXOQ5lDnpSiaM6mgX4gBFbg5wQvn7uicXuh2AqDyI4Z2D51F42p3nXhS3bRyQgQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom: 33%;" />

###  解决幻读的方法

解决幻读的方式有很多，但是它们的核心思想就是一个事务在操作某张表数据的时候，另外一个事务不允许新增或者删除这张表中的数据了。解决幻读的方式主要有以下几种：

1. 将事务隔离级别调整为 `SERIALIZABLE` 。
2. 在可重复读的事务级别下，给事务操作的这张表添加表锁。
3. 在可重复读的事务级别下，给事务操作的这张表添加 `Next-key Lock（Record Lock+Gap Lock）`。

#### MVCC➕Next-key-Lock 防止幻读

`InnoDB`存储引擎在 RR 级别下通过 `MVCC`和 `Next-key Lock` 来解决幻读问题：

**1、执行普通 `select`，此时会以 `MVCC` 快照读的方式读取数据**

在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 `Read View` ，并使用至事务提交。所以在生成 `Read View` 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”

**2、执行 select...for update/lock in share mode、insert、update、delete 等当前读**

在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！`InnoDB` 使用 [Next-key Lockopen in new window](https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html#innodb-next-key-locks) 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读

#### MySQL 可重复读RR隔离级别，完全解决幻读了吗？

##### 快照读是如何避免幻读的？

- 可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是启动事务后，在执行第一个查询语句后，会创建一个 Read View读视图，**后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的**，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。

##### 当前读是如何避免幻读？

- Innodb 引擎为了解决**「可重复读」隔离级别**使用「当前读」而造成的幻读问题，就引出了**间隙锁**。

  - 事务 B 在执行插入语句的时候，判断到插入的位置被事务 A （当前读语句）加了 **next-key lock**，于是**事务 B** 会生成一个**插入意向锁**，同时进入**等待状态**，直到事务 A 提交了事务。这就避免了由于事务 B 插入新记录而导致事务 A 发生幻读的现象

  

  - **针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读。**
  - **针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。**

##### 幻读被完全解决了吗？

可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。

- 对于快照读， MVCC 并不能完全避免幻读现象。因为**当事务 A 更新了一条事务 B 插入的记录**，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。
- 对于当前读，如果事务开启后，并没有执行当前读，而是**先快照读，**然后这期间如果**其他事务插入了一条记录**，那么事务后续使用**当前读**进行**查询**的时候，就会发现**两次查询的记录条目**就不一样了，所以就发生幻读。要避免这类特殊场景下发生幻读的现象的话，就是尽量在**开启事务之后**，**马上执行** select ... for update 这类**当前读**的语句，因为它会对**记录加 next-key lock**，从而避免其他事务插入一条新记录。

### 什么是快照读、当前读？

- **快照读**（**普通 select** 语句），是**通过 MVCC 方式解决了幻读**，因为**可重复读****RR**隔离级别下，事务执行过程中看到的数据，一直跟这个**事务启动时看到的数据是一致**的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。
- **当前读**（**select ... for update** 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 **next-key lock**，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。
- **快照读的补充**：像不加锁的select操作就是**快照读**，即**不加锁的非阻塞读**；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读，快照读的实现是基于多版本并发控制MVCC，提高并发能,MVCC是行锁的一个变种，避免了加锁操作，降低了开销；既然是基于多版本，即**快照读**可能读到的并不一定是数据的最新版本，而有可能是**之前的历史版本**

### 四种隔离级别具体是如何实现的，尤其是RR、RC？

- 对于「**读未提交**」隔离级别的事务来说，因为可以读到未提交事务**修改的数据**，所以**直接读取最新的数据**就好了；
- 对于「**串行化**」隔离级别的事务来说，通过**加读写锁的方式来避免并行访问**；
- 对于「**读提交RC**」和「**可重复读RR**」隔离级别的事务来说，它们是通过 **Read View读视图 **来实现的，它们的区别在于**创建 Read View 的时机不同**，大家可以把 Read View 理解成一个**数据快照**，就像相机拍照那样，定格某一时刻的风景。
- 「**读提交**」隔离级别是在**「每个语句执行前」**都会重新生成一个 Read View，而「**可重复读**」隔离级别是「**启动事务时」生成一个 Read View**，然后**整个事务期间**都在用这个 Read View。

#### RR、RC隔离级别的事务怎么实现的？

对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同：

- 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
- 「可重复读」隔离级别是启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是事务启动前的记录。

这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列」的比对，来控制并发事务访问同一个记录时的行为，这就叫 MVCC（多版本并发控制）。

#### 两种开启事务的命令说说？

执行「开始事务」命令，并不意味着启动了事务。在 MySQL 有两种开启事务的命令，分别是：

- 第一种：begin/start transaction 命令；
- 第二种：start transaction with consistent snapshot 命令；

这两种开启事务的命令，**事务的启动时机是不同**的：

- 执行了 begin/start transaction 命令后，并**不代表事务启动**了。只有在执行这个命令后，**执行了增删查改操作的 SQL 语句**，才是**事务真正启动**的时机；
- 执行了 start transaction with consistent snapshot 命令，就会**马上启动事务**。

#### Read View 读视图说说？

- Read View 有四个重要的字段：

  - m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的**事务 id 列表**，注意是一个列表，**“活跃事务”指的就是，启动了但还没提交的事务**。
  - min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 **id 最小的事务**，也就是 m_ids 的最小值。
  - max_trx_id ：这个并不是 m_ids 的最大值，而是**创建 Read View 时当前数据库中应该给下一个事务的 id 值**，也就是全局事务中**最大的事务 id 值 + 1**；
  - creator_trx_id ：指的是**创建该 Read View 的事务的事务 id**

  对于使用 InnoDB 存储引擎的数据库表，它的**聚簇索引记录**中都包含下面**两个隐藏列**：

  - trx_id，当一个事务对某条聚簇索引**记录进行改动**时，就会**把该事务的事务 id 记录在 trx_id 隐藏列里**；
  - roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后**这个隐藏列是个指针，指向每一个旧版本记录**，于是就可以通过它找到修改前的记录。

- 在创建 Read View 后，可以将记录中的 trx_id 划分这三种情况：

  <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/mysql/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/ReadView.drawio.png" alt="img" style="zoom:50%;" />

#### Read View读视图在MVCC里如何工作的？

- **一个事务去访问记录**的时候，除了事务自己的**更新记录**总是可见之外，还有这几种情况：

  - 如果**记录**的 trx_id 值小于 Read View 中的 `min_trx_id` 值，表示这个版本的记录是在创建 Read View **前**已经提交的事务生成的，所以该版本的记录对当前事务**可见**。

  - 如果**记录**的 trx_id 值大于等于 Read View 中的 `max_trx_id` 值，表示这个版本的**记录**是在创建 Read View **后**才启动的事务生成的，所以该版本的记录对当前事务**不可见**。

  - 如果记录的 trx_id 值在 Read View 的min_trx_id和max_trx_id之间，需要判断 trx_id 是否在 m_ids 列表中：
    - 如果记录的 trx_id 在 `m_ids` 列表中，表示生成该版本记录的**活跃事务依然活跃着（还没提交事务）**，所以该版本的记录对当前事务**不可见**。
    - 如果记录的 trx_id **不在** `m_ids`列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务**可见**。

  **这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。**

### MySQL 默认的隔离级别是什么？

MySQL默认采用的 REPEATABLE_READ隔离级别。

Oracle 默认采用的 READ_COMMITTED 隔离级别。

### 并发事务的控制方式有哪些？

MySQL 中并发事务的控制方式无非就两种：**锁** 和 **MVCC**。锁可以看作是悲观控制的模式，多版本并发控制（MVCC）可以看作是**乐观控制**的模式。

**锁** 控制方式下会通过锁来显示控制共享资源而不是通过调度手段，MySQL 中主要是通过 **读写锁** 来实现并发控制。

- **共享锁（S 锁）**：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。
- **排他锁（X 锁）**：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条记录加任何类型的锁（锁不兼容）。

读写锁可以做到读读并行，但是无法做到写读、写写并行。另外，根据根据锁粒度的不同，又被分为 **表级锁(table-level locking)** 和 **行级锁(row-level locking)** 。InnoDB 不光支持表级锁，还支持行级锁，默认为行级锁。行级锁的粒度更小，仅对相关的记录上锁即可（对一行或者多行记录加锁），所以对于并发写入操作来说， InnoDB 的性能更高。不论是表级锁还是行级锁，都存在共享锁（Share Lock，S 锁）和排他锁（Exclusive Lock，X 锁）这两类。

**MVCC 是多版本并发控制方法，即对一份数据会存储多个版本，通过事务的可见性来保证事务能看到自己应该看到的版本。通常会有一个全局的版本分配器来为每一行数据设置版本号，版本号是唯一的。**

MVCC 在 MySQL 中实现所依赖的手段主要是: **隐藏字段、read view、undo log**。

- undo log : undo log 用于记录某行数据的多个版本的数据。
- read view 和 隐藏字段 : 用来判断当前版本数据的可见性。

关于 InnoDB 对 MVCC 的具体实现可以看这篇文章：[InnoDB 存储引擎对 MVCC 的实现]() 。

### MySQL中为什么要有事务回滚机制？

而在 MySQL 中，恢复机制是通过回滚日志（undo log）实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后在对数据库中的对应行进行写入。 当事务已经被提交之后，就无法再次回滚了。

回滚日志作用： 1)能够在发生错误或者用户执行 ROLLBACK 时提供回滚相关的信息 2) 在整个系统发生崩溃、数据库进程直接被杀死后，当用户再次启动数据库进程时，还能够立刻通过查询回滚日志将之前未完成的事务进行回滚，这也就需要回滚日志必须先于数据持久化到磁盘上，是我们需要先写日志后写数据库的主要原因。

### MVCC 多版本并发控制

**数据库并发场景：**

1. 读-读：不存在任何问题，也不需要并发控制；
2. 读-写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读，幻读，不可重复读；
3. 写-写：有线程安全问题，可能会存在更新丢失问题。

多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。

#### **MVCC 可以为数据库解决以下问题：**

1. 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能；
2. 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题。

#### 什么是MVCC多版本控制？

**答案0：**多版本并发控制（MVCC）是一种用来**解决读-写冲突的无锁并发控制**，**为了实现读-写冲突不加锁，而这个读指的就是快照读, 而非当前读，当前读实际上是一种加锁的操作，是悲观锁的实现。在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读。**是通过保存数据在**某个时间点的快照来实现并发控制**的。不管事务执行多长时间，事务内部看到的数据是不受其它事务影响的，根据事务开始的时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的。

**保存数据的历史版本，通过对数据行的多个版本管理来实现数据库的并发控制。这样我们就可以通过比较版本号决定数据是否显示出来，读取数据的时候不需要加锁也可以保证事务的隔离效果。**是行级锁的一个变种，但是它在很多情况下避免了加锁操作，因此开销更低。虽然实现机制有所不同，但大都实现了非阻塞的读操作，写操作也只锁定必要的行。

**多版本并发控制（MVCC）** 在一定程度上实现了 **读写并发** ，它只在 **可重复读RR** 和**提交读RC** 两个隔离级别下工作。其他两个隔离级别都和 MVCC 不兼容，因为 **未提交读（READ UNCOMMITTED）** ，总是读取**最新的数据行**，而不是符合当前事务版本的数据行。而 **可串行化（SERIALIZABLE）**，则会对**所有读取的行都加锁**。

#### MVCC并发控制是怎么优化的

1. 版本管理：MVCC需要维护数据项的多个版本，因此版本管理是一个关键的优化点。数据库需要确保对每个事务可见的版本是正确的，并定期清理过期版本，以避免不必要的存储开销。
2. 读操作优化：MVCC允许读操作读取历史版本的数据，数据库可以采用快照读优化来避免读取已经过期的数据。快照读可以在读取数据时创建一个数据的快照，并在事务的整个执行过程中使用这个快照，从而保证读操作的一致性和隔离性。
3. 写操作优化：写操作会创建新的版本，因此写操作的性能也是一个重要的优化点。数据库可以优化写操作的执行顺序，避免不必要的写操作和版本冲突，从而提高写操作的效率。
4. 并发控制策略：MVCC需要进行并发控制，以确保并发执行的事务之间不会相互干扰。数据库可以采用细粒度锁或基于时间戳的并发控制策略，来实现对数据项的并发访问控制。
5. 事务回滚优化：MVCC的事务回滚是通过回滚到之前的版本来实现的。数据库可以优化回滚操作，只回滚事务修改的数据项，而不是回滚整个事务，从而减少回滚操作的开销。
6. 并发度调优：数据库可以根据硬件和系统配置，调整并发度参数，如并发连接数、线程池大小等，以优化MVCC的并发性能。
7. 快速快照读：快照读是MVCC的核心优化点之一，数据库可以通过使用位图索引、快照链表等技术，实现快速的快照读操作，提高MVCC的读性能。

#### 多版本控制MVCC 与哪个日志锁紧密相关

多版本控制（MVCC）与Undo Log（回滚日志）紧密相关。

MVCC是一种数据库并发控制技术，通过在每个数据项上维护多个版本（历史快照），使得读操作不会阻塞写操作，同时读操作可以读取到已提交的数据版本，从而提高数据库的并发性能。在MVCC中，每个事务执行写操作时，数据库会在数据项上创建一个新的版本，并在MVCC中维护该版本的时间戳。同时，数据库会将写操作记录到Undo Log中，以便在事务回滚时可以撤销这些操作。

Undo Log是用于事务回滚的日志，记录了事务在执行过程中对数据所做的修改操作，以便在事务回滚时可以撤销这些操作，恢复到事务开始前的状态。每个事务在执行之前都会在Undo Log中记录对应的修改操作，如果事务回滚，则根据Undo Log中的记录逆向执行回滚操作。

**MVCC依赖Undo Log来实现读操作的多版本隔离。当一个事务执行读操作时，数据库会根据MVCC机制来选择合适的数据版本。如果读操作的时间戳在某个版本的时间范围内（在事务开始之前或在事务开始之后但在事务提交之前），则该版本对于当前事务是可见的，数据读取会基于这个版本。如果读操作的时间戳在某个版本的时间范围外（在事务提交之后），则该版本对于当前事务是不可见的，数据库会通过Undo Log来获取之前的版本数据，从而实现读操作的隔离性。**

因此，MVCC与Undo Log密切相关，它们共同实现了高效的并发控制和事务隔离，保证了数据库的一致性和可靠性。

#### 什么是快照读、当前读？

- 针对**快照读**（**普通 select** 语句），是**通过 MVCC 方式解决了幻读**，因为**可重复读**RR隔离级别下，事务执行过程中看到的数据，一直跟这个**事务启动时看到的数据是一致**的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。

  针对**当前读**（**select ... for update** 等语句），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读**，因为当执行 select ... for update 语句的时候，会加上 **next-key lock**，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了**避免幻读**问题。

  **快照读的补充：**像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本

  **当前读** 的补充：读取最新数据，而不是历史版本的数据。加锁的 SELECT 就属于当前读。

#### **RC,RR级别下的InnoDB快照读区别、如何工作的？**

Read View生成时机的不同，从而造成RC,RR级别下快照读的结果的不同

- **可重复读RR隔离级别是启动事务时第一次快照读时生成一个 Read View，然后整个事务期间快照读都在用这个 Read View**，所以对之后的修改不可见。Read View会记录此时所有其他活动事务的快照，这些事务的修改对于当前事务都是不可见的。而早于Read View创建的事务所做的修改均是可见
  - 事务B 发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在 m_ids 范围内，判断的结果是在的，那么说明这条记录是**被还未提交的事务修改**的，这时事务 B 并**不会读取这个版本的记录**。而是沿着 **undo log 链条往下找旧版本的记录**，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录
  - 当事物 A 提交事务后，**由于隔离级别时「可重复读」，所以事务 B 再次读取记录时，还是基于启动事务时创建的 Read View 来判断当前版本的记录是否可见。**

- **读提交RC隔离级别是在每次快照读取数据时，都会生成一个新的快照和Read View**。在事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
  - 在事务 A 提交后，**由于隔离级别是「读提交」，所以事务 B 在每次读数据的时候，会重新创建 Read View**
  - 事务 B 会发现这条记录的 trx_id 是 51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小，这意味着修改这条记录的事务早就在创建 Read View 前提交过了，所以该版本的记录对事务 B 是可见的

#### **多版本并发控制MVCC解决了哪些问题？**

**1. 读写之间阻塞的问题**

- 在**并发读写数据库**时，可以做到在**读操作时不用阻塞写操作**，写操作也不用阻塞读操作，提高了数据库**并发读写**的性能
- 同时还可以**解决脏读，幻读，不可重复读**等事务隔离问题，但**不能解决更新丢失**问题

- **MVCC + 悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突**
- **MVCC + 乐观锁：MVCC解决读写冲突，乐观锁解决读写冲突**

**2. 降低了死锁的概率**

因为 InnoDB 的 MVCC 采用了**乐观锁**的方式，读取数据时并不需要加锁，对于写操作，也只锁定必要的行。

**3. 解决一致性读==快照的问题**

一致性读也被称为 **快照读**，当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到这个时间点之后事务提交的更新结果。

 **一致性** ：事务读取到的数据，要么是 **事务开始前就已经存在的数据** ，要么是 **事务自身插入或者修改过的数据** 

**当前读** ：读取最新数据，而不是历史版本的数据。加锁的 SELECT 就属于当前读

#### RR是如何在RC级的基础上解决不可重复读的？

事务中快照读的结果是非常依赖该事务**首次出现快照读**的地方，即某个事务中首次出现快照读的地方非常关键，它有决定**该事务后续快照读结果**的能力，我们这里测试的是更新，**同时删除和更新**也是一样的，如果事务B的快照读是在事务A操作之后进行的，事务B的快照读也是能读取到最新的数据的。

#### **InnoDB 是如何存储记录的多个版本的？**

**事务版本号**

每**开启一个事务**，我们都会从数据库中获得一个事务 ID（也就是事务版本号），这个事务 ID 是自增长的，通过 ID 大小，我们就可以判断事务的时间顺序。

**行记录的隐藏列**

InnoDB 的叶子段存储了数据页，数据页中保存了行记录，而在行记录中有一些重要的隐藏字段：

- `DB_ROW_ID` ：6-byte，隐藏的行 ID，用来生成默认聚簇索引。如果我们创建数据表的时候没有指定聚簇索引，这时 InnoDB 就会用这个隐藏 ID 来创建聚集索引。采用聚簇索引的方式可以提升数据的查找效率。
- `DB_TRX_ID` ：6-byte，操作这个**数据的事务 ID**，也就是最后一个对该数据进行插入或更新的事务 ID。
- `DB_ROLL_PTR` ：7-byte，**回滚指针**，也就是指向这个记录的 **Undo Log 信息**。

![img](https://pic2.zhimg.com/80/v2-cb0c70813be46fe0634a00c248e43c91_1440w.webp)

**Undo Log**

InnoDB 将**行记录快照**保存在了 Undo Log 里，我们可以在回滚段中找到它们，如下图所示：

![img](https://pic4.zhimg.com/80/v2-37f614539ccacf8e0cd5b4193001f7b3_1440w.webp)

从图中能看到回滚指针将数据行的所有快照记录都通过**链表**的结构串联了起来，每个**快照的记录**都保存了当时的 db_trx_id，也是那个**时间点操作这个数据的事务ID**。这样如果我们想要找**历史快照**，就可以通过遍历**回滚指针**的方式进行查找。

#### **可重复读（RR） 隔离级别下 InnoDB 的 MVCC 是如何工作的？**

**查询（SELECT）**

InnoDB 会根据以下两个条件检查每行记录：

1. InnoDB只查找版本早于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以 **确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的** 。
2. 行的删除版本要么未定义，要么大于当前事务版本号。这可以确保 **事务读取到的行，在事务开始之前未被删除** 。

只有符合上述两个条件的记录，才能返回作为查询结果。

**插入（INSERT）**。InnoDB为新插入的每一行保存当前系统版本号作为**行版本号**。

**删除（DELETE）**。InnoDB为删除的每一行保存当前系统版本号作为**行删除标识**。
删除在内部被视为更新，行中的一个特殊位会被设置为已删除。

**更新（UPDATE）**。InnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。

### InnoDB存储引擎对MVCC的实现

#### 一致性非锁定读和锁定读

#### 一致性非锁定读

对于 [**一致性非锁定读（Consistent Nonlocking Reads）**open in new window](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html)的实现，通常做法是加一个版本号或者时间戳字段，在更新数据的同时版本号 + 1 或者更新时间戳。查询时，将当前可见的版本号与对应记录的版本号进行比对，如果记录的版本小于可见版本，则表示该记录可见

在 `InnoDB` 存储引擎中，[多版本控制 (multi versioning)open in new window](https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html) 就是对非锁定读的实现。如果读取的行正在执行 `DELETE` 或 `UPDATE` 操作，这时读取操作不会去等待行上锁的释放。相反地，`InnoDB` 存储引擎会去读取行的一个快照数据，对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)

在 `Repeatable Read` 和 `Read Committed` 两个隔离级别下，如果是执行普通的 `select` 语句（不包括 `select ... lock in share mode` ,`select ... for update`）则会使用 `一致性非锁定读（MVCC）`。并且在 `Repeatable Read` 下 `MVCC` 实现了可重复读和防止部分幻读

#### 锁定读

如果执行的是下列语句，就是 [**锁定读（Locking Reads）**open in new window](https://dev.mysql.com/doc/refman/5.7/en/innodb-locking-reads.html)

- `select ... lock in share mode`
- `select ... for update`
- `insert`、`update`、`delete` 操作

在锁定读下，读取的是数据的最新版本，这种读也被称为 `当前读（current read）`。锁定读会对读取到的记录加锁：

- `select ... lock in share mode`：对记录加 `S` 锁，其它事务也可以加`S`锁，如果加 `x` 锁则会被阻塞
- `select ... for update`、`insert`、`update`、`delete`：对记录加 `X` 锁，且其它事务不能加任何锁

在一致性非锁定读下，即使读取的记录已被其它事务加上 `X` 锁，这时记录也是可以被读取的，即读取的快照数据。上面说了，在 `Repeatable Read` 下 `MVCC` 防止了部分幻读，这边的 “部分” 是指在 `一致性非锁定读` 情况下，只能读取到第一次查询之前所插入的数据（根据 Read View 判断数据可见性，Read View 在第一次查询时生成）。但是！如果是 `当前读` ，每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。所以， **`InnoDB` 在实现`Repeatable Read` 时，如果执行的是当前读，则会对读取的记录使用 `Next-key Lock` ，来防止其它事务在间隙间插入数据**

#### InnoDB 对 MVCC 的实现

`MVCC` 的实现依赖于：**隐藏字段、Read View、undo log**。在内部实现中，`InnoDB` 通过数据行的 `DB_TRX_ID` 和 `Read View` 来判断数据的可见性，如不可见，则通过数据行的 `DB_ROLL_PTR` 找到 `undo log` 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 `Read View` 之前已经提交的修改和该事务本身做的修改

##### 隐藏字段

在内部，`InnoDB` 存储引擎为每行数据添加了三个 [隐藏字段open in new window](https://dev.mysql.com/doc/refman/5.7/en/innodb-multi-versioning.html)：

- `DB_TRX_ID（6字节）`：表示最后一次插入或更新该行的事务 id。此外，`delete` 操作在内部被视为更新，只不过会在记录头 `Record header` 中的 `deleted_flag` 字段将其标记为已删除
- `DB_ROLL_PTR（7字节）` 回滚指针，指向该行的 `undo log` 。如果该行未被更新，则为空
- `DB_ROW_ID（6字节）`：如果没有设置主键且该表没有唯一非空索引时，`InnoDB` 会使用该 id 来生成聚簇索引

##### ReadView

```c
class ReadView {
  /* ... */
private:
  trx_id_t m_low_limit_id;      /* 大于等于这个 ID 的事务均不可见 */

  trx_id_t m_up_limit_id;       /* 小于这个 ID 的事务均可见 */

  trx_id_t m_creator_trx_id;    /* 创建该 Read View 的事务ID */

  trx_id_t m_low_limit_no;      /* 事务 Number, 小于该 Number 的 Undo Logs 均可以被 Purge */

  ids_t m_ids;                  /* 创建 Read View 时的活跃事务列表 */

  m_closed;                     /* 标记 Read View 是否 close */
}
```

[`Read View`open in new window](https://github.com/facebook/mysql-8.0/blob/8.0/storage/innobase/include/read0types.h#L298) 主要是用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”

主要有以下字段：

- `m_low_limit_id`：目前出现过的最大的事务 ID+1，即下一个将被分配的事务 ID。大于等于这个 ID 的数据版本均不可见
- `m_up_limit_id`：活跃事务列表 `m_ids` 中最小的事务 ID，如果 `m_ids` 为空，则 `m_up_limit_id` 为 `m_low_limit_id`。小于这个 ID 的数据版本均可见
- `m_ids`：`Read View` 创建时其他未提交的活跃事务 ID 列表。创建 `Read View`时，将当前未提交事务 ID 记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。`m_ids` 不包括当前事务自己和已提交的事务（正在内存中）
- `m_creator_trx_id`：创建该 `Read View` 的事务 ID

**事务可见性示意图**（[图源open in new window](https://leviathan.vip/2019/03/20/InnoDB的事务分析-MVCC/#MVCC-1)）：

![trans_visible](https://javaguide.cn/assets/trans_visible-048192c5.png)

##### undo-log

`undo log` 主要有两个作用：

- 当事务回滚时用于将数据恢复到修改前的样子
- 另一个作用是 `MVCC` ，当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过 `undo log` 读取之前的版本数据，以此实现非锁定读

**在 `InnoDB` 存储引擎中 `undo log` 分为两种：`insert undo log` 和 `update undo log`：**

1. **`insert undo log`**：指在 `insert` 操作中产生的 `undo log`。因为 `insert` 操作的记录只对事务本身可见，对其他事务不可见，故该 `undo log` 可以在事务提交后直接删除。不需要进行 `purge` 操作

**`insert` 时的数据初始状态：**

![img](https://javaguide.cn/assets/317e91e1-1ee1-42ad-9412-9098d5c6a9ad-dc43aed3.png)

1. **`update undo log`**：`update` 或 `delete` 操作中产生的 `undo log`。该 `undo log`可能需要提供 `MVCC` 机制，因此不能在事务提交时就进行删除。提交时放入 `undo log` 链表，等待 `purge线程` 进行最后的删除

**数据第一次被修改时：**

![img](https://javaguide.cn/assets/c52ff79f-10e6-46cb-b5d4-3c9cbcc1934a-b60a6e78.png)

**数据第二次被修改时：**

![img](https://javaguide.cn/assets/6a276e7a-b0da-4c7b-bdf7-c0c7b7b3b31c-2e496ea1.png)

不同事务或者相同事务的对同一记录行的修改，会使该记录行的 `undo log` 成为一条链表，链首就是最新的记录，链尾就是最早的旧记录。

##### 数据可见性算法

在 `InnoDB` 存储引擎中，创建一个新事务后，执行每个 `select` 语句前，都会创建一个快照（Read View），**快照中保存了当前数据库系统中正处于活跃（没有 commit）的事务的 ID 号**。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务 ID 列表（即 m_ids）。当用户在这个事务中要读取某个记录行的时候，`InnoDB` 会将该记录行的 `DB_TRX_ID` 与 `Read View` 中的一些变量及当前事务 ID 进行比较，判断是否满足可见性条件

### RC 和 RR 隔离级别下 MVCC 的差异

在事务隔离级别 `RC` 和 `RR` （InnoDB 存储引擎的默认事务隔离级别）下，`InnoDB` 存储引擎使用 `MVCC`（非锁定一致性读），但它们生成 `Read View` 的时机却不同

- 在 RC 隔离级别下的 **`每次select`** 查询前都生成一个`Read View` (m_ids 列表)
- 在 RR 隔离级别下只在事务开始后 **`第一次select`** 数据前生成一个`Read View`（m_ids 列表）

### MVCC 解决不可重复读问题

虽然 RC 和 RR 都通过 `MVCC` 来读取快照数据，但由于 **生成 Read View 时机不同**，从而在 RR 级别下实现可重复读

#### 在 RC 下 ReadView 生成情况

**假设时间线来到 T4 ，那么此时数据行 id = 1 的版本链为：**

![img](https://javaguide.cn/assets/a3fd1ec6-8f37-42fa-b090-7446d488fd04-bf41f07c.png)

由于 RC 级别下每次查询都会生成`Read View` ，并且事务 101、102 并未提交，此时 `103` 事务生成的 `Read View` 中活跃的事务 **`m_ids` 为：[101,102]** ，`m_low_limit_id`为：104，`m_up_limit_id`为：101，`m_creator_trx_id` 为：103

- 此时最新记录的 `DB_TRX_ID` 为 101，m_up_limit_id <= 101 < m_low_limit_id，所以要在 `m_ids` 列表中查找，发现 `DB_TRX_ID` 存在列表中，那么这个记录不可见
- 根据 `DB_ROLL_PTR` 找到 `undo log` 中的上一版本记录，上一条记录的 `DB_TRX_ID` 还是 101，不可见
- 继续找上一条 `DB_TRX_ID`为 1，满足 1 < m_up_limit_id，可见，所以事务 103 查询到数据为 `name = 菜花`

**2. 时间线来到 T6 ，数据的版本链为：**

![img](https://javaguide.cn/assets/528559e9-dae8-4d14-b78d-a5b657c88391-2ff79120.png)

因为在 RC 级别下，重新生成 `Read View`，这时事务 101 已经提交，102 并未提交，所以此时 `Read View` 中活跃的事务 **`m_ids`：[102]** ，`m_low_limit_id`为：104，`m_up_limit_id`为：102，`m_creator_trx_id`为：103

- 此时最新记录的 `DB_TRX_ID` 为 102，m_up_limit_id <= 102 < m_low_limit_id，所以要在 `m_ids` 列表中查找，发现 `DB_TRX_ID` 存在列表中，那么这个记录不可见
- 根据 `DB_ROLL_PTR` 找到 `undo log` 中的上一版本记录，上一条记录的 `DB_TRX_ID` 为 101，满足 101 < m_up_limit_id，记录可见，所以在 `T6` 时间点查询到数据为 `name = 李四`，与时间 T4 查询到的结果不一致，不可重复读！

**3. 时间线来到 T9 ，数据的版本链为：**

![img](https://javaguide.cn/assets/6f82703c-36a1-4458-90fe-d7f4edbac71a-c8de5ed7.png)

重新生成 `Read View`， 这时事务 101 和 102 都已经提交，所以 **m_ids** 为空，则 m_up_limit_id = m_low_limit_id = 104，最新版本事务 ID 为 102，满足 102 < m_low_limit_id，可见，查询结果为 `name = 赵六`

> **总结：** **在 RC 隔离级别下，事务在每次查询开始时都会生成并设置新的 Read View，所以导致不可重复读**

#### 在 RR 下 ReadView 生成情况

在可重复读级别下，只会在事务开始后第一次读取数据时生成一个 Read View（m_ids 列表）

**1. 在 T4 情况下的版本链为：**

![img](https://javaguide.cn/assets/0e906b95-c916-4f30-beda-9cb3e49746bf-3a363d10.png)

在当前执行 `select` 语句时生成一个 `Read View`，此时 **`m_ids`：[101,102]** ，`m_low_limit_id`为：104，`m_up_limit_id`为：101，`m_creator_trx_id` 为：103

此时和 RC 级别下一样：

- 最新记录的 `DB_TRX_ID` 为 101，m_up_limit_id <= 101 < m_low_limit_id，所以要在 `m_ids` 列表中查找，发现 `DB_TRX_ID` 存在列表中，那么这个记录不可见
- 根据 `DB_ROLL_PTR` 找到 `undo log` 中的上一版本记录，上一条记录的 `DB_TRX_ID` 还是 101，不可见
- 继续找上一条 `DB_TRX_ID`为 1，满足 1 < m_up_limit_id，可见，所以事务 103 查询到数据为 `name = 菜花`

**2. 时间点 T6 情况下：**

![img](https://javaguide.cn/assets/79ed6142-7664-4e0b-9023-cf546586aa39-9c5cd303.png)

在 RR 级别下只会生成一次`Read View`，所以此时依然沿用 **`m_ids`：[101,102]** ，`m_low_limit_id`为：104，`m_up_limit_id`为：101，`m_creator_trx_id` 为：103

- 最新记录的 `DB_TRX_ID` 为 102，m_up_limit_id <= 102 < m_low_limit_id，所以要在 `m_ids` 列表中查找，发现 `DB_TRX_ID` 存在列表中，那么这个记录不可见
- 根据 `DB_ROLL_PTR` 找到 `undo log` 中的上一版本记录，上一条记录的 `DB_TRX_ID` 为 101，不可见
- 继续根据 `DB_ROLL_PTR` 找到 `undo log` 中的上一版本记录，上一条记录的 `DB_TRX_ID` 还是 101，不可见
- 继续找上一条 `DB_TRX_ID`为 1，满足 1 < m_up_limit_id，可见，所以事务 103 查询到数据为 `name = 菜花`

**3. 时间点 T9 情况下：**

![img](https://javaguide.cn/assets/cbbedbc5-0e3c-4711-aafd-7f3d68a4ed4e-7b4a86c0.png)

此时情况跟 T6 完全一样，由于已经生成了 `Read View`，此时依然沿用 **`m_ids`：[101,102]** ，所以查询结果依然是 `name = 菜花`

### 听说过视图吗？那游标呢？

视图是一种虚拟的表，通常是有一个表或者多个表的行或列的子集，具有和物理表相同的功能 游标是对查询出来的结果集作为一个单元来有效的处理。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。

### 视图的作用是什么？可以更改吗？

视图是虚拟的表，与包含数据的表不一样，视图只包含使用时动态检索数据的查询；不包含任何列或数据。使用视图可以简化复杂的 sql 操作，隐藏具体的细节，保护数据；视图创建后，可以使用与表相同的方式利用它们。

视图不能被索引，也不能有关联的触发器或默认值，如果视图本身内有order by 则对视图再次order by将被覆盖。

创建视图：create view xxx as xxxx

对于某些视图比如未使用联结子查询分组聚集函数Distinct Union等，是可以对其更新的，对视图的更新将对基表进行更新；但是视图主要用于简化检索，保护数据，并不用于更新，而且大部分视图都不可以更新

# =============================================

## Mysql 锁=====

锁是一种常见的并发事务的控制方式。

### 表级锁和行级锁了解吗？有什么区别？

MyISAM 仅仅支持表级锁(table-level locking)，一锁就锁整张表，这在并发写的情况下性非常差。InnoDB 不光支持表级锁(table-level locking)，还支持行级锁(row-level locking)，默认为行级锁。

行级锁的粒度更小，仅对相关的记录上锁即可（对一行或者多行记录加锁），所以对于并发写入操作来说， InnoDB 的性能更高。

**表级锁和行级锁对比**：

- **表级锁：** MySQL 中锁定粒度最大的一种锁（全局锁除外），是针对非索引字段加的锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。不过，触发锁冲突的概率最高，高并发下效率极低。表级锁和存储引擎无关，MyISAM 和 InnoDB 引擎都支持表级锁。
- **行级锁：** MySQL 中锁定粒度最小的一种锁，是 **针对索引字段加的锁** ，只针对当前操作的行记录进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。行级锁和存储引擎有关，是在存储引擎层面实现的。

#### 全局锁怎么用、场景、缺点

全局锁是怎么用的？

- ```sql
  flush tables with read lock
  ```

- 执行后，**整个数据库就处于只读状态了**，这时其他线程执行以下操作，都会被阻塞：

  - 对数据的增删改操作，比如 insert、delete、update等语句；
  - 对表结构的更改操作，比如 alter table、drop table 等语句。

- 如果要释放全局锁，则要执行这条命令：

  ```sql
  unlock tables
  ```

全局锁应用场景是什么？

- 全局锁主要应用于做**全库逻辑备份**，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。

全局锁有什么缺点？

- 加上全局锁，意味着整个数据库都是只读状态。那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而**不能更新数据**，这样会造成业务停滞。

#### 备份数据库数据使用全局锁会影响业务，避免方式？

- 如果数据库的引擎支持的事务支持**可重复读的隔离级别**，那么在备份数据库之前先开启事务，会先创建 Read View，然后**整个事务执行期间都在用这个 Read View**，而且由于 MVCC 的支持，备份期间业务依然可以**对数据进行更新操作**。因为在可重复读的隔离级别下，即使其他事务更新了表的数据，也不会影响备份数据库时的 Read View，这就是事务四大特性中的隔离性，这样备份期间备份的数据一直是在开启事务时的数据。
- 备份数据库的工具是 **mysqldump**，在使用 mysqldump 时加上 `–single-transaction` 参数的时候，就会在备份数据库之前先开启事务。这种方法只适用于支持「可重复读隔离级别的事务」的存储引擎。InnoDB 存储引擎默认的事务隔离级别正是可重复读，因此可以采用这种方式来备份数据库。

#### 表级锁（表锁、元数据锁、意向锁、AUTO-INC 锁）

表锁；

```sql
lock tables t_student read;
```

- 表锁除了会限制**别的线程的读写**外，也会限制**本线程**接下来的读写操作。
- 如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放
- 尽量避免在使用 InnoDB 引擎的表使用表锁，因为表锁的颗粒度太大，会影响并发性能，**InnoDB 牛逼的地方在于实现了颗粒度更细的行级锁**。

元数据锁（MDL）; 

- MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个**表结构**做了变更。

- 对**数据库表**进行操作时，会自动给这个表加上 MDL：

  - 对一张表进行 CRUD 操作时，加的是 **MDL 读锁**；

  - 对一张表做结构**变更操作**的时候，加的是 **MDL 写锁**；

- MDL 是在事务提交后才会释放，这意味着**事务执行期间，MDL 是一直持有的**。如果数据库有一个长事务，可能会有大量的线程被阻塞住，这时数据库的线程很快就会爆满。

- 申请 MDL 锁的操作会形成一个队列，队列中**写锁获取优先级高于读锁**，一旦出现 MDL 写锁等待，会阻塞后续该表的所有 CRUD 操作==MDL读锁

意向锁；

- 在使用 InnoDB 引擎的**表里对某些记录**加上「共享锁」之前，需要先在**表级别**加上一个「意向共享锁」；
- 在使用 InnoDB 引擎的表里对某些纪录加上「独占锁」之前，需要先在表级别加上一个「意向独占锁」；
- **普通的 select** 是**不会加行级锁**的，普通的 select 语句是利用 MVCC 实现一致性读，是无锁的
- **意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁和独占表锁发生冲突。**
- 表锁和行锁是满足读读共享、读写互斥、写写互斥的。

意向锁的目的?

如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。所以，**意向锁的目的是为了快速判断表里是否有记录被加锁**

AUTO-INC 锁；

表里的**主键**通常都会设置成自增的，这是通过**对主键字段声明 `AUTO_INCREMENT` 属性**实现的。之后可以在插入数据时，可以不指定主键的值，数据库会自动给主键赋值递增的值，这主要是通过 **AUTO-INC 锁**实现的。

原理？

AUTO-INC 锁是特殊的表锁机制，锁**不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放**。

- **在插入数据时，会加一个表级别的 AUTO-INC 锁**，然后为被 `AUTO_INCREMENT` 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。
- 一个事务在持有 AUTO-INC 锁的过程中，其他事务的如果要向该表插入语句都会被阻塞，从而保证插入数据时，被 `AUTO_INCREMENT` 修饰的字段的值是连续递增的

缺点？

AUTO-INC 锁在对大量数据进行插入的时候，会影响插入性能，因为另一个事务中的插入会被阻塞。

轻量级锁比AUTO-INC 锁的优势

- 在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种**轻量级的锁**来实现自增。一样也是在插入数据的时候，会为被 `AUTO_INCREMENT` 修饰的字段加上轻量级锁，**然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁**。

innodb_autoinc_lock_mode 的系统变量

- InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。
  - 当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁；
  - 当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，**申请自增主键后就释放锁**，并不需要等语句执行后才释放。
  - 当 innodb_autoinc_lock_mode = 1：
    - 普通 insert 语句，**自增锁在申请之后**就马上释放；
    - 类似 insert … select 这样的**批量插入数据**的语句，自增锁还是要**等语句结束**后才被释放；
- 当 innodb_autoinc_lock_mode = **2 是性能最高**的方式，但是当搭配 **binlog 的日志格式**是 statement 一起使用的时候，在「主从复制的场景」中会发生**数据不一致的问题**。要解决这问题，binlog 日志格式要设置为 row，这样在 binlog 里面记录的是**主库分配的自增值**，到备库执行的时候，主库的自增值是什么，从库的自增值就是什么。**当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题**

#### 行级锁、哪些种类

普通的 select 语句是不会对记录加锁的，因为它属于**快照读**。如果要在查询时对记录加行锁，可以使用下面这两个方式，这种查询会加锁的语句称为**锁定读**

- 对读取的记录加共享锁：select ... lock in share mode;
- 对读取的记录加独占锁:select ... for update;
- 上面这两条语句必须在一个事务中，**因为当事务提交了，锁就会被释放**，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。

共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。

行级锁有哪些种类？

- Record Lock，**记录锁**，也就是仅仅把一条记录锁上；记录锁是有 S 锁和 X 锁之分的：
  - 当一个事务对一条记录加了 S 型记录锁后，其他事务也可以继续对该记录加 S 型记录锁（S 型与 S 锁兼容），但是不可以对该记录加 X 型记录锁（S 型与 X 锁不兼容）;
  - 当一个事务对一条记录加了 X 型记录锁后，其他事务既不可以对该记录加 S 型记录锁（S 型与 X 锁不兼容），也不可以对该记录加 X 型记录锁（X 型与 X 锁不兼容）。
- Gap Lock，**间隙锁**，锁定一个范围，但是不包含**记录本身**；目的是为了解决**可重复读隔离级别下幻读**的现象
  - 间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，**间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的**
- Next-Key Lock：Record Lock + Gap Lock 的组合，**临键锁**。锁定一个范围，并且**锁定记录本身**
  - **如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**
  - 虽然相同范围的间隙锁是多个事务相互兼容的，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的

- **插入意向锁**。**一个事务在插入一条记录**的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。如果有的话，插入操作就会发生**阻塞**，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个**插入意向锁**，表明**有事务想在某个区间插入新记录，但是现在处于等待状态**。
  - 插入意向锁名字虽然有意向锁，但是它并**不是意向锁，它是一种特殊的间隙锁，属于行级别锁**。
  - 如果说间隙锁锁住的是一个区间，那么**「插入意向锁」锁住**的就是一个点。因而从这个角度来说，插入意向锁确实是一种**特殊的间隙锁**。
  - 插入意向锁与间隙锁的另一个非常重要的差别是：尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁

### 行级锁的使用有什么注意事项？

InnoDB 的行锁是针对索引字段加的锁，表级锁是针对非索引字段加的锁。当我们执行 `UPDATE`、`DELETE` 语句时，如果 `WHERE`条件中字段没有命中唯一索引或者索引失效的话，就会导致扫描全表对表中的所有行记录进行加锁。这个在我们日常工作开发中经常会遇到，一定要多多注意！！！

不过，很多时候即使用了索引也有可能会走全表扫描，这是因为 MySQL 优化器的原因。

#### **行锁的适用场景：**

A用户消费，service层先查询该用户的账户余额，若余额足够，则进行后续的扣款操作；这种情况查询的时候应该对该记录进行加锁。

否则，B用户在A用户查询后消费前先一步将A用户账号上的钱转走，而此时A用户已经进行了用户余额是否足够的判断，则可能会出现余额已经不足但却扣款成功的情况。

为了避免此情况，需要在A用户操作该记录的时候进行for update加锁

#### 怎么加行级锁？

- 对记录加锁时，**加锁的基本单位是 next-key lock**，它是由记录锁和间隙锁组合而成的，**next-key lock 是前开后闭区间，而间隙锁是前开后开区间**。

- MySQL 为什么要这样加锁，主要要以避免幻读角度去分析，这样就很容易理解这些加锁的规则。在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，**如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住**

- next-key lock 在一些场景下会**退化成记录锁或间隙锁**。
  - **唯一索引等值查询**：
    - 当查询的记录是存在的，next-key lock 会退化成「记录锁」。
    - 当查询的记录是不存在的，next-key lock 会退化成「间隙锁」。
  - **非唯一索引等值查询**：
    - 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个**扫描**的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后**在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁**。
    - 当查询的记录「不存在」时，**扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁**。
  - 非唯一索引和主键索引的**范围查询的加锁规则**不同之处在于：
    - 唯一索引在满足一些条件的时候，next-key lock 退化为间隙锁和记录锁。
    - 非唯一索引范围查询，next-key lock 不会退化为间隙锁和记录锁。

### InnoDB 有哪几类行锁？

InnoDB 行锁是通过对索引数据页上的记录加锁实现的，MySQL InnoDB 支持三种行锁定方式：

- **记录锁（Record Lock）**：也被称为记录锁，属于单个行记录上的锁。
- **间隙锁（Gap Lock）**：锁定一个范围，不包括记录本身。
- **临键锁（Next-Key Lock）**：Record Lock+Gap Lock，锁定一个范围，包含记录本身，主要目的是为了解决幻读问题（MySQL 事务部分提到过）。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁。

**在 InnoDB 默认的隔离级别 REPEATABLE-READ 下，行锁默认使用的是 Next-Key Lock。但是，如果操作的索引是唯一索引或主键，InnoDB 会对 Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围。**

### 共享锁和排他锁呢？

不论是表级锁还是行级锁，都存在共享锁（Share Lock，S 锁）和排他锁（Exclusive Lock，X 锁）这两类：

- **共享锁（S 锁）**：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。
- **排他锁（X 锁）**：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条事务加任何类型的锁（锁不兼容）。

排他锁与任何的锁都不兼容，共享锁仅和共享锁兼容。

|      | S 锁   | X 锁 |
| :--- | :----- | :--- |
| S 锁 | 不冲突 | 冲突 |
| X 锁 | 冲突   | 冲突 |

由于 MVCC 的存在，对于一般的 `SELECT` 语句，InnoDB 不会加任何锁。不过， 你可以通过以下语句显式加共享锁或排他锁。



```sql
# 共享锁 可以在 MySQL 5.7 和 MySQL 8.0 中使用
SELECT ... LOCK IN SHARE MODE;
# 共享锁 可以在 MySQL 8.0 中使用
SELECT ... FOR SHARE;
# 排他锁
SELECT ... FOR UPDATE;
```

### 意向锁有什么作用？

如果需要用到表锁的话，如何判断表中的记录没有行锁呢，一行一行遍历肯定是不行，性能太差。我们需要用到一个叫做意向锁的东东来快速判断是否可以对某个表使用表锁。

意向锁是表级锁，共有两种：

- **意向共享锁（Intention Shared Lock，IS 锁）**：事务有意向对表中的某些记录加共享锁（S 锁），加共享锁前必须先取得该表的 IS 锁。
- **意向排他锁（Intention Exclusive Lock，IX 锁）**：事务有意向对表中的某些记录加排他锁（X 锁），加排他锁之前必须先取得该表的 IX 锁。

**意向锁是由数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享/排他锁之前，InooDB 会先获取该数据行所在在数据表的对应意向锁。**

意向锁之间是互相兼容的。

|       | IS 锁 | IX 锁 |
| ----- | ----- | ----- |
| IS 锁 | 兼容  | 兼容  |
| IX 锁 | 兼容  | 兼容  |

意向锁和共享锁和排它锁互斥（这里指的是表级别的共享锁和排他锁，意向锁不会与行级的共享锁和排他锁互斥）。

|      | IS 锁 | IX 锁 |
| ---- | ----- | ----- |
| S 锁 | 兼容  | 互斥  |
| X 锁 | 互斥  | 互斥  |

《MySQL 技术内幕 InnoDB 存储引擎》这本书对应的描述应该是笔误了。

![img](https://oss.javaguide.cn/github/javaguide/mysql/image-20220511171419081.png)

### 当前读和快照读有什么区别？

**快照读**（一致性非锁定读）就是单纯的 `SELECT` 语句，但不包括下面这两类 `SELECT` 语句：

```sql
SELECT ... FOR UPDATE
# 共享锁 可以在 MySQL 5.7 和 MySQL 8.0 中使用
SELECT ... LOCK IN SHARE MODE;
# 共享锁 可以在 MySQL 8.0 中使用
SELECT ... FOR SHARE;
```

快照即记录的历史版本，每行记录可能存在多个历史版本（多版本技术）。

快照读的情况下，如果读取的记录正在执行 UPDATE/DELETE 操作，读取操作不会因此去等待记录上 X 锁的释放，而是会去读取行的一个快照。

只有在事务隔离级别 RC(读取已提交) 和 RR（可重读）下，InnoDB 才会使用一致性非锁定读：

- 在 RC 级别下，对于快照数据，一致性非锁定读总是读取被锁定行的最新一份快照数据。
- 在 RR 级别下，对于快照数据，一致性非锁定读总是读取本事务开始时的行数据版本。

快照读比较适合对于数据一致性要求不是特别高且追求极致性能的业务场景。

**当前读** （一致性锁定读）就是给行记录加 X 锁或 S 锁。

当前读的一些常见 SQL 语句类型如下：

```sql
# 对读的记录加一个X锁
SELECT...FOR UPDATE
# 对读的记录加一个S锁
SELECT...LOCK IN SHARE MODE
# 对读的记录加一个S锁
SELECT...FOR SHARE
# 对修改的记录加一个X锁
INSERT...
UPDATE...
DELETE...
```

### 自增锁有了解吗？

> 不太重要的一个知识点，简单了解即可。

关系型数据库设计表的时候，通常会有一列作为自增主键。InnoDB 中的自增主键会涉及一种比较特殊的表级锁— **自增锁（AUTO-INC Locks）** 。

```sql
CREATE TABLE `sequence_id` (
  `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
  `stub` char(10) NOT NULL DEFAULT '',
  PRIMARY KEY (`id`),
  UNIQUE KEY `stub` (`stub`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

更准确点来说，不仅仅是自增主键，`AUTO_INCREMENT`的列都会涉及到自增锁，毕竟非主键也可以设置自增长。

如果一个事务正在插入数据到有自增列的表时，会先获取自增锁，拿不到就可能会被阻塞住。这里的阻塞行为只是自增锁行为的其中一种，可以理解为自增锁就是一个接口，其具体的实现有多种。具体的配置项为 `innodb_autoinc_lock_mode` （MySQL 5.1.22 引入），可以选择的值如下：

| innodb_autoinc_lock_mode | 介绍                           |
| :----------------------- | :----------------------------- |
| 0                        | 传统模式                       |
| 1                        | 连续模式（MySQL 8.0 之前默认） |
| 2                        | 交错模式(MySQL 8.0 之后默认)   |

交错模式下，所有的“INSERT-LIKE”语句（所有的插入语句，包括：`INSERT`、`REPLACE`、`INSERT…SELECT`、`REPLACE…SELECT`、`LOAD DATA`等）都不使用表级锁，使用的是轻量级互斥锁实现，多条插入语句可以并发执行，速度更快，扩展性也更好。

不过，如果你的 MySQL 数据库有主从同步需求并且 Binlog 存储格式为 Statement 的话，不要将 InnoDB 自增锁模式设置为交叉模式，不然会有数据不一致性问题。这是因为并发情况下插入语句的执行顺序就无法得到保障。

> 如果 MySQL 采用的格式为 Statement ，那么 MySQL 的主从同步实际上同步的就是一条一条的 SQL 语句。

### InnoDB 存储引擎的锁的算法有哪些？

1. Record lock：单个行记录上的锁；
2. Gap lock：间隙锁，锁定一个范围，不包括记录本身；
3. Next-key lock：record+gap 锁定一个范围，包含记录本身。

### MySQL 数据库 CPU 飙升到 500% 的话他怎么处理？

当 CPU 飙升到 500% 时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。

如果是 mysqld 造成的，通过 SHOW PROCESSLIST 查看正在运行的线程，是不是有消耗资源的 SQL 在运行，找出其中消耗高的 SQL，看看执行计划是否准确， index 是否缺失，或者是数据量太大造成。

然后 kill 掉这些线程（同时观察 CPU 使用率是否下降），等进行相应的调整（比如说加索引、改 SQL、改内存参数）之后，再重新跑这些 SQL。

若每个 SQL 消耗资源都不多，只是同一时间大量的 session 连进来导致 CPU 飙升，这种情况就需要分析为何连接数会激增，再做出相应的调整，比如说限制连接数等





### 什么是死锁？如何解决死锁？

死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。

**常见的解决死锁的方法**

- 如果不同程序并发存取多个表，尽量约定 **以相同的顺序访问表**，可以大大降低死锁机会；
- 在同一个事务中，尽可能做到 **一次锁定所需要的所有资源**，减少死锁产生概率；
- 对于非常容易产生死锁的业务部分，可以尝试使用 **升级锁定颗粒度**，通过 **表级锁** 定来减少死锁产生的概率。

### 什么是乐观锁和悲观锁？如何实现？

DBMS 中的 **并发控制** 的任务是确保在 多**个事务同时存取数据库中同一数据** 时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。

**悲观锁**：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。这对于长事务来讲，可能会严重影响系统的并发处理能力。实现方式：使用数据库中的锁机制。

**乐观锁**：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。乐观锁适用于 **读多写少** 的应用场景，这样可以提高吞吐量。实现方式：一般会使用版本号机制或 CAS 算法实现。

### 数据库悲观锁和乐观锁的原理和应用场景分别有什么？

悲观锁，先获取锁，再进行业务操作，一般就是利用类似 SELECT … FOR UPDATE 这样的语句，对数据加锁，避免其他事务意外修改数据。 当数据库执行SELECT … FOR UPDATE时会获取被select中的数据行的行锁，select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。

乐观锁，先进行业务操作，只在最后实际更新数据时进行检查数据是否被更新过。Java 并发包中的 AtomicFieldUpdater 类似，也是利用 CAS 机制，并不会对数据加锁，而是通过对比数据的时间戳或者版本号，来实现乐观锁需要的版本判断。

### update 没使用索引会锁全表？

InnoDB 存储引擎的默认事务隔离级别是「可重复读」，但是在这个隔离级别下，在多个事务并发的时候，会出现幻读的问题，所谓的幻读是指在同一事务下，连续执行两次同样的查询语句，第二次的查询语句可能会返回之前不存在的行。因此 InnoDB 存储引擎自己实现了**行锁**，通过 next-key 锁（记录锁和间隙锁的组合）来锁住**记录本身和记录之间的“间隙”**，防止其他事务在这个记录之间插入新的记录，从而**避免了幻读**现象。

#### 为什么没使用索引就会全表扫描？

当我们执行 update 语句时，实际上是会对**记录加独占锁（X 锁）**的，如果其他事务对持有独占锁的记录进行修改时是会被阻塞的。另外，这个锁并不是执行完 update 语句就会释放的，而是会等**事务结束时**才会释放。

在 InnoDB 事务中，对记录加锁带基本单位是 **next-key 锁**，但是会因为一些条件会**退化成间隙锁，或者记录锁**。加锁的位置准确的说，**锁是加在索引上的而非行**上。

- 比如，在 update 语句的 where 条件**使用了唯一索**引，那么 next-key 锁会退化成记录锁，也就是只会给**一行记录加锁**。
- **在 update 语句的 where 条件没有使用索引，就会全表扫描，于是就会对所有记录加上 next-key 锁（记录锁 + 间隙锁），相当于把整个表锁住了**。
- 那 update 语句的 where 带上索引就能避免全表记录加锁了吗？并不是。**关键还得看这条语句在执行过程种，优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了**。

#### 如何避免全表记录加锁的发生？

- 可以将 MySQL 里的 `sql_safe_updates` 参数设置为 1，开启安全更新模式，当 sql_safe_updates 设置为 1 时：
- **update 语句**必须满足如下条件之一才能执行成功：
  - 使用 where，并且 where 条件中**必须有索引列**；
  - 使用 **limit**；
  - 同时使用 where 和 limit，此时 where 条件中可以没有索引列；
- **delete 语句**必须满足以下条件能执行成功：
  - 同时使用 where 和 limit，此时 where 条件中可以没有索引列；
- 如果 where 条件带上了索引列，但是优化器最终扫描选择的是全表，而不是索引的话，我们可以使用 `force index([index_name])` 可以告诉优化器使用哪个索引，以此避免有几率锁全表带来的隐患。

### 记录锁+间隙锁可以防止删除操作而导致的幻读吗？

- 在 MySQL 的可重复读隔离级别下，针对当前读的语句会对**索引**加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。
- **在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了**，这是挺严重的问题。

### MySQL 死锁了，怎么办？

假设这时有两事务，一个事务要插入订单 1007 记录，另外一个事务要插入订单 1008，因为需要对**订单做幂等性校验**，所以两个事务先要查询该订单是否存在，不存在才插入记录；两个事务都陷入了等待状态（前提没有打开死锁检测），也就是发生了死锁，因为都在相互等待对方释放锁。这里在查询记录是否存在的时候，使用了 **`select ... for update` 语句**，目的为了**防止事务执行的过程中，有其他事务插入了记录，而出现幻读**的问题。如果没有使用 `select ... for update` 语句，而使用了单纯的 select 语句，如果是两个订单号一样的请求同时进来，就会出现两个重复的订单，有可能出现幻读

##### 为什么会产生死锁？（上面）

**Innodb 引擎为了解决「可重复读」隔离级别下的幻读问题，就引出了 next-key 锁**，它是记录锁和间隙锁的组合。

- Record Lock，记录锁，锁的是记录本身；
- Gap Lock，间隙锁，锁的就是两个值之间的空隙，以防止其他事务在这个空隙间插入新的数据，从而避免幻读现象。

普通的 select 语句是不会对记录加锁的，因为它是通过 **MVCC 的机制实现的快照读**

##### 行锁释放时机？

行锁的释放时机是在**事务提交**（commit）后，锁就会被释放，并不是一条语句执行完就释放行锁；如果 update 语句的 **where 条件没有用到索引列**，那么就会**全表扫描**，在一行行扫描的过程中，不仅给行记录加上了行锁，还给行记录两边的空隙也加上了间隙锁，相当于**锁住整个表**，然后直到**事务结束才会释放锁**

- 可以通过 `select * from performance_schema.data_locks\G;` 这条语句，查看事务执行 SQL 过程中加了什么锁
- **插入意向锁与间隙锁是冲突的，所以当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。而间隙锁与间隙锁之间是兼容的，所以两个事务中 `select ... for update` 语句并不会相互影响**

##### 为什么间隙锁与间隙锁之间是兼容的？

**间隙锁的意义只在于阻止区间被插入**，因此是可以共存的。**一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁**，共享和排他的间隙锁是没有区别的，他们相互不冲突，且功能相同，即两个事务可以同时持有包含共同间隙的间隙锁。这里的共同间隙包括两种场景：

- 其一是两个间隙锁的**间隙区间**完全一样；
- 其二是一个间隙锁包含的间隙区间是另一个间隙锁包含间隙区间的子集。

##### **两个事务的next-key lock相同不一定兼容**

有一点要注意，**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。虽然相同范围的间隙锁是多个事务相互兼容的，但对于**记录锁**，我们是要考虑 X 型与 S 型关系。X 型的记录锁与 X 型的记录锁是冲突的，比如一个事务执行了 select ... where id = 1 for update，后一个事务在执行这条语句的时候，就会被阻塞的。

- 但是还要注意！对于这种范围为 (1006, +∞] 的 next-key lock，两个事务是可以同时持有的，不会冲突。因为 **+∞ 并不是一个真实的记录**，自然就不需要考虑 X 型与 S 型关系。

##### 插入意向锁是什么？

**插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作**。

如果说间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一个点。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。

插入意向锁与间隙锁的另一个非常重要的差别是：尽管「插入意向锁」也属于间隙锁，但两个事务却不能在**同一时间**内，一个拥有**间隙锁**，另一个拥有该间隙区间内的**插入意向锁**（当然，插入意向锁如果不在间隙锁区间内则是可以的）

##### 插入意向锁的生成时机？

每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，如果**已加间隙锁**，此时会**生成一个插入意向锁**，然后锁的状态设置为**等待状态**（*PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当**锁状态为正常状态**时，才代表**事务成功获取到了锁***），现象就是 Insert 语句会被阻塞

##### Insert 插入语句是怎么加行级锁的？

###### 隐式锁？

Insert 语句在**正常执行时**是**不会生成锁结构**的，它是靠**聚簇索引**记录自带的 trx_id 隐藏列来作为**隐式锁**来保护记录的。

当事务需要加锁的时，如果这个锁不可能发生冲突，InnoDB会跳过加锁环节，这种机制称为**隐式锁**。隐式锁是 InnoDB 实现的一种**延迟加锁机制**，其特点是只有在可能**发生冲突时才加锁**，从而减少了锁的数量，提高了系统整体性能。

隐式锁就是在 Insert 过程中不加锁，只有在特殊情况下，才会将**隐式锁转换为显示锁**，这里我们列举两个场景。

加行级锁规则：

- 如果记录之间加有间隙锁，为了避免幻读，此时是不能插入记录的；

- 如果 Insert 的记录和已有记录存在**唯一键冲突**，此时也不能插入记录；

  至于是行级锁的类型是记录锁，还是 next-key 锁，跟是「主键冲突」还是「唯一二级索引冲突」有关系。

  如果**主键索引重复**：

- 当隔离级别为**读已提交**时，插入新记录的事务会给已存在的主键值重复的聚簇索引记录**添加 S 型记录锁**。

- 当隔离级别是**可重复读**（默认隔离级别），插入新记录的事务会给已存在的主键值重复的聚簇索引记录**添加 S 型记录锁**。

  如果**唯一二级索引列重复**：

- **不论是哪个隔离级别**，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录**添加 S 型 next-key 锁**。

##### 如何避免死锁？

- 死锁的四个必要条件：**互斥、占有且等待、不可强占用、循环等待**。只要系统发生死锁，这些条件必然成立，但是只要破坏任意一个条件就死锁就不会成立。
- 在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：
  - **设置事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个**事务进行回滚**，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒
  - **开启主动死锁检测**。主动死锁检测在发现死锁后，主动**回滚死锁链条中的某一个事务**，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。



### 5.6 字节面试：加了什么锁，导致死锁的？

- 通过 `select * from performance_schema.data_locks\G;` 这条语句，查看事务执行 SQL 过程中加了什么锁

- 事务 A 和 事务 B 都在执行 insert 语句后，都陷入了等待状态（前提没有打开死锁检测），也就是发生了死锁，因为都在相互等待对方释放锁。

- Time 1 阶段，事务 A 执行以下语句：

  ```sql
  mysql> update t_student set score = 100 where id = 25;
  ```

  加了两个锁，分别是：

  - 表锁：X 类型的意向锁；
  - 行锁：X 类型的间隙锁；

  **此时事务 A 在主键索引（INDEX_NAME : PRIMARY）上加的是间隙锁，锁范围是`(20, 30)`**

- Time 2 阶段，事务B执行以下语句：

  ```sql
  mysql> update t_student set score = 100 where id = 26;
  ```

  事务 A 和 事务 B 的间隙锁范围都是一样的，两个事务的间隙锁之间是相互兼容的，不会产生冲突。**间隙锁的意义只在于阻止区间被插入**，因此是可以共存的。**一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁**，共享（S型）和排他（X型）的间隙锁是没有区别的，他们相互不冲突，且功能相同

- Time 3，事务 A 插入了一条记录：

  ```sql
  mysql> insert into t_student(id, no, name, age,score) value (25, 'S0025', 'sony', 28, 90);
  ```

  此时，事务 A 就陷入了等待状态。事务 A 的状态为等待状态（LOCK_STATUS: WAITING），因为向事务 B 生成的间隙锁（范围 `(20, 30)`）中插入了一条记录，所以事务 A 的插入操作生成了一个插入意向锁（`LOCK_MODE:INSERT_INTENTION`）

  **插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作**。如果说间隙锁锁住的是一个区间，那么「插入意向锁」锁住的就是一个点

  插入意向锁与间隙锁的另一个非常重要的差别是：**尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。所以，插入意向锁和间隙锁之间是冲突的**。

- Time 4，事务 B 插入了一条记录：

  ```sql
  mysql> insert into t_student(id, no, name, age,score) value (26, 'S0026', 'ace', 28, 90);
  ```

  此时，事务 B 就陷入了等待状态。事务 B 在生成插入意向锁时而导致被阻塞，这是因为事务 B 向事务 A 生成的范围为 (20, 30) 的间隙锁插入了一条记录，而**插入意向锁和间隙锁是冲突的**，所以事务 B 在获取插入意向锁时就陷入了等待状态

##### 正题：

- 事务 A 和事务 B 在**执行完后 update 语句后**都持有范围为`(20, 30）`的**间隙锁**，而接下来的**插入操作为了获取到插入意向锁**，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：**互斥、占有且等待、不可强占用、循环等待**，因此发生了死锁
- 两个事务即使生成的间隙锁的范围是一样的，也不会发生冲突，因为间隙锁目的是为了防止其他事务插入数据，因此间隙锁与间隙锁之间是相互兼容的。
- 在执行插入语句时，如果**插入的记录**在其他事务持有间隙锁范围内，插入语句就会被阻塞，因为插入语句在碰到间隙锁时，**会生成一个插入意向锁**，然后**插入意向锁和间隙锁之间是互斥**的关系。

# ==========================

## Mysql 查询缓存====

### 查询缓存、适用场景、不适用

MySQL 中的查询缓存虽然能够提升数据库的查询性能，但是查询同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。

查询缓存是一个适用较少情况的缓存机制。如果你的应用对数据库的更新很少，那么查询缓存将会作用显著。比较典型的如博客系统，一般博客更新相对较慢，数据表相对稳定不变，这时候查询缓存的作用会比较明显。

简单总结一下查询缓存的适用场景：

- 表数据修改不频繁、数据较静态。
- 查询（Select）重复度高。
- 查询结果集小于 1 MB。

对于一个更新频繁的系统来说，查询缓存缓存的作用是很微小的，在某些情况下开启查询缓存会带来性能的下降。

简单总结一下查询缓存不适用的场景：

- 表中的数据、表结构或者索引变动频繁
- 重复的查询很少
- 查询的结果集很大



### 缓存规则

- 查询缓存会将查询语句和结果集保存到内存（一般是 key-value 的形式，key 是查询语句，value 是查询的结果集），下次再查直接从内存中取。
- 缓存的结果是通过 sessions 共享的，所以一个 client 查询的缓存结果，另一个 client 也可以使用。
- SQL 必须完全一致才会导致查询缓存命中（大小写、空格、使用的数据库、协议版本、字符集等必须一致）。检查查询缓存时，MySQL Server 不会对 SQL 做任何处理，它精确的使用客户端传来的查询。
- 不缓存查询中的子查询结果集，仅缓存查询最终结果集。

查询缓存 `SELECT` 选项示例：

```sql
SELECT SQL_CACHE id, name FROM customer;# 会缓存
SELECT SQL_NO_CACHE id, name FROM customer;# 不会缓存
```

### 缓存机制中的内存管理

查询缓存是完全存储在内存中的，所以在配置和使用它之前，我们需要先了解它是如何使用内存的。

MySQL 查询缓存使用内存池技术，自己管理内存释放和分配，而不是通过操作系统。内存池使用的基本单位是变长的 block, 用来存储类型、大小、数据等信息。一个结果集的缓存通过链表把这些 block 串起来。block 最短长度为 `query_cache_min_res_unit`。

当服务器启动的时候，会初始化缓存需要的内存，是一个完整的空闲块。当查询结果需要缓存的时候，先从空闲块中申请一个数据块为参数 `query_cache_min_res_unit` 配置的空间，即使缓存数据很小，申请数据块也是这个，因为查询开始返回结果的时候就分配空间，此时无法预知结果多大。

分配内存块需要先锁住空间块，所以操作很慢，MySQL 会尽量避免这个操作，选择尽可能小的内存块，如果不够，继续申请，如果存储完时有空余则释放多余的。

但是如果并发的操作，余下的需要回收的空间很小，小于 `query_cache_min_res_unit`，不能再次被使用，就会产生碎片。

### MySQL 查询缓存的优缺点

**优点：**

- 查询缓存的查询，发生在 MySQL 接收到客户端的查询请求、查询权限验证之后和查询 SQL 解析之前。也就是说，当 MySQL 接收到客户端的查询 SQL 之后，仅仅只需要对其进行相应的权限验证之后，就会通过查询缓存来查找结果，甚至都不需要经过 Optimizer 模块进行执行计划的分析优化，更不需要发生任何存储引擎的交互。
- 由于查询缓存是基于内存的，直接从内存中返回相应的查询结果，因此减少了大量的磁盘 I/O 和 CPU 计算，导致效率非常高。

**缺点：**

- MySQL 会对每条接收到的 SELECT 类型的查询进行 Hash 计算，然后查找这个查询的缓存结果是否存在。虽然 Hash 计算和查找的效率已经足够高了，一条查询语句所带来的开销可以忽略，但一旦涉及到高并发，有成千上万条查询语句时，hash 计算和查找所带来的开销就必须重视了。
- 查询缓存的失效问题。如果表的变更比较频繁，则会造成查询缓存的失效率非常高。表的变更不仅仅指表中的数据发生变化，还包括表结构或者索引的任何变化。
- 查询语句不同，但查询结果相同的查询都会被缓存，这样便会造成内存资源的过度消耗。查询语句的字符大小写、空格或者注释的不同，查询缓存都会认为是不同的查询（因为他们的 Hash 值会不同）。
- 相关系统变量设置不合理会造成大量的内存碎片，这样便会导致查询缓存频繁清理内存。

### MySQL 查询缓存对性能的影响

在 MySQL Server 中打开查询缓存对数据库的读和写都会带来额外的消耗:

- 读查询开始之前必须检查是否命中缓存。
- 如果读查询可以缓存，那么执行完查询操作后，会查询结果和查询语句写入缓存。
- 当向某个表写入数据的时候，必须将这个表所有的缓存设置为失效，如果缓存空间很大，则消耗也会很大，可能使系统僵死一段时间，因为这个操作是靠全局锁操作来保护的。
- 对 InnoDB 表，当修改一个表时，设置了缓存失效，但是多版本特性会暂时将这修改对其他事务屏蔽，在这个事务提交之前，所有查询都无法使用缓存，直到这个事务被提交，所以长时间的事务，会大大降低查询缓存的命中

# =======================

## MySQL执行计划分析====

## 什么是执行计划？

**执行计划** 是指一条 SQL 语句在经过 **MySQL 查询优化器** 的优化会后，具体的执行方式。

执行计划通常用于 SQL 性能分析、优化等场景。通过 `EXPLAIN` 的结果，可以了解到如数据表的查询顺序、数据查询操作的操作类型、哪些索引可以被命中、哪些索引实际会命中、每个数据表有多少行记录被查询等信息。

## 如何获取执行计划？

MySQL 为我们提供了 `EXPLAIN` 命令，来获取执行计划的相关信息。

需要注意的是，`EXPLAIN` 语句并不会真的去执行相关的语句，而是通过查询优化器对语句进行分析，找出最优的查询方案，并显示对应的信息。

`EXPLAIN` 执行计划支持 `SELECT`、`DELETE`、`INSERT`、`REPLACE` 以及 `UPDATE` 语句。我们一般多用于分析 `SELECT` 查询语句，使用起来非常简单，语法如下：

```sql
EXPLAIN + SELECT 查询语句；
```

我们简单来看下一条查询语句的执行计划：

```sql
mysql> explain SELECT * FROM dept_emp WHERE emp_no IN (SELECT emp_no FROM dept_emp GROUP BY emp_no HAVING COUNT(emp_no)>1);
+----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+
| id | select_type | table    | partitions | type  | possible_keys   | key     | key_len | ref  | rows   | filtered | Extra       |
+----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+
|  1 | PRIMARY     | dept_emp | NULL       | ALL   | NULL            | NULL    | NULL    | NULL | 331143 |   100.00 | Using where |
|  2 | SUBQUERY    | dept_emp | NULL       | index | PRIMARY,dept_no | PRIMARY | 16      | NULL | 331143 |   100.00 | Using index |
+----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+
```

可以看到，执行计划结果中共有 12 列，各列代表的含义总结如下表：

| **列名**      | **含义**                                     |
| ------------- | -------------------------------------------- |
| id            | SELECT 查询的序列标识符                      |
| select_type   | SELECT 关键字对应的查询类型                  |
| table         | 用到的表名                                   |
| partitions    | 匹配的分区，对于未分区的表，值为 NULL        |
| type          | 表的访问方法                                 |
| possible_keys | 可能用到的索引                               |
| key           | 实际用到的索引                               |
| key_len       | 所选索引的长度                               |
| ref           | 当使用索引等值查询时，与索引作比较的列或常量 |
| rows          | 预计要读取的行数                             |
| filtered      | 按表条件过滤后，留存的记录数的百分比         |
| Extra         | 附加信息                                     |

## 如何分析 EXPLAIN 结果？

为了分析 `EXPLAIN` 语句的执行结果，我们需要搞懂执行计划中的重要字段。

id

SELECT 标识符，是查询中 SELECT 的序号，用来标识整个查询中 SELELCT 语句的顺序。

id 如果相同，从上往下依次执行。id 不同，id 值越大，执行优先级越高，如果行引用其他行的并集结果，则该值可以为 NULL。

select_type

查询的类型，主要用于区分普通查询、联合查询、子查询等复杂的查询，常见的值有：

- **SIMPLE**：简单查询，不包含 UNION 或者子查询。
- **PRIMARY**：查询中如果包含子查询或其他部分，外层的 SELECT 将被标记为 PRIMARY。
- **SUBQUERY**：子查询中的第一个 SELECT。
- **UNION**：在 UNION 语句中，UNION 之后出现的 SELECT。
- **DERIVED**：在 FROM 中出现的子查询将被标记为 DERIVED。
- **UNION RESULT**：UNION 查询的结果。

table

查询用到的表名，每行都有对应的表名，表名除了正常的表之外，也可能是以下列出的值：

- **`<unionM,N>`** : 本行引用了 id 为 M 和 N 的行的 UNION 结果；
- **`<derivedN>`** : 本行引用了 id 为 N 的表所产生的的派生表结果。派生表有可能产生自 FROM 语句中的子查询。 -**`<subqueryN>`** : 本行引用了 id 为 N 的表所产生的的物化子查询结果。

### type（重要）

查询执行的类型，描述了查询是如何执行的。所有值的顺序从最优到最差排序为：system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL

常见的几种类型具体含义如下：

- **system**：如果表使用的引擎对于表行数统计是精确的（如：MyISAM），且表中只有一行记录的情况下，访问方法是 system ，是 const 的一种特例。
- **const**：表中最多只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。
- **eq_ref**：当连表查询时，前一张表的行在当前这张表中只有一行与之对应。是除了 system 与 const 之外最好的 join 方式，常用于使用主键或唯一索引的所有字段作为连表条件。
- **ref**：使用普通索引作为查询条件，查询结果可能找到多个符合条件的行。
- **index_merge**：当查询条件使用了多个索引时，表示开启了 Index Merge 优化，此时执行计划中的 key 列列出了使用到的索引。
- **range**：对索引列进行范围查询，执行计划中的 key 列表示哪个索引被使用了。
- **index**：查询遍历了整棵索引树，与 ALL 类似，只不过扫描的是索引，而索引一般在内存中，速度更快。
- **ALL**：全表扫描。

possible_keys

possible_keys 列表示 MySQL 执行查询时可能用到的索引。如果这一列为 NULL ，则表示没有可能用到的索引；这种情况下，需要检查 WHERE 语句中所使用的的列，看是否可以通过给这些列中某个或多个添加索引的方法来提高查询性能。

### key（重要）

key 列表示 MySQL 实际使用到的索引。如果为 NULL，则表示未用到索引。

key_len

key_len 列表示 MySQL 实际使用的索引的最大长度；当使用到联合索引时，有可能是多个列的长度和。在满足需求的前提下越短越好。如果 key 列显示 NULL ，则 key_len 列也显示 NULL 。

rows

rows 列表示根据表统计信息及选用情况，大致估算出找到所需的记录或所需读取的行数，数值越小越好。

### Extra（重要）

这列包含了 MySQL 解析查询的额外信息，通过这些信息，可以更准确的理解 MySQL 到底是如何执行查询的。常见的值如下：

- **Using filesort**：在排序时使用了外部的索引排序，没有用到表内索引进行排序。
- **Using temporary**：MySQL 需要创建临时表来存储查询的结果，常见于 ORDER BY 和 GROUP BY。
- **Using index**：表明查询使用了覆盖索引，不用回表，查询效率非常高。
- **Using index condition**：表示查询优化器选择使用了索引条件下推这个特性。
- **Using where**：表明查询使用了 WHERE 子句进行条件过滤。一般在没有使用到索引的时候会出现。
- **Using join buffer (Block Nested Loop)**：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询。

这里提醒下，当 Extra 列包含 Using filesort 或 Using temporary 时，MySQL 的性能可能会存在问题，需要尽可能避免



# ==========================

## Mysql 性能优化

### 哪些方面可以做到性能优化？

- 为搜索字段创建索引
- 避免使用 Select *，列出需要查询的字段
- 垂直分割分表
- 选择正确的存储引擎

### 能用 MySQL 直接存储文件（比如图片）吗？

可以是可以，直接存储文件对应的二进制数据即可。不过，还是建议不要在数据库中存储文件，会严重影响数据库性能，消耗过多存储空间。

可以选择使用云服务厂商提供的开箱即用的文件存储服务，成熟稳定，价格也比较低。

也可以选择自建文件存储服务，实现起来也不难，基于 FastDFS、MinIO（推荐） 等开源项目就可以实现分布式文件服务。

**数据库只存储文件地址信息，文件由文件存储服务负责存储。**

### MySQL 如何存储 IP 地址？

可以将 IP 地址转换成整形数据存储，性能更好，占用空间也更小。

MySQL 提供了两个方法来处理 ip 地址

- `INET_ATON()`：把 ip 转为无符号整型 (4-8 位)
- `INET_NTOA()` :把整型的 ip 转为地址

插入数据前，先用 `INET_ATON()` 把 ip 地址转为整型，显示数据时，使用 `INET_NTOA()` 把整型的 ip 地址转为地址显示即可。

### 有哪些常见的 SQL 优化手段？

1. 确保正确的索引：使用合适的索引可以大大提高查询性能。根据查询的字段和表结构，创建适当的索引可以减少数据库的读取操作，从而加快查询速度。
2. 优化查询语句：确保编写高效的查询语句，避免使用复杂的子查询和嵌套查询。尽量使用简单的语句来获取所需的数据。
3. 避免使用SELECT *：只选择需要的字段，而不是使用SELECT *，以减少返回的数据量，提高查询效率。
4. 使用JOIN优化：在进行多表查询时，使用合适的JOIN语句可以避免产生笛卡尔积，提高查询性能。
5. 避免在索引列上使用函数：在WHERE子句中避免在索引列上使用函数，因为这会导致数据库无法充分利用索引，从而影响性能。
6. 避免使用大型文本字段进行排序和分组：对大型文本字段进行排序和分组可能会导致性能下降，尽量避免这样的操作。
7. 使用连接池：连接池可以减少连接数据库的开销，重复利用现有连接，提高数据库的性能。
8. 批量操作：尽量使用批量操作来替代循环单条处理数据，减少数据库的交互次数。
9. 分区表：对于大型表，可以使用分区表来将数据拆分为更小的逻辑片段，这样可以减少查询的范围，提高查询性能。

### 如何分析 SQL 的性能？

我们可以使用 `EXPLAIN` 命令来分析 SQL 的 **执行计划** 。执行计划是指一条 SQL 语句在经过 MySQL 查询优化器的优化会后，具体的执行方式。

`EXPLAIN` 并不会真的去执行相关的语句，而是通过 **查询优化器** 对语句进行分析，找出最优的查询方案，并显示对应的信息。

`EXPLAIN` 适用于 `SELECT`, `DELETE`, `INSERT`, `REPLACE`, 和 `UPDATE`语句，我们一般分析 `SELECT` 查询较多。

我们这里简单来演示一下 `EXPLAIN` 的使用。

`EXPLAIN` 的输出格式如下：

```sql
mysql> EXPLAIN SELECT `score`,`name` FROM `cus_order` ORDER BY `score` DESC;
+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+
| id | select_type | table     | partitions | type | possible_keys | key  | key_len | ref  | rows   | filtered | Extra          |
+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+
|  1 | SIMPLE      | cus_order | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 997572 |   100.00 | Using filesort |
+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+----------------+
1 row in set, 1 warning (0.00 sec)
```

各个字段的含义如下：

| **列名**      | **含义**                                     |
| ------------- | -------------------------------------------- |
| id            | SELECT 查询的序列标识符                      |
| select_type   | SELECT 关键字对应的查询类型                  |
| table         | 用到的表名                                   |
| partitions    | 匹配的分区，对于未分区的表，值为 NULL        |
| type          | 表的访问方法                                 |
| possible_keys | 可能用到的索引                               |
| key           | 实际用到的索引                               |
| key_len       | 所选索引的长度                               |
| ref           | 当使用索引等值查询时，与索引作比较的列或常量 |
| rows          | 预计要读取的行数                             |
| filtered      | 按表条件过滤后，留存的记录数的百分比         |
| Extra         | 附加信息                                     |

### MySQL时间类型数据存储建议

#### 不要用字符串存储日期

和绝大部分对数据库不太了解的新手一样，我在大学的时候就这样干过，甚至认为这样是一个不错的表示日期的方法。毕竟简单直白，容易上手。

但是，这是不正确的做法，主要会有下面两个问题：

1. 字符串占用的空间更大！
2. 字符串存储的日期效率比较低（逐个字符进行比对），无法用日期相关的 API 进行计算和比较。

#### Datetime 和 Timestamp 之间的抉择

Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型，可以精确到秒。他们两者究竟该如何选择呢？

下面我们来简单对比一下二者。

#### 时区信息

**DateTime 类型是没有时区信息的（时区无关）** ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。

**Timestamp 和时区有关**。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样。

### MySQL隐式转换造成索引失效

1. 当操作符**左右两边的数据类型不一致**时，会发生**隐式转换**。
2. 当 where 查询操作符**左边为数值类型**时发生了隐式转换，那么对效率影响不大，但还是不推荐这么做。
3. 当 where 查询操作符**左边为字符类型**时发生了隐式转换，那么会导致索引失效，造成全表扫描效率极低。
4. 字符串转换为数值类型时，非数字开头的字符串会转化为`0`，以数字开头的字符串会截取从第一个字符到第一个非数字内容为止的值为转化结果。

所以，我们在写 SQL 时一定要养成良好的习惯，查询的字段是什么类型，等号右边的条件就写成对应的类型。特别当查询的字段是字符串时，等号右边的条件一定要用引号引起来标明这是一个字符串，否则会造成索引失效触发全表扫描。

# =============================

## 场景题============

### 假如你所在的公司选择MySQL数据库作数据存储，一天五万条以上的增量，预计运维三年，你有哪些优化手段？

- 设计良好的数据库结构，允许部分数据冗余，尽量避免join查询，提高效率。
- 选择合适的表字段数据类型和存储引擎，适当的添加索引。
- MySQL库主从读写分离。
- 找规律分表，减少单表中的数据量提高查询速度。
- 添加缓存机制，比如Memcached，Apc等。
- 不经常改动的页面，生成静态页面。
- 书写高效率的SQL。比如 SELECT * FROM TABEL 改为 SELECT field_1, field_2, field_3 FROM TABLE。

### 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作

**大批量操作可能会造成严重的主从延迟**

主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况

**binlog 日志为 row 格式时会产生大量的日志**

大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因

**避免产生大事务操作**

大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。

特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批

### 一条sql执行慢的原因？如何优化？（接上面）

#### 原因：

**1、大多数情况下很正常，偶尔很慢，则有如下原因**

(1)、数据库在**刷新脏页**，例如 redo log 写满了需要同步到磁盘。
(2)、执行的时候，**遇到锁**，如表锁、行锁。
(3)、**sql写的烂**了

**2、这条 SQL 语句一直执行的很慢，则有如下原因**
(1)、没有用上**索引或则索引失效**：例如该**字段没有索引**；或则由于对字段进行**运算、函数操作导致无法用索引**。
(2)、有索引可能会走**全表扫描**

怎样判断是否走全表扫描：
索引区分度（索引的值不同越多，区分度越高），称为基数，而数据量大时不可能全部扫描一遍得到基数，而是**采样部分数据进行预测**，那有可能预测错了，导致走全表扫描。

#### 慢sql优化：

**索引+sql语句+数据库结构优化+优化器优化+架构优化**

**索引**：  
1、尽量覆盖索引，5.6支持索引下推
2、组合索引符合最左匹配原则
3、避免索引失效
4、再写多读少的场景下，可以选择普通索引而不要唯一索引
更新时，**普通索引可以使用change buffer进行优化，减少磁盘IO,将更新操作记录到change bufer**，等查询来了将数据读到内存再进行修改.
5、索引建立原则（一般建在where和order by，基数要大，区分度要高，不要过度索引，外键建索引）
**sql语句**：
1、分页查询优化
该方案适用于主键自增的表，可以把Limit查询转换成某个位置的查询。
select * from tb_sku where id>20000 limit 10;
2、优化insert语句

多条插入语句写成一条

在事务中插数据

数据有序插入（主键索引）
**数据库结构优化**：
1、将字段多的表分解成多个表
有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开。
2、对于经常联合查询的表，可以考虑建立中间表
**优化器优化**：
1、优化器使用MRR
**原理：**MRR 【Multi-Range Read】将ID或键值读到buffer排序，通过把「随机磁盘读」，转化为「顺序磁盘读」，减少磁盘IO，从而提高了索引查询的性能**。**
mysql **>set** optimizer_switch**=**'mrr=on';
explain 查看 Extra多了一个MRR
**explain select \* from** stu **where** age **between** 10 **and** 20；
**对于 Myisam，在去磁盘获取完整数据之前，会先按照 rowid 排好序，再去顺序的读取磁盘。**
**对于 Innodb，则会按照聚簇索引键值排好序，再顺序的读取聚簇索引。**
**磁盘预读：**请求一页的数据时，可以把后面几页的数据也一起返回，放到数据缓冲池中，这样如果下次刚好需要下一页的数据，就不再需要到磁盘读取（局部性原理）
**索引本身就是为了减少磁盘 IO，加快查询，而 MRR，则是把索引减少磁盘 IO 的作用，进一步放大**
**架构优化**：
读/写分离（主库写，从库读）

#### **总结：**

**1、先设置慢查询（my.ini或数据库命令）**
**2、分析慢查询日志**
**3、定位低效率sql（show processlist）**
**4、explain分析执行计划（是否索引失效，用到索引没，用了哪些）**
**5、优化（索引+sql语句+数据库结构优化+优化器优化+架构优化）**

**1、先设置慢查询（my.ini或数据库命令）**
方式一：修改配置文件 在 my.ini 增加几行: 主要是慢查询的定义时间（超过2秒就是慢查询），以及慢查询log日志记录（ slow_query_log）
方式二：通过MySQL数据库开启慢查询:set global slow_query_log=ON
**2、分析慢查询日志**

可以通过如下命令定位低效率

执行sql show processlist；

sql 可以用 explain 分析执行计划。

type：表示MySQL在表中找到所需行的方式，或者叫访问类型
type=ALL，全表扫描，MySQL遍历全表来找到匹配行
type=index，索引全扫描
type=range，索引范围扫描
type=eq_ref，唯一索引
type=NULL，MySQL不用访问表或者索引，直接就能够得到结果（性能最好）
possible_keys: 表示查询可能使用的索引
key: 实际使用的索引
key_len: 使用索引字段的长度
rows: 扫描行的数量

### 数据库高并发是我们经常会遇到的，你有什么好的解决方案吗？

- 在web服务框架中加入缓存。在服务器与数据库层之间加入缓存层，将高频访问的数据存入缓存中，减少数据库的读取负担。
- 增加数据库索引，进而提高查询速度。（不过索引太多会导致速度变慢，并且数据库的写入会导致索引的更新，也会导致速度变慢）
- 主从读写分离，让主服务器负责写，从服务器负责读。
- 将数据库进行拆分，使得数据库的表尽可能小，提高查询的速度。
- 使用分布式架构，分散计算压力。

### MySQL 单表不要超过 2000W 行，靠谱吗？

- MySQL 的**表数据是以页的形式存放**的，页在磁盘中**不一定是连续**的。
- **页的空间是 16K,** 并**不**是**所有的空间**都是用来存放数据的，会有一些固定的信息，如，页头，页尾，页码，校验码等等。
- 在 B+ 树中，叶子节点和非叶子节点的数据结构是一样的，区别在于，叶子节点存放的是实际的行数据，而非叶子节点存放的是**主键和页号**。
- **索引结构不会影响单表最大行数**，2000W 也只是推荐值，超过了这个值可能会**导致 B + 树层级更高**，影响查询性能。

# ========================

## MySQL 日志====

### MySQL的redo log，undo log，bin log都是干什么的

redo log是InnoDB引擎特有的，只记录该引擎中表的修改记录。binlog是MySQL的Server层实现的，会记录所有引擎对数据库的修改。

redo log是物理日志，记录的是在具体某个数据页上做了什么修改；binlog是逻辑日志，记录的是这个语句的原始逻辑。

redo log是循环写的，空间固定会用完；binlog是可以追加写入的，binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



以下是用户补充：

1、redolog记录修改内容（哪一页发生了什么变化），写于事务开始前，用于数据未落磁盘，但数据库挂了后的数据恢复
2、binlog记录修改SQL，写于事务提交时，可用于读写分离
3、undolog记录修改前记录，用于回滚和多版本并发控制

### 主从复制中涉及到哪三个线程？

主要涉及三个线程：binlog 线程、I/O 线程和 SQL 线程。

1. binlog 线程 ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。
2. I/O 线程 ：负责从主服务器上读取二进制日志，并写入从服务器的重放日志（Relay log）中。
3. SQL 线程 ：负责读取重放日志并重放其中的 SQL 语句。

### **执行一条 update 语句，期间发生了什么？**

```sql
UPDATE t_user SET name = 'xiaolin' WHERE id = 1;
```

查询语句的那一套流程，更新语句也是同样会走一遍：

- 客户端先通过连接器建立连接，连接器自会判断用户身份；
- 因为这是一条 update 语句，所以不需要经过查询缓存，但是表上有更新语句，是会把整个表的查询缓存清空的，所以说查询缓存很鸡肋，在 MySQL 8.0 就被移除这个功能了；
- 解析器会通过词法分析识别出关键字 update，表名等等，构建出语法树，接着还会做语法分析，判断输入的语句是否符合 MySQL 语法；
- 预处理器会判断表和字段是否存在；
- 优化器确定执行计划，因为 where 条件中的 id 是主键索引，所以决定要使用 id 这个索引；
- 执行器负责具体执行，找到这一行，然后更新。

更新语句的流程会涉及到 undo log（回滚日志）、redo log（重做日志） 、binlog （归档日志）这三种日志：

- **undo log（回滚日志）**：是 Innodb 存储引擎层生成的日志，实现了事务中的**原子性**，主要**用于事务回滚和 MVCC**。
- **redo log（重做日志）**：是 Innodb 存储引擎层生成的日志，实现了事务中的**持久性**，主要**用于掉电等故障恢复**；
- **binlog （归档日志）**：是 Server 层生成的日志，主要**用于数据备份和主从复制**；

### 为什么需要 undo log回滚日志

#### undo log作用1：

每次在事务执行过程中，都记录下回滚时需要的信息到一个日志里，那么在事务执行中途发生了 MySQL 崩溃后，就不用担心无法回滚到事务之前的数据，可以通过这个日志回滚到事务之前的数据

**实现事务回滚，保障事务的原子性**；undo log 是一种用于撤销回退的日志。在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚；在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作

一条记录的每一次更新操作产生的 undo log 格式都有一个 **roll_pointer 指针**和一个 **trx_id 事务id**：

- 通过 trx_id 可以知道该记录是被哪个事务修改的；
- 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为**版本链**

#### **undo log 作用2：**多版本控制MVCC

**通过 ReadView + undo log 实现 MVCC（多版本并发控制）**

对于「读提交」和「可重复读」隔离级别的事务来说，它们的快照读（普通 select 语句）是通过 Read View + undo log 来实现的，它们的区别在于创建 Read View 的时机不同：

- 「读提交」隔离级别是在**每个 select** 都会**生成一个新的 Read View**，也意味着，事务期间的多次读取同一条数据，前后两次读的数据可能会出现不一致，因为可能这期间另外一个事务修改了该记录，并提交了事务。
- 「可重复读」隔离级别是**启动事务时**生成一个 Read View，然后整个事务期间都在用这个 Read View，这样就保证了在事务期间读到的数据都是**事务启动前的记录**。

这两个隔离级别实现是通过「事务的 Read View 里的字段」和「记录中的两个隐藏列（trx_id 和 roll_pointer）」的比对，如果不满足可见行，就会**顺着 undo log 版本链**里找到满足其**可见性的记录**，从而**控制并发事务访问同一个记录**时的行为，这就叫 **MVCC（多版本并发控制）**

#### undo log 与多版本控制MVCC怎么结合使用的

Undo log和多版本控制MVCC是数据库中常用的并发控制技术，它们可以结合使用来实现高效的并发访问和事务隔离。

1. Undo Log（回滚日志）： Undo log是用于事务回滚的日志，记录了事务在执行过程中对数据所做的修改操作，以便在事务回滚时可以撤销这些操作，恢复到事务开始前的状态。每个事务在执行之前都会在Undo log中记录对应的修改操作，如果事务回滚，则根据Undo log中的记录逆向执行回滚操作。
2. MVCC（多版本并发控制）： MVCC是一种在读写冲突下实现高并发的并发控制技术，通过在每个数据项上维护多个版本（历史快照），使得读操作不会阻塞写操作，同时读操作可以读取到已提交的数据版本，从而提高数据库的并发性能。

结合使用Undo log和MVCC可以实现高效的并发控制和事务隔离。

**具体实现方式如下：**

1. 事务开始：当一个事务开始执行时，数据库会为该事务创建一个Undo log，并在执行过程中记录该事务对数据的修改操作。
2. 数据读取：当一个事务执行读操作时，数据库会根据MVCC机制来选择合适的数据版本。如果读操作的时间戳在某个版本的时间范围内（在事务开始之前或在事务开始之后但在事务提交之前），则该版本对于当前事务是可见的，数据读取会基于这个版本。
3. 数据写入：当一个事务执行写操作时，数据库会在数据项上创建一个新的版本，并在MVCC中维护该版本的时间戳。同时，数据库会将写操作记录到Undo log中，以便在事务回滚时可以撤销这些操作。
4. 事务提交：当一个事务提交时，数据库会将事务的修改操作持久化到数据库中，并将Undo log中的记录标记为已提交。这样其他事务在读取数据时就能够根据MVCC机制读取到已提交的数据版本。
5. 事务回滚：如果一个事务需要回滚，则数据库会根据Undo log中的记录逆向执行回滚操作，将数据恢复到事务开始前的状态。

通过结合使用Undo log和MVCC，数据库可以实现高并发的读写操作，同时保证事务的隔离性和原子性。这样可以提高数据库的性能和并发能力，同时保证数据的一致性和可靠性。

### 为什么需要 redo log重做日志

Buffer Pool 是基于内存的，而内存总是不可靠，万一**断电重启**，还没来得及**落盘的脏页数据**就会丢失。为了防止断电导致数据丢失的问题，当有**一条记录需要更新**的时候，InnoDB 引擎就会**先更新内存（同时标记为脏页）**，然后将本次对这个页的修改以 redo log 的形式记录下来，**这个时候更新就算完成**;InnoDB 引擎会在适当的时候，由**后台线程**将**缓存在 Buffer Pool 的脏页**刷新到磁盘里，这就是 **WAL （Write-Ahead Logging）技术**==预写日志

​       **WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上**。

#### 什么是 redo log重做日志？

redo log 是**物理日志**，记录了**某个数据页做了什么修改**，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志;当系统崩溃时，虽然脏页数据没有持久化，但是 **redo log 已经持久化**，接着 **MySQL 重启**后，可以根据 **redo log 的内容**，将**所有数据恢复到最新**的状态。

#### 被修改 Undo 页面，需要记录对应 redo log 吗？

- 需要的。开启事务后，InnoDB 层**更新记录前**，首先要记录相应的 undo log，如果是更新操作，需要把**被更新的列的旧值**记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。不过，**在内存修改该 Undo 页面后，需要记录对应的 redo log**。

####  Undo回滚日志和 redo log重做日志的区别？

这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于：

- redo log 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；
- undo log 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；

事务提交之**前发生了崩溃**，重启后会通过 undo log 回滚事务，事务提交之**后发生了崩溃**，重启后会通过 redo log 恢复事务，**实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失

#### redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？

- **写入 redo log重做日志** 的方式使用了**追加操作**， 所以磁盘操作是**顺序写**，而写入数据需要先找到**写入位置**，然后才写到磁盘，所以磁盘操作是**随机写**。磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小
- 可以说这是 WAL 预写日志技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是**先记录在日志上，然后在合适的时间再更新到磁盘上** 

#### 产生的 redo log 重做日志是直接写入磁盘的吗？

- 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 **I/O 操作**，而且**磁盘的运行速度远慢于内存**。
- redo log 也有自己的**缓存**—— **redo log buffer**，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在**持久化到磁盘**

#### redo log 什么时候刷盘？

**MySQL 正常关闭**时；

当 redo log buffer 中记录的**写入量大于** redo log buffer 内存空间的**一半**时，会触发落盘；

InnoDB 的**后台线程每隔 1 秒**，将 redo log buffer 持久化到磁盘。

每次**事务提交**时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘

- 单独执行一个**更新语句**的时候，InnoDB 引擎会自己**启动**一个**事务**，在**执行更新语句的过程**中，**生成的 redo log 先写入到 redo log buffer 中**，然后等**事务提交**的时候，再将缓存在 redo log buffer 中的 redo log 按**组的方式「顺序写」**到磁盘
- InnoDB 还提供了另外两种策略，由参数 `innodb_flush_log_at_trx_commit` 参数控制，可取的值有：0（不会主动触发）、1、2（写入**redo log 文件**，操作系统的文件缓存），默认值为 1（每次都刷）

#### innodb_flush_log_at_trx_commit 参数控制的是什么？

- 当设置该**参数为 0 时**，表示每次事务提交时 ，还是**将 redo log 留在 redo log buffer 中** ，该模式下在事务提交时不会主动触发写入磁盘的操作。
- 当设置该**参数为 1 时**，表示每次事务提交时，都**将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘**，这样可以保证 MySQL 异常重启之后数据不会丢失。
- 当设置该**参数为 2 时**，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log **写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘**，因为操作系统的文件系统中有个 Page Cache（如果你想了解 Page Cache，可以看[这篇 (opens new window)](https://xiaolincoding.com/os/6_file_system/pagecache.html)），Page Cache 是专门用来**缓存文件数据**的，所以写入「 redo log文件」意味着写入到了操作系统的**文件缓存**。

#### innodb_flush_log_at_trx_commit 为 0和 2 的时候，什么时候才将 redo log 写入磁盘？

- InnoDB 的**后台线程每隔** 1 秒：
- 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过**调用 `write()`** 写到操作系统的 Page Cache，然后**调用 `fsync()`** 持久化到磁盘。**所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失**;
- 针对参数 2 ：**调用 fsync**，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。**所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失**

#### 这三个参数的应用场景是什么？

这三个参数的数据安全性和写入性能的比较如下：

- 数据安全性：参数 1 > 参数 2 > 参数 0
- 写入性能：参数 0 > 参数 2> 参数 1

所以，数据安全性和写入性能是熊掌不可得兼的，**要不追求数据安全性，牺牲性能；要不追求性能，牺牲数据安全性**。

- 在一些对数据**安全性要求比较高**的场景中，显然 `innodb_flush_log_at_trx_commit` 参数需要设置为 1。
- 在一些可以容忍数据库崩溃时丢失 1s 数据的场景中，我们可以将该值设置为 0，这样可以明显地减少日志同步到磁盘的 I/O 操作。
- 安全性和性能折中的方案就是参数 2，虽然参数 2 没有参数 0 的性能高，但是数据安全性方面比参数 0 强，因为参数 2 只要操作系统不宕机，即使数据库崩溃了，也不会丢失数据，同时性能方便比参数 1 高。

#### redo log 文件写满了怎么办？（类似环形缓冲池）

-  InnoDB 存储引擎有 1 个**重做日志文件组**( redo log Group），重做日志文件组由 2 个 redo log 文件组成，每个 redo log File 的大小是固定且一致的，假设每个 redo log File 设置的上限是 1 GB，那么总共就可以记录 2GB 的操作。
-  重做日志文件组是以**循环写**的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形，InnoDB 用 write pos 表示 redo log 当前记录写到的位置，用 checkpoint 表示当前要擦除的位置.
-  redo log 是为了防止 Buffer Pool 中的脏页丢失而设计的，那么如果随着系统运行，Buffer Pool 的脏页刷新到了磁盘中，那么 redo log 对应的记录也就没用了，这时候我们擦除这些旧记录，以腾出空间记录新的更新操作。
-  如果 write pos 追上了 checkpoint，就意味着 **redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞**（*因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要*），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动，然后 **MySQL 恢复正常运行，继续执行新的更新操作**

### 为什么需要 binlog （主从复制和数据恢复）？

MySQL 在完成一条**更新操作**后，**Server 层**还会生成一条 **binlog**，等之后**事务提交**的时候，会将该**事物执行过程**中产生的所有 binlog 统一写 入 binlog 文件。binlog 文件是记录了**所有数据库表结构变更和表数据**修改的日志，**不会记录查询类**的操作

#### redo log 和 binlog 有什么区别？

*1、适用对象不同：*

- binlog 是 MySQL 的 Server 层实现的日志，**所有存储引擎**都可以使用；
- redo log 是 Innodb 存储引擎实现的日志；

*2、文件格式不同：*

- binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下：
  - STATEMENT：**每一条修改数据的 SQL** 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为**逻辑日志**），主从复制中 slave 端（从设备）再根据 **SQL 语句重现**。
  - ROW：**记录行数据**最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，**而在 STATEMENT 格式下只会记录一个 update 语句而已**；
  - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；
- redo log 是**物理日志**，记录的是在**某个数据页做了什么修改**，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

*3、写入方式不同：*

- binlog 是**追加写**，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是**全量的日志**。
- redo log 是**循环写**，日志空间大小是固定，全部写满就从头开始，保存**未被刷入磁盘的脏页日志**。

*4、用途不同：*

- binlog 用于**备份恢复、主从复制**；
- redo log 用于**掉电等故障恢复**。

#### 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？

- 不可以使用 redo log 文件恢复，只能使用 **binlog 文件恢复**。
- 因为 redo log 文件是**循环写**，是**会边写边擦除日志**的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。
- binlog 文件保存的是**全量的日志**，也就是保存了**所有数据变更的情况**，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据

#### binlog 组提交的原理

**MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**；引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 **commit 阶段**拆分为三个过程：

- **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入binlog 文件（不刷盘）；
- **sync 阶段**：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
- **commit 阶段**：**各个事务按顺序做 InnoDB commit 操作**；

上面的**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序，**第一个**进入队列的事务会成为 leader，leader领导所在队列的所有事务，**全权负责整队的操作**，完成后通知队内其他事务操作结束。对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再**锁住提交事务的整个过程**，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。

#### 提升 binlog 组提交的效果的措施

可以通过设置下面这两个参数来实现：

- `binlog_group_commit_sync_delay= N`，表示在**等待 N 微妙**后，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘，也就是将「 binlog 文件」持久化到磁盘。
- `binlog_group_commit_sync_no_delay_count = N`，表示如果队列中的**事务数达到 N 个**，就忽视binlog_group_commit_sync_delay 的设置，直接调用 fsync，将处于文件系统中 page cache 中的 **binlog 刷盘**。

### 主从复制是怎么实现？

**MySQL 的主从复制依赖于 binlog** ，也就是记录 MySQL 上的所有变化并以**二进制形式**保存在**磁盘**上。复制的过程就是将 **binlog 中的数据从主库传输到从库上**；这个过程一般是**异步**的，也就是主库上执行事务操作的线程**不会等待复制 binlog 的线程**同步完成。

#### MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：**主库写 binlog 日志**，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有**从库**上，每个从库把 binlog 写到**暂存日志**中。
- **回放 Binlog**：回放 binlog，并**更新**存储引擎中的数据。

#### 从库是不是越多越好？

- 从库数量增加，从库连接上来的 I/O 线程也比较多，**主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽**
- 在实际使用中，一个主库一般跟 **2～3 个从库**（1 套数据库，1 主 2 从 1 备主），这就是**一主多从的 MySQL 集群结构**

### binlog 什么时候刷盘？

- **事务执行**过程中，先把日志写到 binlog cache（Server 层的 cache），事务提交的时候，再把 **binlog cache 写到 binlog 文件**中。
- MySQL 给 binlog cache 分配了一片内存，**每个线程一个内存**，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果**超过**了这个参数规定的大小，就要**暂存到磁盘**
- 在**事务提交**的时候，执行器把 **binlog cache** 里的完整事务写入到 **binlog 文件**中，并清空 binlog cache。虽然每个线程有自己 binlog cache，但是最终都写到**同一个 binlog 文件**
- MySQL提供一个 **sync_binlog** 参数来控制数据库的 **binlog 刷到磁盘**上的**频率**：
  - sync_binlog = 0 的时候，表示**每次提交事务都只 write**，不 fsync，后续交由**操作系统决定何时**将数据持久化到磁盘；
  - sync_binlog = 1 的时候，表示**每次提交事务都会 write**，然后**马上执行 fsync**；
  - sync_binlog =N(N>1) 的时候，表示每次提交事务都 write，但**累积 N 个事务后才 fsync**。

### 具体更新一条记录流程？

具体更新一条记录 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 的流程如下:

1. 执行器负责具体执行，会调用存储引擎的接口，通过**主键索引树**搜索获取 id = 1 这一行记录：
   - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. **执行器**得到**聚簇索引记录**后，会看一下更新前的记录和更新后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的**执行更新记录**的操作；
3. **开启事务**， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 **Undo 页面**，不过在**内存修改**该 Undo 页面后，需要**记录对应的 redo log**。
4. InnoDB 层**开始更新记录**，会先更新内存（同时标记为**脏页**），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 **WAL 技术**，MySQL 的写操作并不是立刻写到磁盘上，而是**先写 redo 日志**，然后在合适的时间再将修改的行数据写到磁盘上。
5. 至此，一条记录更新完了。
6. 在一条**更新语句执行完成后**，然后开始**记录该语句对应的 binlog**，此时记录的 binlog 会被保存到 **binlog cache**，并没有刷新到硬盘上的 binlog 文件，在**事务提交**时才会统一将该事务运行过程中的**所有 binlog 刷新到硬盘**。
7. **事务提交**：
   - **prepare 阶段**：将 redo log 重做日志对应的事务状态设置为 prepare，然后将 **redo log 刷新到硬盘**；
   - **commit 阶段**：将 **binlog 刷新到磁盘**，接着调用引擎的提交事务接口，将 **redo log 事务状态设置为 commit**（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；
8. 至此，一条更新语句执行完成。

### 为什么需要 Buffer Pool缓冲池

MySQL 的**数据**都是存在磁盘中的，那么我们要更新一条记录的时候，得先**要从磁盘读取该记录，然后在内存中修改这条记录**。那修改完这条记录选择**缓存起来**好，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据。为此，Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

当**读取数据**时，如果数据存在于 Buffer Pool 缓冲池中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。

​        当**修改数据**时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为**脏页**（该页的内存数据和磁盘上的数据已经不一致），为了**减少磁盘I/O**，**不会立即**将脏页写入磁盘，后续由**后台线程选择一个合适的时机**将脏页写入到磁盘

#### Buffer Pool 缓冲池缓存什么？

在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页；**Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 **Undo 页**，插入缓存、自适应哈希索引、**锁信息**等等

#### Undo 页是记录什么？

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

#### 查询一条记录，就只需要缓冲一条记录吗？

不是的。

当我们查询一条记录时，InnoDB 是会把**整个页的数据加载**到 Buffer Pool 中，将页加载到 Buffer Pool 后，再通过**页里的「页目录」**去定位到某条具体的记录。

### 为什么需要两阶段提交？

事务提交后，redo log重做日志 和 binlog 归档日志都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。这是因为 redo log 重做日志**影响主库的数据**，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证**主从数据一致**。

两阶段提交其实是**分布式事务一致性协议**，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态；**两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。

#### 两阶段提交的过程是怎样的？

当**客户端执行 commit** 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，**分两阶段来完成 XA 事务的提交**，事务的提交过程有两个阶段，就是**将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog**，具体如下：

- **prepare 准备阶段**：将 **XID**（内部 XA 事务的 ID） 写入到 **redo log**，同时将 redo log 对应的**事务状态设置为 prepare**，然后将 **redo log 刷新到硬盘**；
- **commit 提交阶段**：把 **XID 写入到 binlog**，然后将 **binlog 刷新到磁盘**，接着调用引擎的**提交事务接口**，将 **redo log 状态**设置为 commit，此时**该状态并不需要持久化到磁盘**，只需要 **write** 到文件系统的 **page cache** 中就够了，因为只要 binlog 写磁盘成功，就算 **redo log 的状态还是 prepare 也没有关系**，一样会被认为事务已经执行成功；（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件，所以 commit 状态也是会刷盘的）；

#### 在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？

在 MySQL **重启后**会按顺序扫描 **redo log 文件**，碰到处于 **prepare 状态的 redo log**，就拿着 redo log 中的 **XID** 去 binlog 查看是否存在此 XID：

- **如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务**。对应时刻 A 崩溃恢复的情况。
- **如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务**。对应时刻 B 崩溃恢复的情况。

**两阶段提交是以 binlog 写成功为事务提交成功的标识**，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log **相同的 XID**

#### 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计?

- binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。所以，在**主库**上也要提交这个事务。采用这个策略，**主库和备库的数据**就保证了一致性。

#### 事务没提交的时候，redo log 会被持久化到磁盘吗？

- 会的。事务执行**中间过程**的 redo log 也是直接写在 **redo log buffer 中**的，这些缓存在 redo log buffer 里的 redo log 也会被**「后台线程」每隔**一秒一起持久化到磁盘。
- 如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？这种情况 mysql **重启会进行回滚操作**，因为事务没提交的时候，**binlog 是还没持久化到磁盘的**。
- **redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘**

#### 两阶段提交有什么问题？

- **磁盘 I/O 次数高**：对于“双1”配置，每个事务提交都会进行**两次 fsync（刷盘）**，一次是 redo log 刷盘，另一次是 binlog 刷盘。
- **锁竞争激烈**：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在**「多事务」**的情况下，却不能保证**两者的提交顺序**一致，因此，在两阶段提交的流程基础上，还需要**加一个锁来保证提交的原子性**，从而保证**多事务**的情况下，**两个日志的提交顺序一致**。

#### 为什么两阶段提交的磁盘 I/O 次数会很高？

- binlog 和 redo log 在内存中都**对应的缓存空间**，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到**磁盘的时机**分别由下面这两个参数控制。一般为了**避免日志丢失的风险**，会将这两个参数设置为 1：
  - 当 **sync_binlog = 1** 的时候，表示每次**提交事务**都会将 binlog cache 里的 binlog 直接持久到磁盘；
  - 当 **innodb_flush_log_at_trx_commit = 1** 时，表示每次**事务提交**时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘；
- 如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， **都会至少调用 2 次刷盘操作**，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。

#### 为什么锁竞争激烈？

- 在早期的 MySQL 版本中，通过使用 **prepare_commit_mutex 锁**来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。
- 通过加锁虽然完美地解决了**顺序一致性**的问题，但在**并发量较大**的时候，就会导致对锁的争用，性能不佳。



### MySQL 磁盘 I/O 很高，刷盘有什么优化的方法？

- 事务在**提交**的时候，需要将 **binlog 和 redo log 持久化到磁盘**，那么如果出现 MySQL 磁盘 I/O 很高的现象，可以通过控制以下参数，来 “**延迟” binlog 和 redo log 刷盘的时机**，从而降低磁盘 I/O 的频率：
  - 设置组提交的两个参数： binlog_group_commit_**sync_delay** 和 binlog_group_commit_sync_no_delay_count 参数，**延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数**。这个方法是基于“**额外的故意等待**”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要**系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘**。
  - 将 **sync_binlog 设置为大于 1** 的值（比较常见是 100~1000），表示**每次提交事务都 write**，但**累积 N 个事**务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，**主机掉电时会丢 N 个事务**的 binlog 日志。
  - 将 innodb_flush_log_at_trx_commit 设置为 **2**。表示**每次事务提交**时，都只是**缓存在 redo log buffer** 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的**文件系统中有个 Page Cache**，专门用来缓存文件数据的，所以**写入「 redo log文件」**意味着写入到了**操作系统的文件缓存**，然后交由**操作系统**控制持久化到磁盘的**时机。**但是这样做的风险是，**主机掉电**的时候会丢数据

# ===========================================

## 7.1 揭开 Buffer Pool 的面纱====

#### 为什么要有 Buffer Pool？

- 虽然说 MySQL 的数据是存储在磁盘里的，但是也不能每次都从磁盘里面读取数据，这样性能是极差的。要想提升查询性能，加个缓存就行了嘛。所以，当数据从磁盘中取出后，缓存内存中，下次查询同样的数据的时候，直接从内存中读取。为此，Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。
- 有了缓冲池后：
  - 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
  - 当修改数据时，首先是修改 Buffer Pool 中**数据所在的页**，然后将**其页设置为脏页**，最后由**后台线程将脏页写入到磁盘**。

##### Buffer Pool 有多大？

- Buffer Pool 是在 MySQL 启动的时候，向操作系统申请的一片连续的内存空间，默认配置下 Buffer Pool 只有 `128MB` 。
- 可以通过调整 `innodb_buffer_pool_size` 参数来设置 Buffer Pool 的大小，一般建议设置成可用**物理内存的 60%~80%**

##### Buffer Pool 缓存什么？

- InnoDB 会把存储的数据划分为若干个「页」，**以页作为磁盘和内存交互的基本单位**，一个页的默认大小为 **16KB**。因此，Buffer Pool 同样需要按「页」来划分。
- 在 MySQL 启动的时候，**InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的`16KB`的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页**。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。
  - MySQL 刚启动的时候，你会观察到使用的**虚拟内存空间**很大，而使用到的物理内存空间却很小，这是因为只有**这些虚拟内存被访问后**，操作系统才**会触发缺页中断，接着将虚拟地址和物理地址建立映射关系**。
- Buffer Pool 除了**缓存「索引页」和「数据页」**，还包括了 **undo 页，**插入缓存、自适应哈希索引、锁信息等等。为了更好的管理这些在 Buffer Pool 中的缓存页，InnoDB 为每一个缓存页都创建了一个**控制块**，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。**控制块也是占有内存空间**的，它是放在 Buffer Pool 的最前面，接着才是缓存页

##### 为什么会有碎片空间呢？

- 每一个控制块都对应一个缓存页，那在分配足够多的控制块和缓存页后，可能剩余的那点儿空间不够一对控制块和缓存页的大小，自然就用不到喽，这个用不到的那点儿内存空间就被称为碎片

##### 查询一条记录，就只需要缓冲一条记录吗？

不是的。当我们查询一条记录时，InnoDB 是会把**整个页的数据**加载到 Buffer Pool 中，因为，通过**索引只能定位到磁盘中的页**，而不能定位到页中的一条记录。将页加载到 Buffer Pool 后，再通过**页里的页目录**去定位到某条具体的记录。

#### 如何管理 Buffer Pool？

##### 如何管理空闲页？

- Buffer Pool 是一片连续的内存空间，当 MySQL 运行一段时间后，这片连续的内存空间中的缓存页既有空闲的，也有被使用的。那当我们从磁盘读取数据的时候，总不能通过遍历这一片连续的内存空间来找到**空闲的缓存页**吧，这样效率太低了。
- 为了能够快速找到空闲的缓存页，可以使用**链表结构**，将空闲缓存页的**「控制块」**作为链表的节点，这个链表称为 **Free 链表**（空闲链表）。
- Free 链表上除了有控制块，还有一个头节点，该头节点包含链表的**头节点地址，尾节点地址，以及当前链表中节点的数量**等信息。Free 链表节点是一个一个的控制块，而每个控制块包含着对应缓存页的地址，所以相当于 **Free 链表节点都对应一个空闲的缓存页**。
- 有了 Free 链表后，每当需要**从磁盘中加载一个页到 Buffer Pool** 中时，就从 **Free链表中取一个空闲的缓存页**，并且把该缓存页对应的控制块的信息填上，然后把**该缓存页**对应的控制块从 Free 链表中移除

##### 如何管理脏页？

- 设计 Buffer Pool 除了能提高读性能，还能提高写性能，也就是更新数据的时候，不需要每次都要写入磁盘，而是将 Buffer Pool 对应的缓存页标记为**脏页**，然后再由**后台线程将脏页写入到磁盘**。
- 那为了能快速知道哪些缓存页是脏的，于是就设计出 **Flush 链表**，它跟 Free 链表类似的，链表的节点也是控制块，区别在于 **Flush 链表的元素都是脏页**。

##### 如何提高缓存命中率？

- Buffer Pool 的大小是有限的，对于一些**频繁访问的数据**我们希望可以一直留在 Buffer Pool 中，而一些很少访问的数据希望可以在某些时机可以淘汰掉，从而保证 Buffer Pool 不会因为满了而导致无法再缓存新的数据，同时还能保证**常用数据留在 Buffer Pool 中**。
- 最容易想到的就是 **LRU**（Least recently used）算法。该算法的思路是，**链表头部的节点是最近使用的**，而链表末尾的节点是最久没被使用的。那么，当空间不够了，就**淘汰最久没被使用的节点**，从而腾出空间。
- 简单的 LRU 算法的实现思路是这样的：
  - 当访问的**页在 Buffer Pool** 里，就直接把该页对应的 LRU 链表节点移动到链表的头部。
  - 当访问的页不在 Buffer Pool 里，除了要把页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点
- Buffer Pool 里有三种页和链表来管理数据。
  - Free Page（**空闲页**），表示此页未被使用，位于 Free 链表；
  - Clean Page（**干净页**），表示此页已被使用，但是页面未发生修改，位于LRU 链表。
  - Dirty Page（**脏页**），表示此页「已被使用」且「已经被修改」，其数据和磁盘上的数据已经不一致。当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。脏页同时存在于 LRU 链表和 Flush 链表。
- **简单的 LRU 算法**并没有被 MySQL 使用，因为简单的 LRU 算法无法避免下面这两个问题：
  - 预读失效；
  - Buffer Pool 污染；

#### 什么是预读失效？

- MySQL 在**加载数据页**时，会提前把它**相邻的数据页**一并加载进来，目的是为了**减少磁盘 IO**。但是可能这些**被提前加载进来的数据页，并没有被访问**，相当于这个预读是白做了，这个就是**预读失效**
- 如果使用简单的 LRU 算法，就会把**预读页放到 LRU 链表头部**，而当 Buffer Pool空间不够的时候，还需要把末尾的页淘汰掉。如果这些预读页如果一直不会被访问到，就会出现一个很奇怪的问题，不会被访问的预读页却**占用了 LRU 链表前排**的位置，而**末尾淘汰的页**，可能是频繁访问的页，这样就大大降低了缓存命中率。

##### 怎么解决预读失效而导致缓存命中率降低的问题？

- 要避免预读失效带来影响，最好就是**让预读的页停留在 Buffer Pool 里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长**。
- MySQL 改进了 LRU 算法，将 LRU 划分了 2 个区域：**old 区域 和 young 区域**。young 区域在 LRU 链表的前半部分，old 区域则是在后半部分；old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pc` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。

#### 什么是 Buffer Pool 污染？

- 当某一个 SQL 语句**扫描了大量的数据**时，在 Buffer Pool 空间比较有限的情况下，可能会将 **Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了**，等这些热数据又被再次访问的时候，由于**缓存未命中，就会产生大量的磁盘 IO**，MySQL 性能就会急剧下降，这个过程被称为 **Buffer Pool 污染**。

##### 怎么解决出现 Buffer Pool 污染而导致缓存命中率下降的问题？

- **全表扫描**的查询，很多缓冲页其实只会被访问一次，但是它却只因为被访问了一次而进入到 young 区域，从而导致热点数据被替换了。LRU 链表中 young 区域就是热点数据，只要我们**提高进入到 young 区域的门槛**，就能有效地保证 young 区域里的热点数据不会被替换掉。
- MySQL 是这样做的，进入到 young 区域条件增加了一个**停留在 old 区域的时间判断**。在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：
  - 如果后续的访问时间与第一次访问的时间**在某个时间间隔内**，那么**该缓存页就不会被从 old 区域移动到 young 区域的头部**；
  - 如果后续的访问时间与第一次访问的时间**不在某个时间间隔内**，那么**该缓存页移动到 young 区域的头部**；
- **只有同时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部**，这样就解决了 Buffer Pool 污染的问题 。
- 另外，MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点**频繁移动到头部**。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 **3/4被访问了才会**

#### 脏页什么时候会被刷入磁盘？

- 脏页需要被刷入磁盘，保证**缓存和磁盘数据一致**，但是若每次修改数据都刷入磁盘，则性能会很差，因此一般都会在**一定时机进行批量刷盘**。
- InnoDB 的更新操作采用的是 Write Ahead Log 策略，**即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力**。下面几种情况会触发脏页的刷新：
  - 当 **redo log 日志满**了的情况下，会主动触发脏页刷新到磁盘；
  - Buffer Pool 空间不足时，需要将一部分**数据页淘汰掉**，如果淘汰的是脏页，需要**先将脏页同步到磁盘**；
  - MySQL 认为**空闲时**，**后台线程回定期**将适量的脏页刷入到磁盘；
  - MySQL **正常关闭**之前，会把所有的**脏页刷入到磁盘**
- 在开启了**慢 SQL 监控**后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为**脏页在刷新到磁盘时导致数据库性能抖动**。如果在很短的时间出现这种现象，就需要**调大 Buffer Pool 空间或 redo log 日志的大小**。