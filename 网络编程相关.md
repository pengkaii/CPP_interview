## 网络编程相关

#### 服务器单机理论的最大并发数是多少？

分客户端和服务端考虑。记住一个前提：一个 TCP 连接是由一个四元组决定的（源 IP、源端口、目标 IP 、目标端口）。

1. **对于客户端**

   目标 IP 和目标端口是确定的，而对于一个网卡的主机来说，源 IP 也是确定的，所以客户端的连接数据是由端口数目决定的。端口号是 16 位的，所以端口数为 2^{16} = 65536，但是 Linux 对可使用的端口范围有限制，具体可以通过以下命令查看：

   ```bash
   cat /proc/sys/net/ipv4/ip_local_port_range 
   1024 65000
   ```

2. **对于服务端**

   目标端口和目标 IP 是确定的，所以对于服务端来说，最大的连接数就是源 IP 数乘以源端口数，也就是大约 $$2^{32} \cdot 2^{16} = 2^{48}$$（IPv4 为32位，端口号为16位）。但实际上服务器肯定承载不了这么大的连接数，会回到两方面的限制：

   - **文件描述符**， Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；
   - **系统内存**，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；



####  IP 服务器监听一个端口， TCP 最大连接数

- 服务器通常固定在某个本地端口上监听，等待客户端的连接请求。因此，客户端 IP 和 端口是可变的
- 对 IPv4，客户端的 IP 数最多为 `2` 的 `32` 次方，客户端的端口数最多为 `2` 的 `16` 次方，**也就是服务端单机最大 TCP 连接数，约为 `2` 的 `48` 次方。**
- 当然，服务端最大并发 TCP 连接数远不能达到理论上限，会受以下因素影响：
  - **文件描述符限制**，每个 TCP 连接都是一个文件，如果文件描述符被占满了，会发生 too many open files。Linux 对可打开的文件描述符的数量分别作了三个方面的限制：**系统级**，当前系统可打开的最大数量；**用户级**，指定用户可打开的最大数量；**进程级**，单个进程可打开的最大数量
  - **内存限制**，每个 TCP 连接都要占用一定内存，操作系统的内存是有限的，如果内存资源被占满后，会发生 OOM。

#### 浏览器对同Host 建立 TCP 连接数量有没有限制

**有。Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。**

如果图片都是 HTTPS 连接并且在同一个域名下，那么浏览器在 SSL 握手之后会和服务器商量能不能用 HTTP2，如果能的话就使用 Multiplexing 功能在这个连接上进行多路传输。不过也未必会所有挂在这个域名的资源都会使用一个 TCP 连接去获取，但是可以确定的是 Multiplexing 很可能会被用到。

如果用不了 HTTP2 ，只能使用 HTTP/1.1。浏览器就会在一个 HOST 上建立多个 TCP 连接，连接数量的最大限制取决于浏览器设置，这些连接会在空闲的时候被浏览器用来发送新的请求。





#### Reactor 和 Proactor 模式区别

##### Reactor 模式

Reactor 模式也叫 `Dispatcher` 模式，即 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。Netty、Nginx 和 MemCache 用的都是这种方案。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

- Reactor 的数量可以只有一个，也可以有多个；
- 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；

主要有三种形式：单 Reactor 单线程 -> 单 Reactor 多线程 -> 多 Reactor 多线程

1. **单 Reactor 单线程**

   Redis 用的就是这种模式。

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png" alt="img" style="zoom: 50%;" />

   - Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发（根据事件类型）；
   - 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
   - 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
   - Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

   缺点：因为只有一个进程，**无法充分利用 多核 CPU 的性能**；Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

2. **单 Reactor 多线程**

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" style="zoom:50%;" />

   - Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
   - 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

   单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。

   「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**。

   

3. **多 Reactor 多线程**

   Netty、Nginx 和 MemCache都用的这个框架

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" style="zoom:50%;" />

   - 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；
   - 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。
   - 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
   - Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。



##### Proactor

Reactor 是非阻塞同步网络模式，而 **Proactor 是异步网络模式**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/Proactor.png" alt="img" style="zoom:50%;" />

- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核；
- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；
- Handler 完成业务处理；

在 Linux 下的异步 I/O 是不完善的， `aio` 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，**网络编程中的 socket 是不支持的**，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。



##### 区别

- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。



#### 负载均衡算法

**轮询**，只能适用与每个节点的数据都是相同的场景，访问任意节点都能请求到数据。但是**不适用分布式系统**，因为分布式系统意味着数据水平切分到了不同的节点上，访问数据的时候，一定要寻址存储该数据的节点。

**哈希算法**，也就是 KV 存储，虽然能建立数据和节点的映射关系，但是每次在节点数量发生变化的时候，最坏情况下所有数据都需要迁移，这样太麻烦了，所以不适用节点数量变化的场景。

**一致性哈希算法**，为了减少迁移的数据量，将「存储节点」和「数据」都映射到一个首尾相连的**哈希环上**，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。但是一致性哈希算法不能够均匀的分布节点，会出现大量请求都集中在一个节点的情况，在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。

**虚拟节点**，对一个真实节点做多个副本。不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。可以会提高节点的均衡度，还会提高系统的稳定性。所以，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。



#### 网络通信时是否要字节序转换

相同字节序的平台在进行网络通信时可以不进行字节序转换，但是跨平台进行网络数据通信时必须进行字节序转换。网络字节序是大端。当一个小端序计算机发送数据到一个大端序计算机时，接收方需要将接收到的数据转换成其自己的字节序格式。网络编程中函数如`htonl()`、`ntohl()`、`htons()`、`ntohs()`等，它们用于在主机字节序和网络字节序之间进行转换。

**UDP/TCP/IP协议规定：把接收到的第一个字节当作高位字节看待，存放到低地址；**

在网络通信中，为了确保不同架构的计算机能正确地交换数据，确实可能需要进行字节序转换。

1. **大端序（Big Endian）**：数据的高字节保存在内存的低地址中，而数据的低字节保存在内存的高地址中。举个例子，如果我们有一个16位的数`0x1234`，在大端序计算机的内存中，它将被存储为`0x12 0x34`。

2. **小端序（Little Endian）**：数据的低字节保存在内存的低地址中，而数据的高字节保存在内存的高地址中。使用同样的例子，`0x1234`将被存储为`0x34 0x12`。

   

#### 网络通信的开销在哪，如何优化

**开销**

网络通信的主要开销在**协议栈**。在传统的内核协议栈中，当有新的数据包到来时，网卡会通过硬件中断通知协议栈，内核的网卡驱动负责处理这个硬件的中断，将数据包从网卡拷贝到内核开辟的缓冲区中。**协议栈的主要处理开销分为中断处理、内存拷贝、系统调用和文件处理四个方面**。

**优化**

1. **中断优化**

   - **在流量小时使用中断模式，当遇到大流量时切换为轮询模式**：中断处理方式在低速网络I/O场景下非常有效，高速网络中，随着网络 I/O速率的不断上升，网卡面对大量高速数据分组引发频繁的中断，中断引起的上下文切换开销将变得不可忽视，造成较高的时延，并引起吞吐量下降。

2. **内存复制开销优化**

   - **零拷贝技术**：将网卡数据通过 DMA 直接复制到内存（不需要 CPU 参与）

3. **上下文切换开销优化**

   - **用户态协议栈**：直接将协议栈移动到应用层实现，避免昂贵的上下文切换开销， 如mTCP、 lwIP等方案。
   - **批处理系统调用**：批处理系统调用平摊开销，MegaPipe、FlexSC、 VOS均使用批量系统调用来减小开销。

4. **文件系统开销**

   socket是一种文件抽象，Linux 为了实现统一文件管理，通过虚拟文件系统（virtual file system，**VFS**）为套接字绑定了一系列对应的数据结构如inode、dentry等。这些数据结构对于网络通信的用处不大，是额外的开销。

   - **自定义轻量级 Socket**：在**用户态重新定义实现socket**结构体。
   - **简化 VFS 的 socket 实现**：继承VFS的socket实现，但是简化掉inode与 dentry 的初始化与销毁过程，抛弃其中的锁。





#### TCP 抓包

- 常用工具：**tcpdump 和 Wireshark**，用 tcpdump 在 Linux 服务器上抓包，接着把抓包的文件拖出到 Windows 电脑后，用 Wireshark 可视化分析。

- 为了模拟客户端收不到服务端第二次握手 SYN、ACK 包，我的做法是在客户端**加上防火墙限制**，直接粗暴的把来自服务端的数据都丢弃，防火墙的配置如下：

  添加 iptables 限制后， tcpdump 是否能抓到包 ，这要看添加的 iptables 限制条件：

  - 如果添加的是 `INPUT` 规则，则可以抓得到包
  - 如果添加的是 `OUTPUT` 规则，则抓不到包

  网络包进入主机后的顺序如下：

  - 进来的顺序 Wire -> NIC -> **tcpdump -> netfilter/iptables**
  - 出去的顺序 **iptables -> tcpdump** -> NIC -> Wire

  





如何使用`tcpdump`来捕获网络数据包，并解释捕获到的数据包的一些基本信息。

##### 使用tcpdump捕获网络数据包

1. 打开终端。

2. 输入以下命令来捕获经过网络接口（假设为eth0）的前10个TCP数据包，并将输出详细信息：

   ```c++
   sudo tcpdump -i eth0 -c 10 -nn -X tcp
   ```

   说明:

   - `-i eth0`: 指定要监听的网络接口为eth0。
   - `-c 10`: 指定只捕获10个数据包。
   - `-nn`: 不对地址和端口进行名字解析。
   - `-X`: 以十六进制和ASCII形式打印每个数据包。
   - `tcp`: 过滤条件，只捕获TCP数据包。

3. 发送或接收网络请求以产生网络流量。

示例解释

以下是一个捕获到的TCP数据包的示例输出（已简化和格式化以提高可读性）：

```c++
16:22:38.123456 IP 192.168.1.2.55555 > 93.184.216.34.80: Flags [S], seq 123456789, win 65535, options [mss 1460], length 0
	0x0000:  4500 003c 1c46 4000 4006 ac10 01f5  cdee
	0x0010:  02e3 5d8a d431 0050 0755 1e61 0000 0000
	0x0020:  a002 ffff fe30 0000 0204 05b4 0000
```

- 第一行包含时间戳、IP版本、源地址和端口、目的地址和端口、TCP标志（本例中为SYN[S]标志，表示建立连接）、序列号、窗口大小、TCP选项（本例中为最大段大小MSS）和数据长度。
- 接下来的几行是数据包的十六进制和ASCII表示。这些数据包括以太网帧头、IP头、TCP头和可能的数据负载。

在这个例子中，我们捕获到了一个TCP SYN包，这是TCP三次握手过程的第一步，用于建立TCP连接。这个包从源地址`192.168.1.2`的端口`55555`发送到目的地址`93.184.216.34`的端口`80`。



###### tcpdump

`tcpdump` 是一个轻量级的命令行工具，主要用于在Unix/Linux系统中捕获和分析网络数据包。

使用实例：

1. 捕获所有经过eth0网络接口的数据包：

   ```
   tcpdump -i eth0
   ```

2. 捕获所有经过eth0网络接口，并且目的或源地址是192.168.1.1的TCP数据包：

   ```
   tcpdump -i eth0 tcp and host 192.168.1.1
   ```

3. 捕获所有经过eth0网络接口的源地址是192.168.1.1且目的端口是80的数据包：

   ```
   tcpdump -i eth0 src host 192.168.1.1 and dst port 80
   ```

4. 捕获并保存数据包到文件：

   ```
   tcpdump -i eth0 -w /path/to/outputfile
   ```



###### Wireshark

`Wireshark` 是一个具有图形用户界面的网络协议分析工具，提供了强大的数据包捕获和分析功能。

使用实例：

1. 打开Wireshark。
2. 在主界面中选择你想要捕获数据包的网络接口。
3. 点击“开始捕获”按钮开始捕获数据包。
4. 使用“显示过滤器”功能来过滤特定的数据包。例如，输入`tcp.port == 80`来过滤所有目的或源端口为80的TCP数据包。
5. 选择一个特定的数据包，Wireshark将显示其详细信息，包括帧结构、协议信息和数据内容。
6. 如果需要，可以保存捕获的数据包到文件，或者打开之前保存的数据包文件进行分析。



#####  TCP 第一次握手 SYN 丢包

本次实验用了两台虚拟机，一台作为服务端，一台作为客户端，它们的关系如下：

![实验环境](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/21.jpg)

- 客户端和服务端都是 CentOs 6.5 Linux，Linux 内核版本 2.6.32
- 服务端 192.168.12.36，apache web 服务
- 客户端 192.168.12.37

为了模拟 TCP 第一次握手 SYN 丢包的情况，我是在拔掉服务器的网线后，立刻在客户端执行 curl 命令：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/22.jpg)

其间 tcpdump 抓包的命令如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/23.jpg)

过了一会， curl 返回了超时连接的错误：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/24.jpg)

从 `date` 返回的时间，可以发现在超时接近 1 分钟的时间后，curl 返回了错误。

接着，把 **tcp_sys_timeout.pcap 文件用 Wireshark 打开分析**，显示如下图：

![SYN 超时重传五次](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/25.jpg)

从上图可以发现， 客户端发起了 SYN 包后，一直没有收到服务端的 ACK ，所以一直超时重传了 5 次，并且每次 RTO 超时时间是不同的：

- 第一次是在 1 秒超时重传
- 第二次是在 3 秒超时重传
- 第三次是在 7 秒超时重传
- 第四次是在 15 秒超时重传
- 第五次是在 31 秒超时重传

可以发现，每次超时时间 RTO 是**指数（翻倍）上涨的**，当超过最大重传次数后，客户端不再发送 SYN 包。

在 Linux 中，第一次握手的 `SYN` 超时重传次数，是如下内核参数指定的：

```bash
$ cat /proc/sys/net/ipv4/tcp_syn_retries
5
```

`tcp_syn_retries` 默认值为 5，也就是 SYN 最大重传次数是 5 次。

接下来，我们继续做实验，把 `tcp_syn_retries` 设置为 2 次：

```bash
$ echo 2 > /proc/sys/net/ipv4/tcp_syn_retries
```

重传抓包后，用 Wireshark 打开分析，显示如下图：

![SYN 超时重传两次](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/26.jpg)

实验小结

通过实验一的实验结果，我们可以得知，当客户端发起的 TCP 第一次握手 SYN 包，在超时时间内没收到服务端的 ACK，就会在超时重传 SYN 数据包，每次超时重传的 RTO 是翻倍上涨的，直到 SYN 包的重传次数到达 `tcp_syn_retries` 值后，客户端不再发送 SYN 包。

![SYN 超时重传](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-Wireshark/27.jpg)



#### 什么是大端小端，如何判断

**小端**：**低**的有效字节存储在**低的**存储器地址。小端一般为主机字节序；常用的X86结构是小端模式。很多的ARM，DSP都为小端模式。

**大端**：**高**的有效字节存储在**低的**存储器地址。大端为网络字节序；KEIL C51则为大端模式。

**判断方法：**

使用union，因为联合体变量从低地址存储。共用体的各个成员是共用一段内存的。1 是数据的低位，如果 1 被存储在 data 的低字节，就是小端模式，这个时候 data.ch 的值也是 1。如果 1 被存储在 data 的高字节，就是大端模式，这个时候 data.ch 的值就是 0。

<img src="https://img-blog.csdn.net/20170305103408849" alt="img" style="zoom: 67%;" />

```c++
#include <stdio.h>
int main() {
    union {
        int n;
        char ch;  // 可以看作 n 的低地址
    } data;
    data.n = 0x00000001;  //也可以直接写作 data.n = 1;
    if(data.ch == 1) {
        printf("Little-endian\n");
    }
    else {
        printf("Big-endian\n");
    }
    return 0;
}
```

##### 为什么要区分大端与小端

- cpu硬件设计中，为了方便计算一般都是从低位计算的，小端存储方式有利于低位计算。
- **网络通信**：
  - 网络协议通常定义了一种固定的字节序（通常是大端字节序）。如果一台计算机的字节序与网络协议不同，它就需要在发送和接收数据时进行字节序转换。不正确地处理字节序会导致通信错误和数据损坏。
- **数据交换**：
  - 当不同字节序的系统交换数据时，如果不进行适当的转换，接收方可能会错误地解释数据。例如，在文件格式和数据库中，数据的字节序需要清晰定义，以确保所有系统都能正确读取数据。









#### 数据包被 iptables 防火墙封禁，能用tcpdump 抓到包？

可以的，因为 **tcpdump 是工作在协议栈之前，所以可以抓到主机收到的包。**

网络包进入主机后的顺序如下：

- 进来的顺序  网卡 eth0-> 网络中断 -> ***tcpdump -> netfilter/iptables***
- 出去的顺序  ***netfilter/iptables -> tcpdump*** -> 网络中断 ->网卡 eth0



Iptables 是基于 Netfilter 框架实现的报文选择系统，其可以用于报文的过滤、网络地址转换和报文修改等功能。Netfilter 有五个节点：

- `NF_IP_PRE_ROUTING`: 位于路由之前，报文一致性检查之后（报文一致性检查包括: 报文版本、报文长度和checksum）。
- `NF_IP_LOCAL_IN`: 位于报文经过路由之后，并且目的是本机的。
- `NF_IP_FORWARD`：位于在报文路由之后，目的地非本机的。
- `NF_IP_LOCAL_OUT`: 由本机发出去的报文，并且在路由之前。
- `NF_IP_POST_ROUTING`: 所有即将离开本机的报文。

```text
--->[NF_IP_PRE_ROUTING]--->[ROUTE]--->[NF_IP_FORWARD]--->[NF_IP_POST_ROUTING]--->
                              |                        ^
                              |                        |
                              |                     [ROUTE]
                              v                        |
                       [NF_IP_LOCAL_IN]        [NF_IP_LOCAL_OUT]
                              |                        ^
                              |                        |
                              v                        |
```









## 系统调用相关

### 系统调用的底层实现

#### epoll 是同步还是异步的？

**从 I/O 层面来看的话， epoll 是同步的**，因为 `epoll()` 本身就需要阻塞等待操作系统返回值。

**从消息处理层面来看， epoll 是异步的**，因为每个调用 `epoll()` 的事件，不需要阻塞等待 epoll 的返回值，只需要 epoll 返回后，通知事件即可。



#### 介绍一下 I/O 多路复用

1. I/O 多路复用技术是一种使用一个进程来同时监听多个 I/O 流状态的技术，一旦检测到某个文件描述符发生了我们关心的事件，操作系统就会通知程序进行相应的处理。它使得程序能够有效地管理多个输入和输出操作，而不是为每个I/O操作分配一个单独的线程。这种方式在处理大量并发连接时特别有用，比如在网络服务器中。Linux 下实现 I/O 复用的系统调用主要有 select、poll 和 epoll 。

2. **select**，将已连接的 Socket 都放到一个**文件描述符集合** fd_set（可以理解为位图），然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

3. **poll**，poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png" alt="img" style="zoom: 50%;" />

4. **epoll**，epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)` ，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。epoll 使用事件驱动的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。



####  epoll 的事件触发模式

epoll 支持两种事件触发模式，分别是**边缘触发（*edge-triggered，ET*）**和**水平触发（*level-triggered，LT*）**。

1. **ET模式**

   使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

2. **LT模式**

   使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

一般来说，边缘触发的效率比水平触发的效率要高，因为**边缘触发可以减少 epoll_wait 的系统调用次数**，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

还有一个问题，在Linux下，select() 可能会将一个 socket 文件描述符报告为 "准备读取"，而后续的读取块却没有。此时如果使用非阻塞 I/O 则会造成程序阻塞在读的地方。（考虑这种情况：当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。）



#### 五大 IO 模型是哪些

阻塞 IO、非阻塞 IO、多路复用 IO、信号驱动式 IO、异步 IO

**阻塞 IO 模型**

<img src="https://mmbiz.qpic.cn/mmbiz_gif/GLeh42uInXTyY80RSpUTLjIMiaGGicv9zADM8nrhNkEtFpSpLjGicOemZ5mt7orYF8vFC7g83lPVDeSbnlgKl7XaA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片" style="zoom:67%;" />

如果一个文件描述符设置为阻塞的，当应用调用 `read()` 的时候，应用会一直停在 `read()` 函数中，直到应用接收到数据，然后这个过程中会有两个阶段的阻塞，一是数据从网卡拷贝到内核缓存区中，二是从内核缓存区拷贝到用户缓存区。产生的结果就是如果对方一直不发送数据，那么应用会一直阻塞在这个地方。



**非阻塞 IO**

<img src="https://mmbiz.qpic.cn/mmbiz_gif/GLeh42uInXTyY80RSpUTLjIMiaGGicv9zAT6rHhibbzK5rXiarLuJU0P4MGrHNl35vVCV4JdS4FeejOkl8bBGz9nVQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片" style="zoom:67%;" />

非阻塞 IO 就是，当一个文件描述符设置为非阻塞，应用调用 `read()` 的时候，如果文件描述符读没有就绪的话，函数会直接返回 -1，避免用户阻塞在 `read()` 函数，当描述符读就绪后，才会将数据从内核缓冲区搬到用户缓冲区，这部分是阻塞的。非阻塞 IO 一般与 IO 多路复用配合使用。



**多路复用 IO**

I/O 多路复用技术是一种使用一个进程来同时监听多个 I/O 流状态的技术，一旦检测到某个文件描述符发生了我们关心的事件，操作系统就会通知程序进行相应的处理。Linux 下实现 I/O 复用的系统调用主要有 select、poll 和 epoll 。



**信号驱动 IO**

在信号驱动IO模型中，当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。这个一般用于UDP中，对TCP套接口几乎是没用的，原因是该信号产生得过于频繁，并且该信号的出现并没有告诉我们发生了什么事情，而 UDP 的信号只有两个数据到达套接字和套接字发生错误。



**异步 IO**

前面四个实际上都是同步 IO，因为都需要应用参与到读取数据的过程中，要么需要等待数据从网卡拷贝到内核缓冲区，要么需要等待数据从内核缓冲区拷贝到用户缓冲区。而异步IO的优化思路是解决了应用程序需要先后发送询问请求、发送接收数据请求两个阶段的模式，在异步IO的模式下，只需要向内核发送一次请求就可以完成状态询问和数拷贝的所有操作。



#### sleep、wait 、join、yield的区别

1. 1. sleep():

   - `sleep()` 方法使当前执行的线程暂停执行一个指定的时间周期。
   - 在`sleep()`期间，线程不会释放已经持有的锁。
   - 这是一个静态方法，意味着它影响当前执行的线程。

   2. wait():

   - `wait()` 通常用于多线程同步的场景，它是 Object 类的一个方法。
   - 当一个线程调用一个共享对象的 `wait()` 方法时，它会进入该对象的等待池并释放已经持有的锁，直到另一个线程调用同一对象的 `notify()` 或 `notifyAll()` 方法。
   - `wait()` 通常和 `notify()` 或 `notifyAll()` 方法一起使用，用于在线程之间进行信号传递。

   3. join():

   - `join()` 方法用于等待另一个线程完成其执行。
   - 如果线程 A 执行了线程 B 的 `join()` 方法，线程 A 会被阻塞，直到线程 B 完成执行。
   - `join()` 方法可用于确保程序的顺序执行，它会等待被 `join()` 的线程结束后才继续执行。

   4. yield():

   - `yield()` 方法用于让出当前线程的执行权，给同优先级的其他线程或者优先级更高的线程以执行的机会。
   - `yield()` 方法不会释放锁。
   - 调用 `yield()` 方法的效果没有明确的定义，它取决于具体的操作系统和CPU的实现。
   - `yield()` 通常用于调试或测试目的，或者用在性能优化中，但并不保证能带来明显的性能提升。





### socket系列函数

1. `int socket(int domain, int type, int protocol)`

   - domain：协议域，AF_INET/AF_INET6/AF_LOCAL，决定使用的socket地址类型
   - type：socket类型，SOCK_STREAM、SOCK_DGRAM
   - protocol：协议，IPPROTO_TCP、IPPTOTO_UDP

2. `int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen)`

   - sockfd：通过socket()函数创建的fd，bind()函数就是将给这个fd绑定一个名字。

   - addr：一个const struct sockaddr *指针，指向要绑定给sockfd的协议地址。

     ```c
     struct sockaddr_in {
         sa_family_t    sin_family; /* address family: AF_INET */
         in_port_t      sin_port;   /* port in network byte order */
         struct in_addr sin_addr;   /* internet address */
     };
     
     /* Internet address. */
     struct in_addr {
         uint32_t       s_addr;     /* address in network byte order */
     };
     ```

   - addrlen：对应的是地址的长度。

3. `int listen(int sockfd, int backlog)`

   - sockfd：要监听的socket描述字
   - backlog：可以排队的最大连接个数

4. `int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen)`

   - sockfd：要监听的socket描述字
   - addr：服务器的socket地址
   - addrlen：socket的长度

5. `int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen)`

   - sockfd：服务器的socket描述字
   - addr：返回客户端的协议地址
   - addrlen：协议地址的长度

## Socket编程API、返回值

1. **socket()**
   创建一个新的套接字。

   ```C++
   int socket(int domain, int type, int protocol);
   ```

2. **bind()**
   将套接字绑定到特定的地址和端口。

   ```C++
   int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
   ```

3. **listen()**
   让套接字进入监听模式，等待连接请求。

   ```C++
   int listen(int sockfd, int backlog);
   ```

4. **accept()**
   接受来自客户端的连接请求。

   ```C++
   int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
   ```

5. **connect()**
   尝试与服务器端的套接字建立连接。

   ```C++
   int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
   ```

6. **send()**, **recv()**
   用于发送和接收数据。

   ```C++
   ssize_t send(int sockfd, const void *buf, size_t len, int flags);
   ssize_t recv(int sockfd, void *buf, size_t len, int flags);
   ```

7. **sendto()**, **recvfrom()**
   用于无连接的数据传输（例如UDP）。

   ```C++
   ssize_t sendto(int sockfd, const void *buf, size_t len, int flags, const struct sockaddr *dest_addr, socklen_t addrlen);
   
   ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);
   ```

8. **close()**
   用于关闭套接字。

   ```c++
   int close(int fd);
   ```

9. **getaddrinfo()**, **freeaddrinfo()**, **gai_strerror()**

   这些函数用于DNS查找，将域名转换为套接字地址结构。

   ```C++
   int getaddrinfo(const char *node, const char *service, const struct addrinfo *hints, struct addrinfo **res);
   
   void freeaddrinfo(struct addrinfo *res);
   
   const char *gai_strerror(int errcode);
   ```

   **setsockopt()**, **getsockopt()**
   设置或获取套接字选项。

   ```c++
   cCopy codeint setsockopt(int sockfd, int level, int optname, const void *optval, socklen_t optlen);
   
   int getsockopt(int sockfd, int level, int optname, void *optval, socklen_t *optlen);
   ```

### 返回值:

大多数Socket API函数在成功时返回非负值，但在出错时返回-1。

当这些函数返回-1时，可以使用`errno`来确定发生了什么错误，并使用`perror()`或`strerror(errno)`来获取描述性的错误消息。

例如：

```c++
if (bind(sockfd, (struct sockaddr *)&server_addr, sizeof(server_addr)) == -1) {
    perror("bind");
    // 或者: printf("bind: %s\n", strerror(errno));
    exit(1);
}
```

`accept()`, `recv()`, `send()`, `recvfrom()`, 和 `sendto()` 这些函数返回的是其他特定的值，如传输的字节数，或者在错误时返回-1。

当处理Sockets和网络错误时，errno`有`ECONNREFUSED`, `ETIMEDOUT`, `EHOSTUNREACH`等。



### epoll的函数

主要由三个：

```c++
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
```

> 1. **调用epoll_create**建立一个epoll对象(在epoll文件系统中给这个句柄分配资源)；向操作系统申请创建文件描述符的空间**（这个文件描述符都是在内核空间中）；
>
> 2. **调用epoll_ctl**，刚创立的socket加入到epoll对象中进行监控，或者将某个正在监控的socket移除，不在监控；
>
> 3. **`epoll_wait` 函数**是 Linux 上用于 I/O 多路复用的系统调用之一，它属于 `epoll` 系列函数的一部分。`epoll` 机制允许程序监视多个文件描述符，以便能够在其中一个或多个文件描述符上有数据可读、可写或出现异常时，立即得到通知。
>
> 4. 参数说明
>
>    - **`int epfd`:** `epoll` 实例的文件描述符，由 `epoll_create` 或 `epoll_create1` 函数创建。
>
>    - **`struct epoll_event *events`:** 用来从内核得到事件的集合。`events` 参数指向的数组用来返回满足条件的事件，它是一个 `epoll_event` 结构体数组。
>
>    - **`int maxevents`:** 告诉内核这个 `events` 数组有多大，即最多返回多少个事件。`maxevents` 必须大于零。
>
>    - **`int timeout`:** 指定超时时间，单位是毫秒。函数在等待文件描述符准备好的事件时将阻塞。
>
>      如果 `timeout` 设为 -1，`epoll_wait` 将一直阻塞直到有事件发生；如果设为 0，则 `epoll_wait` 将立即返回，即使没有事件发生；如果设为一个正数，则 `epoll_wait` 会等待指定的毫秒数。
>
>    ### 返回值
>
>    - 返回发生的事件数：如果 `epoll_wait` 成功运行，并且检测到了事件，它将返回一个大于 0 的数，表示发生的事件数。
>    - 返回 0：如果没有检测到事件，并且 `timeout` 时间已经过去，`epoll_wait` 将返回 0。
>    - 返回 -1：如果在等待事件时发生了错误，`epoll_wait` 将返回 -1，并且 `errno` 被设置为错误的具体原因。
>

#### epoll系列函数

1. `int epoll_create(int size)`

   - size 只要大于0即可

2. `int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event)`

   - epfd：epoll_create的返回值

   - op：对红黑树的操作，增加、删除、修改节点

     添加事件：相当于往红黑树添加一个节点，每个客户端连接服务器后会有一个通讯套接字，每个连接的通讯套接字都不重复，所以这个通讯套接字就是红黑树的 key。

     修改事件：把红黑树上监听的 socket 对应的监听事件做修改。

     删除事件：相当于取消监听 socket 的事件。

   - fd：需要添加监听的fd

   - event：设置感兴趣的事件信息

3. `int epoll_wait(int epfd, struct epoll_event* event, int maxevents, int timeout)`

   - epid：epoll_create的返回值
   - events：存放就绪的事件集合，是传出参数
   - maxevents：可以存放的事件个数
   - 阻塞等待的事件长短，-1代表阻塞等待



### pthread_cond_wait传递 mutex 参数

**结论**

**`mutex` 参数在 `pthread_cond_wait` 中的作用是确保条件变量的使用是安全和正确的。它保护了与条件变量相关的共享资源，提供了原子操作的能力，帮助处理虚假唤醒的情况，并确保了多线程环境下对共享资源的顺序一致访问。**

`pthread_cond_wait` 函数是 POSIX 线程库中用于线程同步的函数之一。这个函数的作用是让当前线程在指定的条件变量上等待，直到另一个线程通过 `pthread_cond_signal` 或 `pthread_cond_broadcast` 函数通知它继续执行。

**函数的原型是：**

```c++
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
```

`pthread_cond_wait` 需要传递 `mutex` 参数的原因主要有以下几点：

1. 保护条件的一致性

`mutex` 用来保护与条件变量相关联的共享资源。当一个线程检查条件是否满足，并决定进入等待状态时，这两个操作需要是原子的，以防止其他线程在这个检查和等待之间修改共享资源。互斥锁 `mutex` 提供了这种原子操作的能力。

2. 防止虚假唤醒

虚假唤醒是指一个或多个线程被唤醒，即使没有线程显式地发出通知。虽然 POSIX 标准并没有要求实现避免虚假唤醒，但在实践中，程序应该能够处理这种情况。通过在 `pthread_cond_wait` 返回后重新检查条件，并在必要时再次等待，可以防止虚假唤醒造成的问题。`mutex` 用于确保这个重新检查过程是安全的。

3. 顺序一致性

`mutex` 参数还帮助确保在多个线程之间对共享资源的访问是顺序一致的。使用互斥锁可以确保对共享资源的所有访问都是串行化的，这样就可以避免复杂的并发问题。

### open和fopen的区别

1. **来源不同**：`open` 是 UNIX 的系统调用函数，返回文件描述符。而 `fopen` 是C语言的库函数，在不同的系统中会调用不同的系统api。返回一个指向文件结构的指针。
2. **移植性**：`fopen` 是C标准函数，因此拥有良好的移植性；而 `open` 是UNIX系统调用，移植性有限。
3. **适用范围**：因为在Linux下，一切设备都是文件，所以Linux下的接口 `open` 适用于硬件、普通文件、网络套接字等。而 `fopen` 只适用于普通文件。
4. **有无缓冲**：`open` 无缓冲，`fopen` 有缓冲。前者与 `read`, `write` 等配合使用， 后者与`fread` , `fwrite` 等配合使用。













# OS手撕相关

## 两个线程交替打印

用一个计数器 `cnt` 表示轮到哪个线程（奇数第一个线程，偶数第二个线程）打印，每个线程函数打印自己的东西，使用 `lock_guard(mutex)` 保护 `cnt`

**例题：**

1. 三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串
2. 两个线程交替打印 0~100 的奇偶数
3. 通过 N 个线程顺序循环打印从 0 至 100
4. 多线程按顺序调用，A->B->C，AA 打印 5 次，BB 打印10 次，CC 打印 15 次，重复 10 次
5. 用两个线程，一个输出字母，一个输出数字，交替输出 1A2B3C4D...26Z

```cpp
// 包含头文件，分别为输入输出流，互斥锁和线程相关功能
#include <iostream>
#include <mutex>
#include <thread>

// 使用命名空间std，省去每次都要写std::的麻烦
using namespace std;

// 全局变量cnt，用于记录打印次数
int cnt = 0;

// 全局互斥锁mtx，用于线程间的同步
mutex mtx;

// 线程函数f1，用于打印字母A
void f1() {
    // 无限循环，确保可以持续打印
    while (true) {
        // 上锁，确保同一时间只有一个线程能访问cnt
        lock_guard<mutex> lock(mtx);
        // 如果cnt是偶数，打印A，并将cnt增加1
        if (cnt % 2 == 0) {
            ++cnt;
            cout << "A";
        }
        // 如果打印次数超过100，跳出循环，结束线程
        if (cnt > 100)    break;
    }
}

// 线程函数f2，用于打印字母B
void f2() {
    // 无限循环，确保可以持续打印
    while (true) {
        // 上锁，确保同一时间只有一个线程能访问cnt
        lock_guard<mutex> lock(mtx);
        // 如果cnt是奇数，打印B，并将cnt增加1
        if (cnt % 2 == 1) {
            ++cnt;
            cout << "B";
        }
        // 如果打印次数超过100，跳出循环，结束线程
        if (cnt > 100)    break;
    }
}

// 主函数
int main() {
    // 创建两个线程，分别执行f1和f2函数
    thread t1(f1), t2(f2);
    // 等待线程t1结束
    t1.join();
    // 等待线程t2结束
    t2.join();

    // 返回0，正常退出程序
    return 0;
}

```



