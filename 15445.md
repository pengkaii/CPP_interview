# Lab1 可扩展哈希、LRU-K、缓冲池==========

### 1 可扩展哈希表如何扩容，如何缩容的？

**扩容规则：当 bucket 需要分裂时，如果此时已经有多个指针指向 bucket，无需对 dir 进行扩容，**

**当需要插入 K 时，发现 bucket 已满，首先判断当前 bucket 的 local depth 是否等于 global depth：**

- **若相等，即仅有一个指针指向 bucket，需要对 dir 扩容。**
- **若不相等，即有多个指针指向 bucket，则无需扩容，将原来指向此 bucket 的指针重新分配。**

可扩展哈希表是一种动态调整大小的哈希表，用于解决哈希冲突和动态数据集大小的问题。当哈希表中的元素数量达到一定阈值时，需要进行扩容操作；当元素数量减少时，也可以进行缩容操作。

扩容的过程如下：

- 创建一个更大的哈希表，通常是当前哈希表大小的两倍或其他倍数。
- 将当前哈希表中的所有元素重新计算哈希值，并根据新的哈希值将它们插入到新的哈希表中。
- 更新哈希表的大小和其他相关参数，使其反映新的扩容状态。
- 释放原始哈希表占用的内存空间。

缩容的过程如下：

- 创建一个较小的哈希表，通常是当前哈希表大小的一半或其他合适的大小。
- 将当前哈希表中的所有元素重新计算哈希值，并根据新的哈希值将它们插入到新的哈希表中。
- 更新哈希表的大小和其他相关参数，使其反映新的缩容状态。
- 释放原始哈希表占用的内存空间。

可扩展哈希表的扩容和缩容过程是动态的，使得哈希表可以根据实际情况自适应地调整大小。

#### LRU-K 什么实现的

##### LRU-K算法思想、工作原理

**1、算法思想**

LRU-K中的K代表最近使用的次数，因此LRU可以认为是LRU-1。LRU-K的主要目的是为了解决LRU算法“缓存污染”的问题，其核心思想是将“最近使用过1次”的判断标准扩展为“最近使用过K次”。

**2、工作原理**

相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。

> 与LRU算法不同，LRU-K算法需要维护两个队列：`历史队列`和`缓存队列`。
>
> `历史队列`保存着每次访问的页面，当页面访问次数达到了k次，该页面出栈，并保存至`缓存队列`；若尚未达到k次则继续保存，直至`历史队列`也满了，那就根据一定的缓存策略(FIFO、LRU、LFU)进行淘汰。
>
> `缓存队列`则是保存已经访问k次的页面，当该队列满了之后，则淘汰最后一个页面，也就是`第k次访问距离现在最久`的那个页面。

详细实现如下

(1). 数据第一次被访问，加入到访问历史列表；

(2). 如果数据在访问历史列表里后没有达到K次访问，则按照一定规则（FIFO，LRU）淘汰；

(3). 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；

(4). 缓存数据队列中被再次访问后，重新排序；

(5). 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第K次访问离现在最久”的数据。

LRU-K具有LRU的优点，同时能够避免LRU的缺点，实际应用中LRU-2是综合各种因素后最优的选择，LRU-3或者更大的K值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。

![img](https://upload-images.jianshu.io/upload_images/2099201-a41c570dcac9fcad.png?imageMogr2/auto-orient/strip|imageView2/2/w/481/format/webp)



**LRU-K算法实现细节：**

1. 使用一个双向来维护缓存中的数据项，链表头部是最近使用的数据项，链表尾部是最久未使用的数据项。
2. 使用一个哈希表（散列表）来快速查找数据项在链表中的位置，以实现常数时间的访问。
3. 当有新的数据项访问时，首先在哈希表中查找该数据项是否已存在： a. 如果数据项已存在，将其从链表中移动到链表头部，表示最近使用。 b. 如果数据项不存在，首先检查链表长度是否已经达到缓存容量上限：
   - 如果链表长度小于容量上限，直接在链表头部插入新的数据项。
   - 如果链表长度等于容量上限，需要删除链表尾部的数据项（最久未使用的数据项），然后再在链表头部插入新的数据项。
4. 为了实现LRU-K算法，还需要维护每个数据项的访问次数。可以使用一个额外的计数器来记录访问次数，并在数据项被访问时增加计数。
5. 当删除数据项时，根据LRU-K算法，需要考虑最近K次的访问情况。即删除链表中访问次数最小的数据项。如果有多个访问次数相同的数据项，可以选择其中一个。

需要注意的是，实现LRU-K算法可能会涉及到线程同步的问题，特别是在多线程环境下进行缓存的并发访问。在并发环境中，需要考虑使用线程安全的数据结构或采用适当的同步机制，以保证数据的一致性和正确性。

总结起来，LRU-K算法的实现涉及到双向链表、哈希表以及访问计数等数据结构，以及相应的插入、删除和查找算法，以保证缓存的高效使用和淘汰最近K次未使用的数据项。

### 2  LRU-K算法的应用场景？

​     LRU-K算法是Least Recently Used - K的缩写，是一种页面置换算法，用于管理缓存中的页面。它是LRU算法的一种改进，LRU-K算法在实际应用中可以在更灵活的场景下提供更好的性能。

应用场景：

- 缓存管理：LRU-K算法可以用于管理缓存中的数据，可以根据实际需求灵活调整K的大小，适应不同的访问模式，提供更高的缓存命中率。
- 数据库管理：在数据库查询中，对于热门数据的频繁访问，LRU-K算法可以提供更好的缓存策略，减少磁盘IO，加快查询速度。
- 操作系统页面置换：在操作系统的虚拟内存管理中，LRU-K算法可以用于页面置换策略，更好地适应不同应用程序的访问模式，优化页面置换效率。

### 3  LRU-K算法相对于LRU算法的优势有哪些？

- 更灵活的缓存管理：LRU-K算法可以根据实际需求调整K的大小，从而更好地适应不同访问模式。K可以决定多少次访问被认为是"最近使用"，使得LRU-K算法在不同场景下具有更好的性能。
- 更好的性能预测：LRU-K算法通过考虑多次访问的历史，更好地预测未来可能的访问模式，提高了缓存命中率，减少了缓存失效导致的性能下降。
- 缓解突发访问问题：LRU-K算法可以在一定程度上缓解突发访问导致的缓存失效问题。在K的范围内，即使某个数据长时间没有访问，只要在K次访问内被再次访问，它仍然可以保持在缓存中，不容易被淘汰。

#### LRU-K 解决了什么问题

LRU-K算法解决了传统LRU算法的一些问题，比如如果该热点页面在偶然一个时间节点被其他大量仅访问了一次的页面所取代，那自然造成了浪费。主要集中在以下几个方面：

1. **解决冷热数据混合的问题**：传统的LRU算法只考虑了最近最少使用的数据进行淘汰，这可能导致一些热门数据被淘汰出缓存，而较少访问的数据却被保留。LRU-K算法通过引入K个元素的组合，能够更好地捕捉到较短时间内的数据访问热度，从而更有效地保留热门数据，降低缓存的命中率。
2. **适应不同访问模式**：不同应用和不同时间段的数据访问模式可能会有很大的差异，传统LRU算法无法灵活地适应这些变化。LRU-K算法通过调整K值，可以根据实际的访问模式来灵活地选择淘汰策略，提高缓存的命中率和效率。
3. **减少缓存淘汰频率**：传统LRU算法可能因为只关注最近的访问情况，而导致频繁的缓存淘汰和数据替换，这对系统的性能和响应时间有一定的影响。LRU-K算法能够在一定程度上减少淘汰频率，避免过于频繁地替换缓存中的数据，提高缓存的利用率。

### 4  缓冲池的执行流程

​    缓冲池是一块内存区域，用于存放经常被访问的数据，以加快数据的读取和写入速度。缓冲池通常用于缓存数据库的数据页或文件系统的数据块。

**缓冲池的整体流程：**

1、执行引擎需要2号页，去缓冲池要。

2、缓冲池说没有，就去磁盘读

3、从磁盘读入数据到缓冲池

​      a. 缓冲池有足够空间，直接放入

​      b. 缓冲池空间不够，替换算法，选择一个页淘汰。

​               i.没脏，直接替换。

​               ii. 脏了。先写入磁盘，再替换（将对应的 **frame 里的 page 数据写入 disk**，并重置 dirty 为 false。清空 frame 数据，并移除 **page_table 可扩展哈希表里的 page id**，移除 replacer 里的**引用记录**。）

4、然后把2好页给执行引擎

### 5  Page Cache的执行流程

1. 当应用程序需要读取文件中的数据时，首先会检查Page Cache中是否有该数据块的缓存。
2. 如果Page Cache中存在该数据块的缓存，则直接从Page Cache中读取数据，无需访问磁盘。
3. 如果Page Cache中不存在该数据块的缓存，则进行缺页（page fault）处理。
4. 缺页处理将触发操作系统从磁盘读取相应的数据块，并将其加载到Page Cache中。
5. 当数据块被写入时，操作系统会首先写入Page Cache中对应的缓存，并标记为“脏”（dirty）。
6. 根据操作系统的页面置换策略（如LRU-K算法），如果Page Cache已满，可能需要将一些缓存页替换出去，以腾出空间存放新的数据块。
7. 当操作系统认为合适的时机，将脏页（被修改过的页）写回磁盘，以保持数据的一致性。

Page Cache的存在可以大大提高文件系统的读取性能，因为从内存中读取数据远比从磁盘中读取数据快得多。

#### 缓冲池和Page Cache的执行流程 的区别

虽然缓冲池和Page Cache的目标是类似的，都是为了提高数据读写的性能，但它们在执行流程和应用领域上有一些区别。以下是它们之间的主要区别：

1. **应用领域**：
   - 缓冲池（Buffer Pool）主要用于数据库管理系统DBMS内部的一部分，用于缓存数据库的数据页，减少频繁的磁盘IO，提高数据库的读写性能。
   - Page Cache主要用于操作系统中的文件系统。它是操作系统内核实现的一种机制，用于在内存中缓存文件的数据块，以提高文件读取的性能。
2. **数据来源**：
   - 缓冲池主要缓存的是数据库的数据页。当数据库进行读取或写入操作时，数据首先会被缓存在缓冲池中。
   - Page Cache主要缓存的是文件系统中的数据块。当应用程序进行文件读取操作时，数据会被缓存到Page Cache中。
3. **粒度**：
   - 缓冲池以数据库的数据页为单位进行管理。每个缓冲池的页通常对应一个数据库数据页。
   - Page Cache以文件系统的数据块（通常是4KB大小）为单位进行管理。每个Page Cache的页对应一个文件系统的数据块。
4. **管理策略**：
   - 缓冲池的管理涉及到数据页的加载、替换、脏页回写等策略，以保持数据库的一致性和性能。
   - Page Cache的管理也涉及到数据块的加载、替换策略，但通常不需要关心数据的一致性，因为文件系统具有较强的一致性保障机制。
5. **应用场景**：
   - 缓冲池主要用于数据库系统，以提高数据库读写性能，适用于大规模数据处理场景。
   - Page Cache主要用于操作系统中的文件系统，用于加速文件读取，适用于一般的文件操作场景。

尽管缓冲池和Page Cache有区别，但它们都是通过在内存中缓存数据，减少磁盘IO的次数，从而提高数据读写性能。具体使用哪种机制取决于应用场景和所使用的系统。在数据库系统中，通常会同时使用缓冲池和Page Cache，以最大程度地提高数据访问的效率。

### 6  什么情况下可以用O_Direct

​    O_DIRECT是一个文件打开选项，用于直接I/O（Direct I/O），即绕过操作系统的缓存，直接从磁盘读取或写入数据。这种文件访问模式通常用于特定的情况，主要是为了提高性能和避免缓存的额外开销。

以下是一些情况下可以考虑使用O_DIRECT的场景：

1. 数据库系统：在一些数据库系统中，可以使用O_DIRECT来绕过操作系统缓存，从而更好地控制数据的读写，并提高数据库的性能。
2. 大数据处理：当处理大量数据时，使用O_DIRECT可以降低不必要的内存缓存开销，避免频繁的内存数据复制操作。
3. 实时系统：某些实时应用程序需要在严格的时间限制下进行数据读取和写入，O_DIRECT可以减少由于操作系统缓存带来的不可预测的延迟。
4. 磁盘备份和恢复：在备份和恢复数据时，O_DIRECT可以确保数据的一致性，避免将脏数据写入缓存。

尽管O_DIRECT在某些情况下有其优势，但使用它也有一些限制和注意事项：

- O_DIRECT要求读写的数据大小必须是磁盘块大小的整数倍，否则会导致EINVAL错误。
- O_DIRECT可能对某些文件系统不可用，例如FAT32和exFAT等。
- 使用O_DIRECT的性能提升并不是在所有情况下都显著的，它依赖于具体的应用和系统配置。

### 7 如果页的大小为16KB，而磁盘只保证4KB的读写是原子的，刷脏页的时候只刷了部分页进程就崩溃了，怎么处理？

​       这种情况可能会导致部分页数据丢失或不一致，如果进程在刷脏页时崩溃，磁盘上的数据可能处于不确定的状态。为了解决这个问题，可以采取以下几个措施：

- 写时复制（Copy-on-write）：可以使用写时复制技术，即在写入脏页时，首先将页的副本写入一个临时区域，确保数据写入磁盘是原子的。**在写时复制机制下，当进行页的修改时，不直接在原始数据上进行写入，而是先创建一个副本，对副本进行修改，然后再将修改后的副本写回磁盘。这样，在崩溃发生时，原始数据仍然保持不变，因为只有在修改成功后才会替换原始数据。如果进程在修改副本期间崩溃，只需丢弃这个副本，不会影响原始数据的完整性。当然，写时复制也会带来一定的性能开销，因为需要额外的存储空间来保存副本。**
- 日志：使用日志（Log）记录页的写操作，即将要写入的数据写入日志，然后异步地将数据写入磁盘。在系统崩溃后，可以根据日志回放的方式来恢复数据。
- 采用较小的页大小：如果磁盘只保证4KB的读写是原子的，可以考虑使用较小的页大小，以减少部分页数据丢失的影响。
- 数据备份：定期对重要的数据进行备份，以防止数据丢失或不一致。

处理数据在写入过程中的崩溃是一个复杂的问题，需要根据具体的应用场景和要求采取合适的解决方案。

### 8（part0)什么场景下适合使用字典树？

​     字典树（Trie树）适合用于处理字符串集合的相关问题，它是一种多叉树数据结构，用于高效地存储、查找和处理大量字符串。

适合使用字典树的场景包括：

- 字符串搜索：当需要高效地进行字符串搜索时，字典树可以在O(m)的时间复杂度内查找字符串，其中m是字符串的长度，不受字符串集合的大小影响。
- 字符串前缀匹配：字典树可以用于快速判断一个字符串是否为另一个字符串的前缀，例如自动补全、搜索提示等功能。
- 单词查找：字典树可以用于构建词典，实现高效的单词查找功能。
- 字符串统计：字典树可以用于统计一组字符串中某个字符串出现的次数。

### 9  有限状态自动机FST了解吗？跟字典树有什么联系？

​     有限状态自动机（Finite State Automaton，FSA）是一种抽象的数学模型，用于表示有限状态和状态之间的转移。有限状态自动机可以用于字符串匹配、词法分析、编译器等领域。

​     有限状态自动机FST（Finite State Transducer）是在有限状态自动机的基础上，增加了状态转移的输出。在FST中，每个状态转移都伴随着一个输出符号，表示状态之间的转移可以同时产生输入和输出。

​     **联系：** 字典树可以看作是一种特殊的有限状态自动机，它主要用于字符串集合的存储和查找。而FST相较于普通有限状态自动机，增加了输出的功能，可以用于更复杂的字符串处理任务，例如字符串转换、编码解码等。

### 10  怎么理解的倒排索引？

  倒排索引（Inverted Index）是一种用于加速文本搜索的数据结构和算法。它是搜索引擎等信息检索系统中最常用的技术之一。倒排索引的名称源于其工作方式：它反转了原始文本数据的结构，将文档和其中出现的单词之间的映射关系反转，从而实现了以单词为基础进行文本搜索的能力。

在理解倒排索引时，可以通过以下几个关键概念来解释：

1. 文档（Document）：文档是信息检索系统中的基本单位，可以是网页、文本文件、文章等。每个文档都有一个唯一的标识符，通常是一个数字或字符串。
2. 单词（Term）：单词是文档中的基本单元，也称为词项（Term）。在构建倒排索引时，会提取文档中的每个单词，并建立与这些单词的映射关系。
3. 倒排列表（Inverted List）：倒排列表是倒排索引的核心组成部分。它是一个包含了所有包含该单词的文档标识符的列表。每个单词都对应一个倒排列表，该列表记录了所有包含该单词的文档标识符，以及在文档中出现的位置等信息。
4. 倒排索引表（Inverted Index Table）：倒排索引表是包含所有单词及其对应倒排列表的数据结构。它将每个单词映射到其对应的倒排列表，使得通过单词可以快速找到包含该单词的文档。
5. 搜索过程：当用户输入一个查询词（或多个查询词）时，系统会通过倒排索引表快速定位包含这些查询词的文档。然后，根据倒排列表中的位置信息，可以很快地找到这些单词在文档中的出现位置，并返回相关文档给用户。

倒排索引在搜索引擎中的应用非常广泛，它极大地加速了文本搜索的速度，使得搜索引擎能够快速地响应用户的查询，并返回相关度高的搜索结果。倒排索引的优势在于它将搜索时间从文档数量线性降低到了查询词的数量，使得搜索引擎在大规模文本数据中能够快速地定位相关文档。它被广泛应用于搜索引擎、数据库管理系统、信息检索等领域。

### 补充：

### 1. NewPage和FetchPage两个函数有什么区别？

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1689674860597-6d3a4af6-05a3-4b19-9b71-2ad0a89a9dd9.png)

### 2 page和frame是什么关系？

​       "page"和"frame"通常用于描述数据库管理系统（DBMS）中的内存管理和磁盘管理的概念。

1. Page（页）： "Page"是数据库管理系统（DBMS）中**磁盘**上的最小数据单元，通常也称为"数据库页"。一个页的大小是固定的，常见的页大小为4KB或8KB。数据库的数据在磁盘上被分割成许多页，每个页都有一个唯一的标识符，用于定位和读取该页上的数据。
2. Frame（帧）： "Frame"是数据库管理系统（DBMS）中**内存**的最小数据单元。内存也被分割成许多大小相同的块，每个块被称为"帧"。与页相似，帧的大小通常也是4KB或8KB。数据库管理系统使用内存中的帧来缓存磁盘上的页，以加快对数据的访问。

关系：这个关系可以通过“页表”数据结构来描述。它记录了磁盘上每个页与内存中帧的映射关系。当数据库需要访问磁盘上的某个页时，首先会查找页表，看该页是否已经被缓存在内存的某个帧中。如果存在对应的帧，数据库就可以直接从内存中读取数据，避免了频繁的磁盘读取操作，从而提高了数据库的性能。

### 3 flush到磁盘后要不要清空page中的数据？

​         **将数据从内存的页刷新到磁盘后，数据不会立即被清空，**刷新到磁盘意味着将内存中的数据写回到磁盘的存储位置，以确保数据持久性。然而，内存中的数据可能在之后的操作中被修改，如果立即清空页中的数据，将会导致数据的丢失。**所以留在内存缓存中，缓存的数据可以更快地读取和写入，避免频繁地访问较慢的磁盘，以提高读写性能。**

​        当数据被刷新到磁盘后，操作系统会维护一个脏页表或脏位（dirty bit）来跟踪哪些页的数据已经被修改过。只有当这些页被再次替换出内存时，操作系统会将它们的数据清空，以便空间可以被重用。

当操作系统需要**回收页的内存空间**时，一些文件系统可能会采用“**懒惰清空**”（lazy cleaning）策略，即只有**在新的数据写入时才会真正清空内存页**。这样可以避免不必要的数据清空操作，从而提高系统性能。

### 4 pip_count引用计数不为0能不能flush刷新到磁盘?

​     通常情况下，引用计数不为0的页是不会被刷新到磁盘的。

​       引用计数是用来跟踪一个页被多少个进程或数据结构引用。当一个页的引用计数不为0时，表示**该页仍然被使用中**，可能被多个进程共享引用该页。在这种情况下，将页刷新到磁盘可能会导致其他进程或数据结构无法访问到该页的数据，因为它仍然被使用中。

  只有在引用计数为0的情况下，才表示没有任何进程或数据结构在使用该页的数据，此时可以安全地将页刷新到磁盘，并释放该页所占用的内存空间。

### 5 内存中的page和磁盘的page有什么区别？

缓冲池内存中的Page用于加速数据库的读取操作，减少磁盘I/O的频率，提高查询性能。而磁盘上的Page是数据库的永久存储方式，用于保持数据的持久性，并在需要时从磁盘读取或写入数据。缓冲池内存中的Page和磁盘上的Page之间通过**缓冲池管理机制**进行数据交换和同步，以保证数据的一致性。

- 内存中的 "page"：在计算机内存管理中，页是虚拟内存的一部分，它是内存管理的最小单位。操作系统将虚拟内存分割成固定大小的页，这些页被映射到物理内存的帧上。当程序访问虚拟内存时，操作系统将虚拟内存地址映射到对应的物理内存地址，以实现虚拟内存与物理内存之间的转换。
- 磁盘中的 "page"：在磁盘存储中，"page" 通常指的是磁盘上的固定大小的数据块。磁盘被划分成许多扇区，每个扇区可以存储一个 "page" 大小的数据。磁盘上的 "page" 被用来存储文件系统的数据，操作系统可以将内存中的数据刷新到磁盘的 "page" 中，以实现数据的持久性和长期存储。

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1689674901347-73cccb7a-ab1d-4af9-8ad1-b0d74f9e87e1.png)

### 6 Page 逻辑地址的基本单位,Frame 物理地址的基本单位

​    在计算机内存管理中，"Page" 是**虚拟内存**的基本单位，而 "Frame" 是物理内存的基本单位。

- Page：在**虚拟内存**中，内存被划分为固定大小的页（Page），通常是4KB、8KB或其他大小的固定块。程序使用的**逻辑地址**（Logical Address）由**页号和页内偏移**（Offset）组成，页号指示了该地址所属的页，页内偏移指示了在该页中的偏移量。
- Frame：在**物理内存**中，内存被划分为固定大小的帧（Frame），帧的大小与页的大小相同。物理地址（Physical Address）由帧号和页内偏移组成，帧号指示了该地址所属的帧，页内偏移指示了在该帧中的偏移量。

**逻辑地址通过页表映射到物理地址，将虚拟内存映射到物理内存，**实现虚拟内存与物理内存的转换。

### 7 采用的write-back cache，也就是延迟写回磁盘，等到替换页的时候再写回。

​      "Write-back cache" 是一种缓存策略，用于存储器的读写优化。当使用 write-back cache 时，数据在写入缓存时，不会立即写回到磁盘，而是在缓存替换策略触发时，才将数据写回到磁盘。

这样的设计有助于**提高系统性能和减少频繁的磁盘写操作**。当数据被修改后，它首先会被标记为“脏”（dirty），表示该数据已经发生了变化。当操作系统需要替换当前在缓存中的页时，只有**标记为“脏”的页才会被写回到磁盘**，以确保**数据的持久性和一致性**。

相对应的，另一种缓存策略是 "Write-through cache"，在这种策略下，数据在被写入缓存的同时，立即被写回到磁盘。

Write-through cache（写透模式缓存）是计算机系统中一种用于提高数据访问速度并保持缓存与其后备存储（如主内存或磁盘）数据一致性的缓存技术。在写透模式缓存中，数据在缓存和后备存储中同时写入或更新，确保缓存中的数据始终与后备存储中的数据保持同步。

写透模式缓存的工作原理如下：

1. 数据读取： 当处理器或应用程序请求数据时，首先检查缓存。如果数据在缓存中找到（缓存命中），则直接返回给处理器，减少访问时间，因为缓存比后备存储快。如果数据不在缓存中（缓存未命中），则先从后备存储中获取数据，然后将其放入缓存，再返回给处理器。
2. 数据写入： 当处理器更新或写入新数据时，首先在缓存中执行写操作。数据被更新或写入到缓存中，然后缓存控制器确保更新的数据也被写回到后备存储。这确保了缓存和后备存储中的数据保持一致。

### 8 什么时候把page读入内存，什么把page写入磁盘

​    在**数据库管理系统**（DBMS）中，Page的读入和写入通常涉及到缓冲池（Buffer Pool）的管理，以及数据在内存和磁盘之间的交换。以下是Page读入缓冲池内存和写入磁盘的时机：

1. **Page读入缓冲池内存：** 当数据库需要访问一个Page时，首先会检查缓冲池是否已经缓存了该Page。如果该Page已经在缓冲池中（缓冲池命中），数据库可以直接从内存中获取数据，避免频繁的磁盘读取操作，从而提高查询性能。**如果该Page不在缓冲池中（缓冲池未命中），则数据库需要从磁盘读取该Page，并将其放入缓冲池内存中，**以便后续的查询可以直接访问缓冲池中的数据。Page的读入缓冲池通常是在数据库首次访问某个Page或缓冲池中没有该Page时发生的。

2. Page写入磁盘： 在数据库中，Page的写入通常涉及到数据的持久性和事务管理。当数据库需要将修改过的Page写入磁盘时，有以下几种时机：

   a. 写透模式（Write-through）：在写透模式下，每当数据库更新或写入新的数据，数据会先写入缓冲池，并立即将其写回磁盘，保持数据的一致性和持久性。

   b. 写回模式（Write-back）：在写回模式下，数据库在更新或写入新的数据时，只将数据写入缓冲池，不立即将其写回磁盘。数据会暂时驻留在缓冲池，直到缓冲池空间不足或发生特定事件时，再将修改过的数据批量写回磁盘。

   c. **脏页刷新（Dirty Page Flushing）：当Page在缓冲池中被修改后，这个Page就被称为"脏页"，表示缓冲池中的数据与磁盘上的数据不一致。脏页刷新是指将脏页写回磁盘，以保持数据的一致性。脏页刷新通常在后台进程或特定的时间点进行，以避免过多的磁盘I/O操作对性能的影响。**

在**虚拟内存管理**中，当需要访问一个虚拟地址对应的页时，会触发以下两种操作：

- 把Page读入内存：当程序访问一个虚拟地址，而该地址对应的页（Page）当前不在物理内存中时，发生了缺页（page fault）。缺页是一种中断，通常由硬件或操作系统的页表管理单元检测到。当缺页发生时，操作系统会根据页表信息找到对应的页面数据所在的磁盘位置，然后将该页面读入物理内存（Frame）。这个过程称为页的调入（page-in），也称为页面置换（page-in）。
- 把Page写入磁盘：**当虚拟内存中的某个页被修改后**，为了保持数据的一致性和持久性，该页会被标记为“脏”（dirty）。当操作系统需要回收某个物理内存帧以供其他页使用时，如果该帧对应的页是“脏”的，即该页的数据已被修改过，则该页会被写回到磁盘，以保持数据的一致性。这个过程称为页的写回（page-out），也称为页面置换（page-out）或页回写（page write-back）。





# Task1 可扩展哈希表设计==========

### 可扩展哈希表是什么

**Extendible Hashing** 是一种动态哈希方法，其中目录和桶被用于哈希数据。可扩展哈希是一个有力的灵活的方法，其中哈希函数也经历动态的改变。

Extendible Hash Table 由一个 directory 和多个 bucket 桶组成，在 Buffer Pool Manager 中主要用来存储 **buffer pool 中 page id 和 frame id** 的映射关系。

- **directory**目录: 存放指向 bucket 的指针，是一个数组。用于寻找 key 对应 value 所在的 bucket。目录在指针中存储桶的地址。每个目录被分配一个`id` ，每次目录扩张时，`id`可能会发生变化。哈希函数返回这个目录的id，这个`id`被用来指向合适的桶。
- **bucket**桶: 存放 value，是一个链表。一个 bucket 可以至多存放指定数量的 value。桶被用于哈希真实的数据。如果局部深度小于全局深度时，一个桶可能包含不止一个指针指向它。
- **全局深度**：它跟目录相关联。它们表示哈希函数使用的比特位数目去分类这些键。全局深度=目录id的比特位数。
- **局部深度**：和全局深度类似，除了局部深度是跟桶关联，而不是跟目录。当桶溢出发生时，局部深度根据全局深度去决定执行的行为。局部深度通常小于等于全局深度。
- **目录扩容**：当桶溢出时，产生目录扩容。当溢出桶的局部深度等于全局深度时，目录扩容被执行。

#### Extendible Hash Table 与 Chained Hash Table 最大的区别：

Extendible Hash 中，**不同的指针可以指向同一个 bucket**，**如果 bucket 到达容量上限，则对桶会进行一次 split 操作**。

而 Chained Hash 中每个指针对应一个 bucket。发生冲突时，Chained Hash 简单地将新的 value 追加到其 key 对应 bucket 链表的最后，也就是说 Chained Hash 的 bucket 没有容量上限。

### 可扩展哈希基本工作流程

1. **分析数据元素：**数据元素可能以各种形式存在，比如整型，字符串，浮点数等等...当前，我们考虑整形这类数据元素，比如49。

2. **转换成二进制形式：**将数据元素转换为二进制形式。对于字符串元素，考虑ASCII码起始字符的对应整数，然后转换成二进制形式。因为我们是将49作为数据元素，它的二进制形式是11001。

3. **检查目录的全局深度：**假设哈希目录的全局深度是3。

4. **识别目录索引：**考虑在二进制下最低“全局深度”位，然后去匹配目录id。比如，二进制是：110001，全局深度是3，所以哈希函数会返回110**001**最后三位，即001。

5. **导航**：现在，访问目录id=001指向的桶。

6. **插入和溢出检查：**插入元素并且检查桶是否溢出。如果遇到溢出，转向步骤7和步骤8，否则转到步骤9。

7. **处理数据插入过程中的溢出情况:**  很多情况下，当在桶里插入数据时，可能发生桶溢出。在这种情况下，我们需要采取下面合适的流程去避免误操作数据。首先，检查局部深度是否小于或等于全局深度。

   **情况1:** 如果溢出桶的本地深度等于全局深度，那么需要执行目录扩张和桶分裂。然后全局深度和局部深度的值增加1，并且指定合适的指针。目录扩张会double哈希结果中的目录数量。

   **情况2:**如果本地深度小于全局深度，那么仅仅发生桶分裂。然后仅仅把局部深度增加1。并且指定合适的指针。

8. **分裂桶的元素重新哈希**：在被分裂的溢出桶中呈现的元素会根据目录的全局深度进行重新哈希。

9. 元素被成功哈希。

### **可扩展哈希 的插入流程**

将一个键值对 (K,V) 插入哈希表时，会先用哈希函数计算 K 的哈希值 H(K)，并用此哈希值计算出**索引**，将 **V 放入索引对应的 bucket** 中。

Extendible Hash 计算索引的方式是直接取哈希值 H(K) 的低 n 位。在这里，我们把 n 叫做 global depth。例如，K 对应的 H(K) = 1010 0010b，此时 global depth 为 4，则对应的 **index** 为 0010，即应将 V 放入 directory 里 index 为 2 的指针指向的 bucket 中。

插入之前判断桶是否会分裂扩容。

### split 分割扩容步骤：

1. **global depth++**

2. **directory 容量翻倍**

3. **创建一个新的 bucket**（创建 bucket 后，自然需要将 dir 指针重新安排，0 指向 bucket 0，1 指向 bucket 1）

4. **重新安排指针**（**重新安排指向需要 split 的 bucket 的兄弟指针**）

5. **重新分配 KV 对**（重新计算发生 split 的 bucket，即 **bucket 0 中所有 KV 对的新位置，并重新分配**。）

   每个 bucket 都有一个自己的 local depth，bucket 实际上只用到了 H(K) 的低 local depth 位作为索引。local depth 的初始值为 0。在 **bucket 发生 split 时，local depth++**：

6. **扩容规则**：当 bucket 需要分裂时，如果此时已经有多个指针指向 bucket，无需对 dir 进行扩容，

   当需要插入 K 时，发现 bucket 已满，首先判断**当前 bucket 的 local depth 是否等于 global depth**：

   - 若相等，即仅有一个指针指向 bucket，需要对 dir 扩容。
   - 若不相等，即有多个指针指向 bucket，则无需扩容，将原来指向此 bucket 的指针重新分配。

#### dir 扩容时，新的指针应该指向哪里？

假如 global depth=2，原索引为 000b 001b 010b 011b，则扩容添加的索引为 100b 101b 110b 111b，可以看出低两位的值是一一对应的，**新索引应指向低位对应索引的 bucket**。我们把指向同一个 bucket 的指针称为兄弟指针。

#### 如何重新安排指针？ 

重新安排指针实际上是**重新安排指向需要 split 的 bucket 的兄弟指针**。需要注意的是，兄弟指针不一定只有两个，而可以有 2^n 次个。**兄弟指针的个数为 2^(global depth - local depth)**

 **000b 010b 100b 110b，分别为 0 2 4 6**。这样我们就找到了所有的兄弟指针，**local depth 变为 2，用到低 2 位，则兄弟指针可以分为两组，x00b 和 x10b**，即 0 4 一组，2 6 一组。其中，一组指向原 bucket 0，另一组指向新 bucket 4。这样就完成了指针的重新分配。

#### 如何重新分配 KV 对？

 仅需用 global depth 重新计算一遍 K 对应的 index 并插入对应 bucket。

## 项目可扩展哈希的实现

> ExtendibleHashTable类实现函数：
>
> - Find(K, V) : 对于给定的键K，检查它在哈希表中是否存在。如果存在，那么将这个指针相对应的值存进V中，并返回true。如果这个键不存在，那么返回false。
> - Insert(K, V) : 插入键值对到哈希表。如果键K已经存在，那么使用新的值V覆盖它，并返回true。如果键值对不能被插入进桶（因为桶满了并且这个键不是更新已存在的键值对）
> - Remove(K) : 对于给定的键K，从哈希表中移除对应的键值对并返回true。如果键K不存在，返回false。
> - GetGlobalDepth() :返回整个哈希表当前的全局深度。
> - GetLocalDepth(dir_index) : 返回指定目录索引指针指向桶的当前局部深度。
> - GetNumBuckets(): 返回在哈希表中分配的桶的总数量。
>
> 1. 如果桶的本地深度等于全局深度，增加全局深度并且2倍目录的大小。
> 2. 增加本地桶的深度
> 3. 分裂桶并且在桶中重新分布目录指针和键值对。

整个 table 一把大锁，再分区加多把小锁，或者更简单的做法，每个 bucket 一把小锁。均使用读写锁。

对于每个 bucket，**在 Find 时上读锁，Insert 和 Remove 时上写锁**。

对于整张表，Find 和 Remove 时上读锁。Find 上读锁好理解，而 Remove 实际上只会改变 bucket 的内部变量，其线程安全由 bucket 内部锁保证，因此也可以只上读锁。Insert 在无需 split 时也可以仅上读锁，需要 split 时上写锁。

**每次 Insert 前要判断一下是否需要 split**。而 split 之后不一定代表可以直接 Insert，因为可能重新分配 KV 对时，所有的 KV 对又被塞到了同一个 bucket 里，而凑巧的是需要插入的 KV 对也被带到了这个 bucket。因此需要循环判断，可能需要多次 split 才能成功插入。

```
可扩展哈希的实现：GPT
可扩展哈希的实现思路是通过动态地增加哈希表的大小，以容纳更多的数据，并且在数据量减少时也能适应减小哈希表的大小，从而减少空间的浪费。以下是可扩展哈希的基本思路：

初始化：创建一个初始大小的哈希表，通常是一个包含固定数量桶（buckets）的数组。每个桶可以存储键值对，最初数组中的所有桶都为空。

哈希函数：选择或设计一个合适的哈希函数，用于将键映射到哈希表中的桶。哈希函数应该尽可能均匀地分布键，以减少碰撞的可能性。

存储：当有新的键值对要插入哈希表时，使用哈希函数找到对应的桶，并将键值对存储在该桶中。如果发现该桶已经存在相同的键，则可以更新对应的值。

加载因子：跟踪哈希表的负载情况，即已存储键值对的数量与桶的总数之比。一般情况下，设置一个加载因子阈值，当负载超过该阈值时，触发哈希表的扩展操作。

扩展：当加载因子超过预设阈值时，执行扩展操作。扩展过程包括增加哈希表的桶数量，并重新哈希所有已有的键值对，使其分布到新的桶中。这样可以保持哈希表的负载较低，减少碰撞的可能性。

收缩：类似地，如果哈希表中的数据量减少，导致加载因子低于一定阈值，可以考虑收缩哈希表的大小，减少桶的数量，节省内存空间。

冲突解决：在插入键值对时，可能会出现碰撞，即不同的键映射到同一个桶中。为了解决碰撞，可采用链地址法（Chaining），即在每个桶中使用链表或其他数据结构来存储多个键值对。

哈希表性能：选择适当的扩展策略和冲突解决方法对哈希表的性能至关重要。较好的哈希函数和负载控制可以提高查询和插入操作的效率。

动态调整：在实际应用中，哈希表的动态调整可能涉及到一些策略和调整参数，以平衡空间和时间效率，同时应满足数据插入和查询的性能要求。

总之，可扩展哈希的实现思路是在需要时动态地增加哈希表的大小，并且在数据量减少时也能适应减小哈希表的大小，从而实现更高效的数据存储和检索。
```



```
GPT：
可扩展哈希是一种解决哈希表动态扩容的技术，用于处理当哈希表中的数据量增加时，自动调整哈希表大小并重新分布数据。下面是可扩展哈希的插入流程：
初始化：开始时，创建一个初始大小的哈希表，通常是一个小的固定大小。同时，设定一个负载因子（load factor）阈值，表示当哈希表中元素数量达到该值时需要进行扩容操作。
哈希函数：选择一个合适的哈希函数来将关键字（键）映射为哈希表中的索引位置。确保哈希函数能够均匀地将不同的关键字映射到不同的索引位置，以减少冲突。
插入元素：当要向哈希表中插入一个新的键值对时，首先通过哈希函数找到对应的索引位置。
冲突处理：如果该索引位置已经存在其他元素，即发生了冲突，那么需要处理冲突。常见的冲突处理方法有：
a. 链地址法（Chaining）：将冲突的元素以链表的形式连接在同一个索引位置。
b. 开放地址法（Open Addressing）：通过一定的探测方法，在哈希表中寻找空槽位来存放冲突的元素。
扩容判断：在插入新元素后，检查当前哈希表的负载因子是否超过了预设的阈值。如果超过了阈值，说明哈希表已经过于拥挤，需要进行扩容。
哈希表扩容：在哈希表扩容时，会增加哈希表的大小，并重新计算所有元素的哈希值，然后重新将它们分布到新的哈希表中。这样可以保持哈希表的平衡性，并且降低冲突的概率。
数据迁移：在扩容过程中，需要将旧哈希表中的元素逐个移动到新的哈希表中。这一过程需要保证在移动过程中并发的插入操作能够正确处理。
扩容完成：完成数据迁移后，新的哈希表取代旧的哈希表，扩容操作完成。
继续插入：现在可以继续向新的哈希表中插入新的键值对。

通过可扩展哈希技术，可以有效地避免哈希表的拥挤问题，提高了哈希表的性能和容量。同时，需要注意合理设置负载因子阈值，避免频繁的扩容操作。
```

## 简历解析：

1、在你的存储器中实现一个缓冲池。**缓冲池负责从主存到磁盘移动物理页**。系统使用唯一的标识符page_id_t请求缓冲池的一个页，并且系统不知道这个页是已经在内存中，还是系统要去磁盘获取。缓冲池允许一个数据库管理系统去支撑比系统中可用内存容量更大的数据库。缓冲池的操作对系统中的其他部分透明。

2、构建可扩展哈希表，使用无序的桶去存储唯一的键值对。哈希表在**未指定最大容量的前提下支持插入、删除键值对**。哈希表能够增量扩容，但不需要收缩。支持检查一个key是否在这个哈希表中存在并且返回对应的值。

3、ExtendibleHashTable类中实现下面这些函数：

- ExtendibleHashTable**类实现函数：**

  > - Find(K, V) : 对于给定的键K，检查它在哈希表中是否存在。如果存在，那么将这个指针相对应的值存进V中，并返回true。如果这个键不存在，那么返回false。
  > - Insert(K, V) : 插入键值对到哈希表。如果键K已经存在，那么使用新的值V覆盖它，并返回true。如果键值对不能被插入进桶（因为桶满了并且这个键不是更新已存在的键值对）
  > - Remove(K) : 对于给定的键K，从哈希表中移除对应的键值对并返回true。如果键K不存在，返回false。
  > - GetGlobalDepth() :返回整个哈希表当前的全局深度。
  > - GetLocalDepth(dir_index) : 返回指定目录索引指针指向桶的当前局部深度。
  > - GetNumBuckets(): 返回在哈希表中分配的桶的总数量。
  >
  > 1. 如果桶的本地深度等于全局深度，增加全局深度并且2倍目录的大小。
  > 2. 增加本地桶的深度
  > 3. 分裂桶并且在桶中重新分布目录指针和键值对。

4、如果在一个插入之后桶满了，那么要实现分裂桶。如果桶满了请检查，然后在插入之前实现分裂。在IndexOf(K)私有函数去计算一个给定的key哈希目录索引。

## 可扩展哈希的优缺点与特点

**特点：**

1. 如果局部深度小于全局深度，一个桶会被超过1个指针指向。
2. 当桶发生溢出时，在这个桶里的所有元素都要用新的局部深度重新哈希。
3. 如果溢出桶的局部深度，在数据插入操作开始之后桶的大小不能改变。

**优势：**

1. 数据检索很便宜（根据计算）
2. 存储容量动态增长，数据丢失没有问题。
3. 随着在哈希函数中的动态改变，相关联的旧值被重新哈希到新的哈希函数中。

**缺点：**

1. 当记录保持非均匀分布，如果多个记录被哈希到同一个目录，那么目录的大小会显著增加。
2. 每个**桶的大小是固定**的。
3. 当全局深度和局部深度差距很大时，内存在指针上是浪费的。

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1689677095895-ed25cc9f-509b-4707-b3d7-3ea79e5351e9.png)

## Task2 LRU-K 替换器（看代码）

### LRU-K算法思想、工作原理

**1、算法思想**

LRU-K中的K代表最近使用的次数，因此LRU可以认为是LRU-1。LRU-K的主要目的是为了解决LRU算法“缓存污染”的问题，其核心思想是将“最近使用过1次”的判断标准扩展为“最近使用过K次”。

**2、工作原理**

相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会淘汰第K次访问时间距当前时间最大的数据。

> 与LRU算法不同，LRU-K算法需要维护两个队列：`历史队列`和`缓存队列`。
>
> `历史队列`保存着每次访问的页面，当页面访问次数达到了k次，该页面出栈，并保存至`缓存队列`；若尚未达到k次则继续保存，直至`历史队列`也满了，那就根据一定的缓存策略(FIFO、LRU、LFU)进行淘汰。
>
> `缓存队列`则是保存已经访问k次的页面，当该队列满了之后，则淘汰最后一个页面，也就是`第k次访问距离现在最久`的那个页面。

详细实现如下

(1). 数据第一次被访问，加入到访问历史列表；

(2). 如果数据在访问历史列表里后没有达到K次访问，则按照一定规则（FIFO，LRU）淘汰；

(3). 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；

(4). 缓存数据队列中被再次访问后，重新排序；

(5). 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第K次访问离现在最久”的数据。

LRU-K具有LRU的优点，同时能够避免LRU的缺点，实际应用中LRU-2是综合各种因素后最优的选择，LRU-3或者更大的K值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。

![img](https://upload-images.jianshu.io/upload_images/2099201-a41c570dcac9fcad.png?imageMogr2/auto-orient/strip|imageView2/2/w/481/format/webp)

#### 补：

LRU-K Replacer 用于存储 buffer pool 中 **page 被引用的记录**，并根据**引用记录来选出在 buffer pool 满时需要被驱逐的 page**。

在 LRU-K 算法中，当所有的 page 都被引用了大于等于 K 次时，需要比较最近第 K 次被引用的时间，驱逐最早的。而当存在引用次数少于 K 次的 page 时，会将这些 page 挑选出来，用普通的 LRU 算法来比较这些 page 第一次被引用的时间，驱逐最早的。

LRU-K Replacer 中的 page 有一个 evictable 属性，当一个 page 的 evicitable 为 false 时，上述算法跳过此 page 即可。这里主要是为了上层调用者可以 pin 住一个 page，对其进行一些读写操作，此时需要保证 page 驻留在内存中。

### LRU-K 替换器 实现

历史队列history_list:历史队列中的数据为第一次访问时的位置，只要未达到K次访问频率，位置一直保持不变。

缓存队列cache_list：大于等于K次的访问频率，需要根据最近访问时间切换元素在队列中的位置。

### 代码中的实现：

- **淘汰页面Evict(**frame_id_t*)：帧id对应的页面，

  需要判断当前页面是否可以驱逐，可以用布尔数组来标识，直接判断下标即可。

  优先从历史队列中淘汰。

- **记录访问逻辑RecordAccess**(frame_id_t)：

  频率等于k

  ​      history_queue删除

  ​      cache_queue新增

  频率小于k

  频率大于k

  ​    cache_queue刷新位置

  给定frame_id_t的记录在当前时间戳被访问。在BufferPoolManager中这个方法应该在一个页被固定之后调用。

- **Remove**(frame_id_t)：

  清除frame相关的历史访问。在BufferPoolManager中这个方法仅仅当一个页被删除之后才被调用。

- **设置可被驱逐SetEvictable**(frame_id_t, bool set_evictable)：

  **如果访问频率为0，直接忽略**

  **其他情况需要考虑当前可被替换的页面的增减**

  这个方法控制一个frame是否可被驱逐。同时也控制LRUKReplacer的大小。当你实现BufferPoolManager的时候你会知道什么时候调用这个函数。更详细地说，**当一个页的pin count达到了0，它相对应的页就被标记为可驱逐的，并且替换器的大小增加**。

- Size() ：这个方法返回在LRUKReplacer中当前可被驱逐的数量。

## Task3 缓冲池

### **简历用处：**

**1、实现缓冲池管理器：**

 缓冲池管理者负责从磁盘管理者中获取数据库页面并将它们存储在内存中。 缓冲池管理者需要**驱逐页面时**为新页面腾出空间时将脏页写出到磁盘。

**2、Page 对象是缓冲池中内存的容器，因此并不特定于唯一的页面。**

**每个 Page 对象**都包含一个**内存块**，DiskManager 将使用**该内存块作为位置**来复制它从磁盘读取的物理页面的内容。 BufferPoolManager 将**重用同一个 Page 对象**来存储数据，因为它在磁盘上来回移动。 这意味着在系统的整个生命周期中，同一个 Page 对象可能包含**不同的物理页面**。 Page 对象的标识符 **(page_id) 跟踪它包含的物理页面**； 如果 Page 对象不包含物理页面，则其 page_id 必须设置为 INVALID_PAGE_ID。

**3、每个 Page 对象为“固定”该页面的线程数维护一个计数器。**

 BufferPoolManager **不允许释放固定的页面**。 **每个 Page 对象跟踪它是否脏了**。 记录**页面**在取消固定之前是否被修改。  BufferPoolManager 必须**先将脏页的内容写回磁盘，然后才能重用该对象**。

**4、 BufferPoolManager 实现使用创建的 LRUKReplacer 类。**

LRUKReplacer 将**跟踪何时访问 Page 对象**，决定在必须**释放帧**以便为从磁盘复制新物理页面腾出空间时驱逐哪个对象。 在 BufferPoolManager 中将 **page_id 映射到 frame_id** 时，再次警告 STL 容器不是线程安全的。

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1689675567835-46d4bb35-9af4-49fa-a93b-38a75063913b.png)

**5、FlushPage 应刷新页面，而不管其 pin 状态。**

对于 FetchPage，如果空闲列表中没有可用页面并且当前所有其他页面都已固定，则应返回 nullptr。 

**6、UnpinPage，is_dirty 参数跟踪页面在固定锁期间时是否被修改。**

7、在 NewPage() 中创建新页面时，**AllocatePage** 私有方法会为 BufferPoolManager 提供一个唯一的新页面 ID。 另一方面，DeallocatePage() 方法是一个模拟**释放磁盘页面**的空操作，应该在 **DeletePage()** 实现中调用它。

### 缓冲池的实现

Buffer Pool Manager 给上层调用者提供的两个最重要的功能是 **new page 和 fetch page**

buffer Pool Manager 成员变量的作用：

- **pages：**buffer pool 中缓存 **pages 的指针数组**，数组记录每个内存page的信息（page_id_、pin_count_ 、is_dirty_、rwlatch、Data(磁盘page)）

- **disk_manager：**框架提供，可以用来读取 disk 上指定 **page id 的 page 数据**，或者向 disk 上给定 page id 对应的 page 里写入数据

- **page_table可扩展哈希表**：

  用来记录**page id 映射到 frame id**的映射关系，即 page 在 buffer pool 中的**位置**

  当调用指定**page_id**之后，能够快速地定位到**page在缓冲区中的位置**。

- **replacer：**LRU-K Replacer需要**驱逐 page 腾出空间**时，告诉我们应该驱逐哪个 page

- free_list：**空闲的 frame帧 列表**

#### New Page实现

上层调用者在缓冲池新建一个 page，只在缓冲池独一份，磁盘无该页面；page_id是新创建的，要传出参数返回给调用者，调用 NewPgImp。

如果当前 buffer pool 已满并且所有 page 都是 unevictable 的，直接返回。否则：

- 如果当前 buffer pool 里还有**空闲的 frame**，创建一个空的 page 放置在 frame 中。
- 如果当前 buffer pool 里没有空闲的 frame，但有 evitable 的 page，利用 LRU-K Replacer 获取可以**驱逐的 frame id**，将 frame 中原 page 驱逐，并创建新的 page 放在此 frame 中。驱逐时，
- **！！**如果**当前 frame 为 dirty**(发生过写操作)，将对应的 **frame 里的 page 数据写入 disk**，并重置 dirty 为 false。清空 frame 数据，并移除 **page_table 可扩展哈希表里的 page id**，移除 replacer 里的**引用记录**。
- 如果当前 frame 不为 dirty，直接清空 frame 数据，并移除 page_table 里的 page id，移除 replacer 里的引用记录。

在 replacer 里记录 **frame 的引用记录**，并将 frame 的 **evictable 设为 false**。因为上层调用者拿到 page 后可能需要对其进行**读写操作**，此时 page 必须驻留在内存中。

使用 **AllocatePage 分配一个新的 page id(从0递增)**。

将此 **page id 和存放 page 的 frame id** 插入 page_table可扩展哈希表。

page 的 pin_count 加 1。

#### Fetch Page实现

上层调用者指定一个 page id，这个页本身在缓冲池，Buffer Pool Manager 返回对应的 page 指针。调FetchPgImp。

假如可以在 buffer pool 中找到对应 page，直接返回。

否则需要**将磁盘上的 page 载入内存**，也就是放进 buffer pool**内存**。

如果当前 buffer pool 已满并且所有 page 都是 unevictable 的，直接返回空指针。否则同 New Page 操作，先尝试在 **free list** 中找**空闲的 frame** 存放需要读取的 page，如果没有 frame 空闲，就驱逐一张 page。获得一个空闲的 frame。

通过 disk_manager 读取 page id 对应 page 的数据，存放在空闲的 frame 中。在 replacer 里记录引用，将 evictable 设为 false，将 page id 插入 page_table，page 的 pin_count 加 1。

流程还是比较简单的，总的来说就是 buffer pool 里没空位也腾不出空位，直接返回，暂时处理不了请求，如果有空位，就先用空位，没空位但可以驱逐，就驱逐一个 page 腾出空位。这样就可以在内存中缓存一个 page 方便上层调用者操作。同时，还需要同步一些信息，比如 page_table可扩展哈希表 和 replacer替换器，**驱逐 page 时，如果是 dirty page 也需要先将其数据写回 disk**。

####  pin 和 unpin

当上层调用者新建一个 page 或者 fecth 一个 page 时，Buffer Pool Manager 会**自动 pin 一下这个 page**。

接下来上层调用者对这个 page 进行一系列**读写操作**，操作完之后调用 unpin，告诉 Buffer Pool Manager，这个 page 我用完了，你可以把它直接丢掉或者 flush 掉了（也不一定真的可以，可能与此同时有其他调用者也在使用这个 page，具体能不能 unpin 掉要 Buffer Pool Manager 在内部判断一下 **page 的 pin_count 是否为 0**）。

调用 unpin 时，同时传入一个 is_dirty 参数，告诉 Buffer Pool Manager 对这个 page 进行的是读操作还是写操作。要注意，Buffer Pool Manager 不能够直接将 page 的 dirty flag 设为 is_dirty。假设原本 dirty flag 为 true，则不能改变，代表其他调用者进行过写操作。只有原本 dirty flag 为 false 时，才能将 dirty flag 直接设为 is_dirty。

#### 缓冲池的整体流程：

1、执行引擎需要2号页，去缓冲池要。

2、缓冲池说没有，就去磁盘读

3、从磁盘读入数据到缓冲池

​      a. 缓冲池有足够空间，直接放入

​      b. 缓冲池空间不够，替换算法，选择一个页淘汰。

​               i.没脏，直接替换。

​               ii. 脏了。先写入磁盘，再替换（将对应的 **frame 里的 page 数据写入 disk**，并重置 dirty 为 false。清空 frame 数据，并移除 **page_table 可扩展哈希表里的 page id**，移除 replacer 里的**引用记录**。）

4、然后把2好页给执行引擎

##### 关键：

可扩展哈希充当了MMU的功能：page_id转为frame_id。

先 insert(page_id，frame_id)

后find(page_id,frame_id)



==============



# 面试lab1========

### lru-k 比 lru 好在哪

LRU-K算法引入一个"k"值，允许在最近"k"次访问内，就是一个缓存项在最近没有被使用，仍保留在缓存中。对于一些特定的访问模式，LRU-K能够更好地捕捉到频繁访问的特征，避免了LRU算法中由于不恰当地淘汰缓存项而导致的性能下降。

在LRU算法中，最久未被使用的缓存项会被优先淘汰，但是如果某个缓存项在最近被频繁使用，但在一段时间内不再使用，那么在LRU中该缓存项可能会被淘汰，而在此期间它实际上还有可能再次被使用。

### k怎么选择？依据？

较大的"k"值能够**更好地适应长期未使用但偶尔会被频繁使用**的情况，但也会增加缓存的管理开销。较小的"k"值可能更适合**强调近期访问模式**，但可能会错失某些长期未使用但会再次被用到的缓存项。

### **Lru-k的实现细节**

实现如下

(1). 数据第一次被访问，加入到访问历史列表；

(2). 如果数据在访问历史列表里后没有达到K次访问，则按照一定规则（FIFO，LRU）淘汰；

(3). 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序；

(4). 缓存数据队列中被再次访问后，重新排序；

(5). 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即：淘汰“倒数第K次访问离现在最久”的数据。

LRU-K具有LRU的优点，同时能够避免LRU的缺点，实际应用中LRU-2是综合各种因素后最优的选择，LRU-3或者更大的K值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。

用umap来访问页面对应的记录的队列，set用来排序，

**内存方面的占用，**分析了一下缓中池页面的个数和访问记录的个数以及k，

说明很小，这个不会是瓶颈







### 缓冲池用来干什么的，怎么实现的

**用处：**

缓冲池管理者负责从磁盘管理者中获取数据库页面并将它们存储在内存中。 缓冲池管理者需要**驱逐页面时**为新页面腾出空间时将脏页写出到磁盘。

#### 缓冲池的整体实现：

1、执行引擎需要2号页，去缓冲池要。

2、缓冲池说没有，就去磁盘读

3、从磁盘读入数据到缓冲池

​      a. 缓冲池有足够空间，直接放入

​      b. 缓冲池空间不够，替换算法，选择一个页淘汰。

​               i.没脏，直接替换。

​               ii. 脏了。先写入磁盘，再替换（将对应的 **frame 里的 page 数据写入 disk**，并重置 dirty 为 false。清空 frame 数据，并移除 **page_table 可扩展哈希表里的 page id**，移除 replacer 里的**引用记录**。）

4、然后把2好页给执行引擎



#### New Page实现

上层调用者在缓冲池新建一个 page，只在缓冲池独一份，磁盘无该页面；page_id是新创建的，要传出参数返回给调用者，调用 NewPgImp。

如果当前 buffer pool 已满并且所有 page 都是 unevictable 的，直接返回。否则：

- 如果当前 buffer pool 里还有**空闲的 frame**，创建一个空的 page 放置在 frame 中。
- 如果当前 buffer pool 里没有空闲的 frame，但有 evitable 的 page，利用 LRU-K Replacer 获取可以**驱逐的 frame id**，将 frame 中原 page 驱逐，并创建新的 page 放在此 frame 中。驱逐时，
- **！！**如果**当前 frame 为 dirty**(发生过写操作)，将对应的 **frame 里的 page 数据写入 disk**，并重置 dirty 为 false。清空 frame 数据，并移除 **page_table 可扩展哈希表里的 page id**，移除 replacer 里的**引用记录**。
- 如果当前 frame 不为 dirty，直接清空 frame 数据，并移除 page_table 里的 page id，移除 replacer 里的引用记录。

在 replacer 里记录 **frame 的引用记录**，并将 frame 的 **evictable 设为 false**。因为上层调用者拿到 page 后可能需要对其进行**读写操作**，此时 page 必须驻留在内存中。

使用 **AllocatePage 分配一个新的 page id(从0递增)**。

将此 **page id 和存放 page 的 frame id** 插入 page_table可扩展哈希表。

page 的 pin_count 加 1。

#### Fetch Page实现

上层调用者指定一个 page id，这个页本身在缓冲池，Buffer Pool Manager 返回对应的 page 指针。调FetchPgImp。

假如可以在 buffer pool 中找到对应 page，直接返回。

否则需要**将磁盘上的 page 载入内存**，也就是放进 buffer pool**内存**。

如果当前 buffer pool 已满并且所有 page 都是 unevictable 的，直接返回空指针。否则同 New Page 操作，先尝试在 **free list** 中找**空闲的 frame** 存放需要读取的 page，如果没有 frame 空闲，就驱逐一张 page。获得一个空闲的 frame。

通过 disk_manager 读取 page id 对应 page 的数据，存放在空闲的 frame 中。在 replacer 里记录引用，将 evictable 设为 false，将 page id 插入 page_table，page 的 pin_count 加 1。

流程还是比较简单的，总的来说就是 buffer pool 里没空位也腾不出空位，直接返回，暂时处理不了请求，如果有空位，就先用空位，没空位但可以驱逐，就驱逐一个 page 腾出空位。这样就可以在内存中缓存一个 page 方便上层调用者操作。同时，还需要同步一些信息，比如 page_table可扩展哈希表 和 replacer替换器，**驱逐 page 时，如果是 dirty page 也需要先将其数据写回 disk**。



### 为什么要做缓存池，操作系统不是有pagecache吗？

虽然缓冲池和Page Cache的目标是类似的，都是为了提高数据读写的性能，但它们在执行流程和应用领域上有一些区别。以下是它们之间的主要区别：

1. **应用领域**：
   - 缓冲池（Buffer Pool）主要用于数据库管理系统DBMS内部的一部分，用于缓存数据库的数据页，减少频繁的磁盘IO，提高数据库的读写性能。
   - Page Cache主要用于操作系统中的文件系统。它是操作系统内核实现的一种机制，用于在内存中缓存文件的数据块，以提高文件读取的性能。
2. **数据来源**：
   - 缓冲池主要缓存的是数据库的数据页。当数据库进行读取或写入操作时，数据首先会被缓存在缓冲池中。
   - Page Cache主要缓存的是文件系统中的数据块。当应用程序进行文件读取操作时，数据会被缓存到Page Cache中。
3. **粒度**：
   - 缓冲池以数据库的数据页为单位进行管理。每个缓冲池的页通常对应一个数据库数据页。
   - Page Cache以文件系统的数据块（通常是4KB大小）为单位进行管理。每个Page Cache的页对应一个文件系统的数据块。
4. **管理策略**：
   - 缓冲池的管理涉及到数据页的加载、替换、脏页回写等策略，以保持数据库的一致性和性能。
   - Page Cache的管理也涉及到数据块的加载、替换策略，但通常不需要关心数据的一致性，因为文件系统具有较强的一致性保障机制。
5. **应用场景**：
   - 缓冲池主要用于数据库系统，以提高数据库读写性能，适用于大规模数据处理场景。
   - Page Cache主要用于操作系统中的文件系统，用于加速文件读取，适用于一般的文件操作场景。



### fsync出现卡顿怎么处理 ？

`fsync`是一个用于将文件数据同步到磁盘的系统调用。当应用程序使用`fsync`将数据写入磁盘时，它会等待操作完成后再继续执行后续的操作。这可能会导致卡顿，特别是在磁盘 I/O 较慢的情况下。

处理`fsync`导致的卡顿问题，可以考虑以下几种方法：

1. **优化文件写入**：尽量减少不必要的`fsync`调用。你可以将多个小的文件写入合并为一个较大的文件写入，从而减少`fsync`的调用次数。此外，可以调整应用程序的写入策略，合理选择数据写入时机，避免频繁的写入操作。
2. **使用异步写入**：考虑使用异步写入（Asynchronous I/O），即应用程序在写入数据后立即返回，并在后台进行`fsync`操作。这样可以避免主线程被`fsync`调用阻塞。
3. **使用内存映射文件**：内存映射文件（Memory-mapped files）可以将文件映射到进程的虚拟内存空间，使得读写操作可以直接在内存中进行。系统会自动管理数据同步到磁盘的时机，减少显式的`fsync`调用。
4. **使用缓存**：应用程序可以使用缓存技术，将需要写入磁盘的数据暂存在内存中，然后定期进行批量写入。这样可以降低磁盘 I/O 的频率，减少`fsync`带来的卡顿。



### 实现的哈希表的作用

**Extendible Hashing** 是一种动态哈希方法，其中目录和桶被用于哈希数据。可扩展哈希是一个有力的灵活的方法，其中哈希函数也经历动态的改变。

Extendible Hash Table 由一个 directory 和多个 bucket 桶组成，在 Buffer Pool Manager 中主要用来存储 **buffer pool 中 page id 和 frame id** 的映射关系。



### 怎么判断页面在不在缓冲池里（待答）



### B+树在项目里的用途，每次修改都要刷到磁盘吗（待答）

### 





### unique_ptr 可以拷贝吗，shared_ptr线程安全吗？

unique_ptr` 不能直接拷贝。它表示对动态分配对象的独占所有权，并且使用移动语义来传递所有权。你可以使用 `std::move` 将所有权从一个 `unique_ptr` 转移到另一个，但不能像普通指针或其他智能指针那样直接拷贝。

如果需要在多线程环境中使用 `shared_ptr` 并且有多个线程修改所指向的对象，你需要互斥锁以确保对象的数据线程安全。

虽然 `shared_ptr` 在引用计数机制上是线程安全的（即多个线程可以安全地创建 `shared_ptr` 的副本），但它并不保证管理的对象在多线程环境下是线程安全的。多个线程同时访问和修改由 `shared_ptr` 管理的对象可能会导致数据竞争和其他未定义行为。



# Lab2 B+树============

### 1 B+树和B树的区别？

B树和B+树都是常见的自平衡树数据结构，用于管理有序数据的索引。它们的区别主要体现在以下几个方面：

- 数据项存储：B树中，每个节点既存储索引键也存储对应的数据，即索引键和数据在同一节点；而B+树中，只有叶子节点存储数据，非叶子节点只存储索引键，数据全部存储在叶子节点中，这使得B+树的非叶子节点可以存储更多的索引项。
- 叶子节点关联：B树的叶子节点之间互相关联，形成一个链表，可以支持范围查询。而B+树的叶子节点之间也是关联的，但不形成链表，只有一个指针指向最左的叶子节点，因此只能通过最左叶子节点进行范围查询。
- 节点分裂：B树在插入新节点时可能导致节点分裂，并更新父节点的索引，导致频繁的平衡操作。B+树在插入时只需在叶子节点进行分裂，并不需要修改父节点的索引，从而减少了平衡的开销。
- 查找性能：由于B+树的非叶子节点存储更多的索引项，相对于B树，B+树的高度更低，减少了查找的IO次数，从而提高了查找性能。

### 2 为什么MySQL使用B+树而不使用B树？

 MySQL使用B+树作为其默认的索引结构，而不使用B树，主要有以下几个原因：

- 内存使用效率：B+树的非叶子节点只存储索引项，而叶子节点存储实际数据，这使得B+树的内存使用更加高效，每个节点可以存储更多的索引项。
- 范围查询性能：由于B+树的叶子节点形成链表，可以支持范围查询，对于数据库的范围查询非常常见，因此B+树更适合作为数据库索引结构。
- 顺序访问性能：B+树的叶子节点之间有序排列，便于范围扫描和顺序访问，这对于数据库的全表扫描等操作有很大的优势。
- 减少IO次数：B+树的高度更低，每次查询只需较少的IO次数，减少了磁盘IO的开销，提高了查询性能。
- 有序性：B+树的叶子节点形成有序链表，方便范围查询和排序操作。

### 3 B树的应用场景有哪些？

B树在数据库、文件系统等领域有广泛的应用。主要应用场景包括：

- 数据库管理系统：B树用于数据库的索引结构，加快对数据库表的查找、插入和删除操作。
- 文件系统：B树可以用于文件系统的索引，加快对文件的查找和存取。
- 操作系统内存管理：操作系统中的虚拟内存管理和页面置换算法中，B树可用于管理页面的访问和替换。
- 网络路由表：在路由器等网络设备中，B树用于快速查找最长匹配的路由表项。
- 数据库索引结构：B树还可以用于数据库管理系统的其他索引结构，如B+树、B*树等的基础实现。

### 4 B+树和LSM-Tree的使用场景？LSM-Tree存在的问题有哪些？针对这些问题有什么解决思路？

- B+树的使用场景：
  - 数据库管理系统：B+树作为数据库的主要索引结构，用于加速对数据表的查找、插入和删除操作。
  - 文件系统：B+树可以用于文件系统的索引，提高对文件的查找性能。
  - 数据库缓存：B+树可用于数据库缓存的管理，加速对缓存数据的访问。
- LSM-Tree的使用场景：
  - 分布式存储系统：LSM-Tree适用于分布式存储系统中的数据索引和存储结构，如Apache Cassandra等。
  - 日志结构化存储：LSM-Tree常用于日志结构化存储引擎中，如LevelDB、RocksDB等。

LSM-Tree存在的问题包括：

- 写放大：由于LSM-Tree采用多层次结构，写操作可能会导致数据在多个层次进行写入，从而导致写放大问题，增加了写入开销。
- 读放大：读操作可能需要在多个层次进行查找，从而导致读放大问题，增加了读取开销。
- 写放大和读放大问题导致了LSM-Tree在高并发写和随机读取场景下性能下降。

针对LSM-Tree的问题，一些解决思路包括：

- 使用B+树优化：可以在LSM-Tree的某些层次引入B+树结构，减少写放大和读放大的问题。
- 合并和压缩：定期合并和压缩LSM-Tree中的层次，减少不必要的层次，优化读取性能。
- 前缀压缩：在写入时对相邻数据进行前缀压缩，减少数据的冗余和写入开销。
- 原子写入：引入原子写入操作，减少写入时可能的重复写入问题。

### 5 B+树并发处理有哪些优化思路？Blink树有什么特点？

B+树并发处理的优化思路主要包括：

- 读写锁：使用读写锁（Read-Write Lock）来允许多个读操作并发执行，但只允许一个写操作。这样可以保证读操作之间的并发性，同时避免写操作之间的竞争条件。
- 锁粒度优化：减小锁的粒度，尽量只锁定需要修改的节点，而不是整棵树。这样可以增加并发性，减少锁冲突。
- 乐观并发控制：引入乐观并发控制机制，不立即加锁，而是在修改节点时进行版本检查，只有在版本匹配时才进行更新。

Blink树是一种基于B+树的改进版本，它的特点包括：

- 支持部分节点分裂：Blink树支持将节点的部分关键字进行分裂，而不是整个节点，这样可以减少节点的复制和移动操作，提高性能。
- 延迟更新：Blink树引入了延迟更新的概念，在节点被分裂时，并不立即进行更新，而是推迟到后续某个时刻进行，从而减少了更新的开销。
- 无锁并发：Blink树尽量避免使用锁，通过乐观并发控制和版本管理来实现并发处理，提高了并发性能。
- 减少复制：Blink树通过避免节点的完全复制和移动，减少了数据的冗余拷贝，提高了更新性能。

Blink树的设计目标是在高并发和大规模数据处理场景下提供更高的性能和扩展性。

### 6.螃蟹锁策略

1. 先锁住 parent page，
2.  再锁住 child page， 
1. 假设 child page 是*安全*的，则释放 parent page 的锁。*安全*指当前 page 在当前操作下一定不会发生 split/steal/merge。同时，*安全*对不同操作的定义是不同的，Search 时，任何节点都安全；Insert 时，判断 max size；Delete 时，判断 min size。



# 面试题lab2=========

#### B+ 树并发管理怎么做，这么做的好处？

B+树的并发管理的处理在于**读取操作的并发和写入操作的并发。**

对于读取操作的并发管理，B+树通常是多线程安全的。多个线程可以同时进行搜索操作，因为在搜索过程中只涉及到读取数据，而B+树的结构和数据是不会改变的。**读取操作可以并发执行，无需加锁。**

对于写入操作的并发管理，需要**使用锁来保护B+树的结构和数据的一致性**。在**插入或删除节点时，需要获取相应的锁**来保护关键数据结构，以防止**多个线程同时修改B+树**的结构，从而导致数据错误。常见的锁策略包括细粒度锁和粗粒度锁、**互斥锁**

**好处：**

1. 并发查询：多个线程可以同时进行读取操作，提高系统查询性能。
2. 数据一致性：通过适当的锁策略，可以保证多线程下对B+树的修改是安全的，从而确保数据一致性。

#### 乐观锁怎么做，悲观锁？ 

乐观锁和悲观锁是两种不同的并发控制机制：

1. **乐观锁：** 乐观锁假设在大多数情况下，数据访问是不会发生冲突的。当进行写入操作时，乐观锁不会立即加锁，而是先进行操作，然后在提交时检查是否发生冲突。如果没有冲突，则提交成功；如果发生冲突，就需要回滚重试。

乐观锁适用于读操作较多、冲突较少的情况，可以减少锁的开销，但需要处理冲突和重试的逻辑。

​    **2.悲观锁：** 悲观锁假设数据访问可能会发生冲突，因此在进行任何操作之前，会先获取适当的锁来保护数据。在读取或写入操作期间，其他线程必须等待锁的释放。

悲观锁适用于写操作较多、冲突较多的情况，保证了数据的完整性，但可能会降低并发性能。

#### 乐观悲观什么区别，适用什么场景？

1. 区别： 乐观锁是在操作提交时检查冲突，不加锁进行操作；悲观锁是先获取锁，然后进行操作，其他线程必须等待锁释放。
2. 适用场景： 乐观锁适用于冲突较少的情况，减少锁的开销，适合读操作较多的场景。例如，多个线程读取同一数据，但写操作较少。

悲观锁适用于冲突较多的情况，保证数据的完整性，适合写操作较多的场景。例如，多个线程同时修改同一数据。

#### 讲一讲优化器?

优化器负责解析和优化SQL查询，以获得最佳的执行计划。优化器的目标是选择最有效的执行计划，使得查询在最短的时间内返回正确的结果，并且消耗最少的系统资源。

优化器的工作过程：

1. 解析：首先，优化器对SQL查询进行解析，识别查询中的表、列、条件和连接等信息。
2. 查询重写：优化器可能会对查询进行重写，将复杂的查询转换成等价的简化形式，以便更好地进行优化。
3. 统计信息收集：优化器需要收集与查询相关的表和索引的统计信息，如表的行数、索引的唯一性等。这些统计信息有助于优化器评估不同执行计划的成本。
4. 生成执行计划：优化器根据查询的结构和统计信息，生成不同的执行计划。执行计划是指执行查询的具体操作步骤，包括表的访问顺序、连接方式、索引使用等。
5. 成本估算：对于生成的不同执行计划，优化器会估算每个执行计划的成本，以便选择成本最低的执行计划作为最终的执行方式。
6. 选择最优执行计划：优化器通过成本估算选择最优的执行计划，并将其传递给执行引擎执行查询。

#### 如果改变表 join 顺序

如果改变表的 JOIN 顺序，可能会对查询的性能产生影响。JOIN 的顺序可以影响查询执行计划，从而导致不同的执行时间和资源消耗。数据库查询优化器通常会尝试找到最优的执行计划，但在某些情况下，改变 JOIN 的顺序可能会导致性能的提升或下降。

如果改变表 join 顺序，优化器会根据查询的条件、表的大小和索引等信息重新生成新的执行计划。优化器可能会尝试不同的表 join 顺序，然后选择成本最低的执行计划。

#### 没有 join 条件怎么办

如果没有明确的 JOIN 条件，无法直接对表进行连接。在 SQL 中，连接（JOIN）操作需要明确的关联条件来将不同的表连接在一起。

**联合查询（UNION）**：如果这些表具有相同的列结构，可以使用 UNION 操作将它们合并成一个结果集。UNION 操作将多个查询结果合并为一个结果集，并去除重复的行。

**交叉连接（Cartesian Product）**：如果你希望返回所有表的所有组合，可以使用交叉连接。交叉连接将返回表之间的笛卡尔积，即每个表的每一行都与其他表的每一行组合在一起。

**子查询和聚合**：如果没有连接条件，你可以使用子查询或者聚合函数来获取每个表的汇总信息。

#### 10个表一起 join

在数据库中，同时连接（JOIN）10个表可能会导致查询性能大幅下降，尤其是在数据量较大的情况下。多表连接的操作会涉及大量的数据比较和组合，这可能会导致查询的执行时间显著增加，并且消耗更多的系统资源。

当需要连接多个表时，可以考虑以下方法来优化查询：

1. **筛选和优化数据：** 在连接之前，使用WHERE条件来筛选数据，尽量减少连接的数据量。优化数据可以使连接操作更加高效。
2. **合理选择连接的表顺序：** 根据数据量大小和预期结果，选择合适的表连接顺序。通常，将较小的表放在前面，这样可以减少连接操作的数据量。
3. **使用索引：** 确保连接字段上有索引。索引可以大幅提高连接操作的速度，特别是当连接字段不是主键时。
4. **使用合适的JOIN类型：** 根据连接条件和需求，选择合适的JOIN类型（如INNER JOIN，LEFT JOIN等）来减少不必要的数据匹配。
5. **考虑使用子查询：** 将一部分连接操作封装成子查询，然后再与其他表进行连接。这有助于将复杂的连接操作拆分成较小的部分，提高查询的可读性和性能。





### 数据库为什么要用 buffer pool，不用 mmap 管理内存。

数据库使用 Buffer Poo用于缓存数据库的数据和索引，从而减少对物理磁盘的频繁读写操作，提高数据的访问速度。

相比于使用 mmap 管理内存，Buffer Pool 在数据库内部有以下优势：

1. **数据预读和缓存策略**：Buffer Pool 提供了高级的缓存管理策略，如预读（Read-Ahead）机制。数据库可以根据读写模式和数据访问模式预先将数据缓存到内存中，避免频繁的磁盘访问。
2. **脏数据管理**：Buffer Pool 允许数据库管理脏数据（Dirty Data），即已被修改但尚未写回磁盘的数据。数据库可以使用合适的机制来延迟将脏数据写回磁盘，从而在一定程度上平衡内存和磁盘的负载。
3. **内存控制和限制**：Buffer Pool 允许数据库设置内存限制，从而避免数据库过度消耗系统内存，导致系统性能下降或发生内存不足问题。
4. **并发控制**：Buffer Pool 可以与数据库的并发控制机制结合使用，确保多个并发事务对共享数据的访问不会导致数据一致性问题。
5. **持久性**：Buffer Pool 可以通过合适的机制保证数据的持久性，确保在数据库异常崩溃或重启时数据不会丢失。

**尽管 mmap** 也可以用于将文件映射到内存中，但它**更适合处理大文件的读写操作**，而不是像数据库这样的复杂数据管理系统。使用 mmap 时，数据库可能**难以有效地控制内存缓存、脏数据管理、预读策略等高级功能。**

### nested loop join 具体怎么做。与hash join 的区别

**Nested Loop Join（嵌套循环连接）**：

嵌套循环连接适用于连接较小表（驱动表）和较大表（被驱动表）。它的工作原理如下：

1. 从驱动表中取出一行数据（称为外部循环）。
2. 对于每一行驱动表的数据，遍历被驱动表，检查是否满足连接条件。
3. 如果满足连接条件，则将驱动表的数据和被驱动表的数据组合成一行，并添加到结果集中。
4. 重复上述步骤，直到驱动表中的所有行都与被驱动表进行了匹配。

嵌套循环连接的优点是适用于任意连接条件，不需要额外的内存，但缺点是在连接大表时性能可能较差，因为需要对大表进行多次遍历。

**Hash Join（哈希连接）**：

哈希连接是一种高效的连接算法，适用于连接较大表。它的工作原理如下：

1. 将驱动表的连接字段（Join Key）通过哈希函数计算，将结果存储在哈希表中。
2. 遍历被驱动表的每一行数据，并通过哈希函数计算连接字段的哈希值。
3. 在哈希表中查找是否存在与当前被驱动表行的连接字段哈希值相等的驱动表行。
4. 如果存在匹配的驱动表行，则将驱动表的数据和被驱动表的数据组合成一行，并添加到结果集中。

哈希连接的优点是适用于连接较大表，具有较好的性能，特别是当连接字段上有适当的索引时。但缺点是需要额外的内存用于构建哈希表，如果内存不足可能导致性能下降。

**区别**：

1. **适用场景**：嵌套循环连接适用于连接较小表，而哈希连接适用于连接较大表。
2. **内存消耗**：嵌套循环连接不需要额外的内存，而哈希连接需要使用内存构建哈希表。
3. **连接条件**：嵌套循环连接适用于任意连接条件，而哈希连接需要计算哈希值，适用于等值连接条件。

### 什么是由用 hash join 什么时候 用 nested loop join ?

Hash Join适用于连接较大的表，且连接条件是等值连接的情况。而Nested Loop Join适用于连接较小的表，且适用于任意连接条件。

**Hash Join适用情况**：

1. **连接较大表**：Hash Join适用于连接较大的表，因为它的性能在处理大表时通常更优秀。对于大表，Hash Join能够充分利用哈希表的索引，以较快的速度完成连接操作。
2. **等值连接条件**：Hash Join适用于等值连接条件，即连接条件使用"="来进行匹配。在等值连接的情况下，Hash Join的哈希表可以更好地处理连接字段的匹配。

**Nested Loop Join适用情况**：

1. **连接较小表**：Nested Loop Join适用于连接较小的表，尤其是当一个表非常小，而另一个表相对较大时。对于小表，嵌套循环连接的性能通常较好。
2. **任意连接条件**：Nested Loop Join适用于任意连接条件，包括等值连接条件、不等值连接条件以及其他复杂的连接条件。因为它不依赖于哈希表，可以灵活地处理各种连接条件。

### 除了火山模型还了解什么？有什么区别？

1. **迭代器模型（Iterator Model）**：迭代器模型是一种基于迭代器的查询处理模型，也称为迭代器模式。它将查询计划表示为一系列迭代器，每个迭代器代表一个基本操作，例如扫描表、连接、聚合等。迭代器模型的优点在于简单直观，易于实现和理解。与火山模型相比，迭代器模型更注重执行过程中的数据流控制。
2. **管道模型（Pipeline Model）**：管道模型是一种将查询执行过程分解为多个阶段的模型，每个阶段处理查询的一部分操作。查询执行过程类似于流水线，每个阶段处理一个算子，然后将结果传递给下一个阶段。管道模型可以提高查询的并行性和效率。
3. **代价模型（Cost Model）**：代价模型是一种基于代价估算的查询优化模型。在代价模型中，优化器会为每个可能的查询计划估算执行代价，并选择代价最小的计划作为最优计划。代价模型允许优化器考虑不同的执行代价，从而选择最适合当前数据和系统资源的执行计划。
4. **基于规则的优化（Rule-Based Optimization）**：基于规则的优化是一种通过应用一系列规则来转换查询计划的方法。这些规则包括代数规则、逻辑等价性规则等，用于将查询计划转换为更高效的形式。

区别：

1. **抽象层次不同**：火山模型、迭代器模型和管道模型都是在查询执行层面的模型，着重于描述查询计划的执行过程和数据流。而代价模型和动态规划方法是在查询优化层面的模型，用于选择最优的查询计划。
2. **执行策略不同**：火山模型和迭代器模型都采用基于迭代器的执行策略，即按照算子顺序逐个处理数据。管道模型将查询过程分解成多个阶段，每个阶段处理一个算子，实现并行执行。
3. **优化方式不同**：火山模型、迭代器模型和管道模型通常不涉及具体的优化策略，更侧重于执行过程的控制和数据流的传递。代价模型和动态规划方法是优化器使用的策略，用于选择最优的查询计划。
4. **复杂性不同**：火山模型、迭代器模型和管道模型相对较简单，容易理解和实现。代价模型和动态规划方法通常涉及更复杂的数学和计算过程，需要更高级的优化算法。

### 用火山模型一次一次吐出多个 tuple 和向量化模型有什么区别？

向量化模型是一种高效的查询执行方式，特别适用于处理大量数据的计算密集型任务。它充分利用硬件的矢量化指令和并行性，加速查询执行过程。火山模型相对简单，适用于一般性的查询执行，但在处理大规模数据时可能性能较差。在现代数据库系统中，一些查询引擎会尝试在特定情况下采用向量化模型来获得更好的性能。

**火山模型和向量化模型是数据库查询处理和优化中两种不同的执行策略，它们在处理多个 tuple（元组）时有不同的方式。**

**火山模型**：

在火山模型中，数据库查询执行过程被抽象成一系列的算子，每个算子处理一个 tuple。每次执行一个算子，它从输入流中取出一个 tuple，并进行相应的处理，然后将结果传递给下一个算子。这种方式下，处理每个 tuple 需要依次经过算子的处理过程，数据在不同算子之间以数据流的形式传递，一个 tuple 的处理完成后，下一个 tuple 才会被处理。这是一个逐个处理 tuple 的过程。

**向量化模型**：

向量化模型是一种更为高效的查询执行方式，它以向量的形式处理多个 tuple。在向量化模型中，不是一个一个地处理 tuple，而是将多个 tuple 同时处理成一个向量（Vector）。这种方式允许在一个操作中同时处理多个 tuple 的数据，从而充分利用现代计算机的矢量化指令集和硬件优化。

**区别**：

1. **处理方式**：火山模型逐个处理 tuple，一个 tuple 的处理完成后才会处理下一个 tuple；而向量化模型以向量的形式处理多个 tuple，充分利用矢量化指令，可以同时处理多个 tuple 的数据。
2. **计算效率**：向量化模型通常比火山模型执行速度更快，因为它利用了硬件的并行性和优化指令集。向量化操作能够更有效地利用 CPU 和内存的计算能力，从而加速查询执行过程。
3. **硬件优化**：向量化模型更容易受益于硬件优化。现代 CPU 和GPU都支持矢量化指令，这些指令能够同时处理多个数据，向量化模型更容易与这些硬件优化相结合，获得更好的执行性能。
4. **实现复杂性**：向量化模型的实现相对复杂，需要设计和编写能够利用矢量化指令的操作，以及合适的内存访问策略。相比之下，火山模型的实现较为简单。

### 怎么实现基于代价优化？为什么交换表的顺序就能达到优化效果。

基于代价优化是数据库查询优化中常见的一种方法，它的目标是选择最优的查询计划，使得查询在最短时间内得到执行结果。实现基于代价优化的过程通常分为以下几个步骤：

1. **查询解析和查询树构建**：首先，数据库系统会对用户提交的查询进行解析，将查询语句转换成查询树或者表达式树的形式。查询树表示查询的逻辑结构，包括选择、投影、连接等操作。
2. **生成所有可能的查询计划**：在这一步，数据库系统会为查询树生成所有可能的查询计划。由于查询计划通常是一棵树状结构，可能有多种不同的组合方式，所以会生成多个查询计划。
3. **代价估算**：对于每个生成的查询计划，数据库系统会估算执行该计划的代价。代价通常是以时间为单位的估算，代表执行该计划所需要的计算和IO成本。
4. **选择最优查询计划**：根据代价估算结果，数据库系统会选择代价最小的查询计划作为最优计划。这就是基于代价优化的核心步骤，选择代价最小的计划可以使查询在最短时间内得到执行结果。

**为什么交换表的顺序能够达到优化效果呢？**

在查询优化中，选择不同的连接顺序会影响查询计划的执行效率。对于多表连接的查询，表的连接顺序不同，生成的查询计划也会不同。由于不同表的大小、索引情况、数据分布等因素不同，选择不同的连接顺序可能会导致不同的查询性能。

通过交换表的连接顺序，优化器可以尝试不同的查询计划，估算每个计划的代价，然后选择代价最小的计划作为最优计划。在交换表的顺序时，优化器会利用表的统计信息和索引信息，结合代价估算模型来进行选择。

例如，对于三表连接A JOIN B JOIN C，优化器可以尝试两种连接顺序：(A JOIN B) JOIN C 和 A JOIN (B JOIN C)。假设 A 表很大而 B 表和 C 表较小，那么在第一种连接顺序下，首先将 A 和 B 连接，产生的结果再与 C 连接，可能会有大量的中间结果需要处理，导致代价较大；而在第二种连接顺序下，首先将 B 和 C 连接，产生的结果再与 A 连接，中间结果较小，代价较小。因此，优化器可能会选择第二种连接顺序作为最优计划。

总的来说，交换表的连接顺序是基于代价优化的一种策略，通过尝试不同的连接顺序，选择代价最小的查询计划，以达到优化查询执行性能的目的。

### 事务ACID

事务ACID是数据库事务的四个特性，分别代表着原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。这四个特性确保了数据库事务的正确执行和数据的完整性。

### 事务持久化是通过什么实现的

**重做日志（Redo Log）**：重做日志是用于实现事务持久性的一种日志类型。当事务提交时，它的修改操作被记录到重做日志中。重做日志用于在数据库发生故障时，将已提交的事务修改操作重新执行一遍，从而恢复数据库到故障之前的状态。

通过使用日志系统实现事务持久化，数据库能够保证在事务提交之前，即使系统发生故障或崩溃，修改的结果也不会丢失。在数据库重新启动后，可以通过回放日志中的操作，将数据库恢复到故障之前的状态，确保数据的一致性和持久性。日志系统是数据库事务ACID特性中持久性（Durability）的重要实现手段。



## ===================

### 445内存管理如何做的

内存管理主要用于缓存数据库的数据和索引，以提高查询性能。数据库通常使用Buffer Pool（缓冲池）来管理内存，将热门的数据和索引缓存到内存中，减少对磁盘的频繁读写操作。内存管理包括以下方面：

- 内存分配：分配合适大小的内存块来存储数据和索引。
- 页面替换：当内存不足时，需要从缓冲池中选择合适的页面进行替换，通常采用缓存淘汰策略（如LRU）来决定替换哪些页面。
- 内存回收：释放不再使用的内存块，以便其他数据和索引可以使用。

### lruk怎么实现？ 淘汰策略？与普通lru比的好处。

LRUK（Least Recently Used with k-Recency）是一种缓存淘汰策略，它是基于LRU（Least Recently Used，最近最少使用）和LFU（Least Frequently Used，最不经常使用）淘汰策略的结合，旨在提高缓存系统的性能。

**LRUK的实现**：

LRUK维护了一个缓存页面的列表，列表中的每个页面都有一个访问计数值和一个时间戳。在访问页面时，会更新页面的访问计数值和时间戳。当需要淘汰页面时，LRUK根据页面的访问计数值和时间戳来选择淘汰哪些页面。

**淘汰策略**：

在LRUK中，淘汰页面的优先级由以下两个因素决定：

1. **k-Recency**：表示一个页面在过去k个页面访问中被访问的次数。k-Recency越大，表示该页面在最近的k个访问中越活跃。LRUK考虑了k-Recency来保留活跃的页面。
2. **Time-to-live (TTL)**：表示一个页面在缓存中的存活时间。TTL越小，表示该页面被访问的时间越久。LRUK考虑了TTL来淘汰访问时间较久的页面。

综合以上两个因素，LRUK根据页面的k-Recency和TTL来进行页面淘汰。

**与普通LRU比较的好处**：

普通LRU是一种经典的缓存淘汰策略，它根据页面的最近访问时间来进行淘汰，即最久未被访问的页面先被淘汰。与普通LRU相比，LRUK的优势在于更加智能地选择页面淘汰，考虑了页面的访问频率和时间。由于LRUK考虑了k-Recency和TTL，可以更好地适应不同访问模式下的缓存需求。

**好处包括**：

1. **更好的适应性**：LRUK考虑了页面的访问频率和时间，使得缓存系统更加适应不同的访问模式。对于活跃页面，LRUK可以保留它们以提高缓存命中率；对于长时间未被访问的页面，LRUK可以及时淘汰，释放空间。
2. **更好的性能**：由于LRUK基于更多的信息进行淘汰策略的选择，可以在某些情况下比普通LRU更好地提高缓存的性能和效率。
3. **灵活性**：LRUK的参数k可以根据具体应用场景进行调整，从而灵活地控制淘汰策略的行为。

### b+树并发控制怎么做？悲观乐观性能差距。

在B+树中，对于并发控制，可以采用悲观并发控制和乐观并发控制两种方式。

**悲观并发控制**：

在悲观并发控制中，假设在并发执行过程中会发生冲突，因此在访问B+树节点之前，会先获取相应的锁。悲观并发控制采用锁的机制，来限制对共享资源的访问。常见的锁包括共享锁（Shared Lock）和排他锁（Exclusive Lock）。

- 共享锁（Shared Lock）：多个事务可以同时获得共享锁，用于读取节点的内容，不允许修改。
- 排他锁（Exclusive Lock）：排他锁只允许一个事务获取，用于修改节点的内容，其他事务不能同时获取共享锁或排他锁。

悲观并发控制可以保证数据的一致性，因为在访问B+树节点之前会先获取锁，所以不会出现数据冲突的情况。但是悲观并发控制需要频繁地加锁和解锁操作，可能会导致并发性能下降。

**乐观并发控制**：

乐观并发控制假设在并发执行过程中不会发生冲突。在访问B+树节点时，不获取锁，而是通过版本号或时间戳来判断数据是否过期。如果在读取数据后，发现数据已经被其他事务修改，则进行回滚或重新尝试。

乐观并发控制可以提高并发性能，因为不需要频繁地加锁和解锁，事务可以并行执行。然而，乐观并发控制需要进行冲突检测和重试，可能会导致一些额外的开销。

**性能差距**：

悲观并发控制相对保守，确保数据的一致性，但由于需要频繁地加锁和解锁，可能导致并发性能下降。尤其在高并发的情况下，锁竞争会导致事务等待和阻塞，影响数据库性能。

乐观并发控制则更加乐观，可以提高并发性能，允许事务并行执行，减少了锁竞争。但是它需要进行冲突检测和重试，可能会导致一些额外的开销。

在实际应用中，如果数据的一致性要求较高，可以选择悲观并发控制；如果对并发性能要求较高，可以选择乐观并发控制。同时，也可以考虑混合使用两种方式，根据不同的数据访问模式和场景来灵活选择并发控制策略。

### 事务并发控制如何做？锁的类型有哪几种

事务并发控制是数据库管理系统为了保证并发执行的多个事务之间不会产生不一致的结果而采取的措施。常见的并发控制方法包括锁定和时间戳。

**事务并发控制方法**：

1. **锁定（Locking）**：事务在访问数据之前会先获取适当的锁来限制对共享资源的访问。当事务需要读取或修改某个资源时，会尝试获取相应的锁。如果锁不可用（即被其他事务持有），事务可能会等待或被阻塞，直到锁可用为止。锁定是悲观并发控制的一种实现方式，可以保证数据的一致性。
2. **时间戳（Timestamping）**：时间戳是为每个事务分配的唯一标识符，用于标记事务执行的顺序。每个事务都有一个开始时间戳和一个结束时间戳。事务在执行过程中根据时间戳的顺序来决定是否执行、回滚或等待。时间戳是乐观并发控制的一种实现方式，可以提高并发性能。

**锁的类型**：

在锁定的并发控制方法中，常见的锁类型有：

1. **共享锁（Shared Lock）**：也称为读锁（Read Lock），允许多个事务同时获取共享锁，用于读取共享资源。多个事务可以同时持有共享锁，但不能持有排他锁。共享锁之间不互斥，可以共享资源。
2. **排他锁（Exclusive Lock）**：也称为写锁（Write Lock），排他锁只允许一个事务获取，用于修改或写入资源。排他锁之间和共享锁之间都互斥，即在一个事务持有排他锁期间，其他事务不能获取共享锁或排他锁。

在悲观并发控制中，事务在访问数据时需要先获取适当的锁，以避免数据冲突。共享锁用于读取操作，排他锁用于写入操作。根据事务的访问模式和数据访问要求，事务可以根据需要选择获取共享锁或排他锁来实现并发控制。

锁的类型和并发控制方式的选择会根据具体的应用场景和性能需求来决定，不同的场景可能选择不同的锁类型和并发控制策略来达到最佳性能和数据一致性。

### 死锁依赖关系怎么建立的


死锁依赖关系是指多个事务之间因为相互持有对方所需资源而导致发生死锁的情况。在数据库系统中，死锁依赖关系可以通过以下步骤来建立：

1. **多个事务同时获取资源**：假设有多个事务同时运行，并且每个事务需要获取多个资源才能完成自己的操作。
2. **相互等待资源**：当多个事务同时运行时，某个事务可能已经获取了部分资源，但因为其他事务同时需要这些资源，所以它暂时无法继续执行。这个事务会进入等待状态，等待其他事务释放所需的资源。
3. **持有资源不释放**：同时，其他事务也可能因为需要被第一个事务持有的资源而无法继续执行，于是它们也进入等待状态。
4. **形成循环依赖**：当多个事务之间的资源请求形成一个环状结构时，就形成了死锁依赖关系。也就是说，事务A等待事务B持有的资源，事务B等待事务C持有的资源，而事务C又等待事务A持有的资源，这样形成了一个闭环，导致所有事务都无法继续执行，进入了死锁状态。

举例说明：

假设有两个事务A和B，它们需要访问两个资源X和Y，并且按照以下顺序获取资源：

- 事务A获取资源X，然后等待获取资源Y。
- 事务B获取资源Y，然后等待获取资源X。

在这种情况下，事务A和B形成了一个闭环，因为它们互相持有对方需要的资源，导致了死锁。事务A等待事务B释放资源Y，而事务B等待事务A释放资源X，造成了死锁依赖关系。

解决死锁的方法包括死锁检测和死锁解除策略，通常数据库系统会采用这些策略来检测和解除死锁，以保证系统的稳定性和正确性。

六、
 两阶段锁（加锁阶段/解锁阶段）流程

两阶段锁（Two-Phase Locking，2PL）是一种常见的并发控制协议，用于管理事务对共享资源的访问。该协议分为两个阶段：加锁阶段和解锁阶段。在加锁阶段，事务需要获取所有需要的锁，而在解锁阶段，事务需要释放已经持有的锁。这样可以避免死锁的发生，并确保事务的串行化执行。

**两阶段锁的流程如下**：

1. **加锁阶段（Growing Phase）**：
   - 事务开始：事务开始时，处于加锁阶段。
   - 事务获取锁：当事务需要访问某个共享资源时，它会请求获取相应的锁。根据事务的操作类型（读取或修改），可以获取共享锁（Shared Lock）或排他锁（Exclusive Lock）。
   - 锁定资源：事务成功获取锁后，将共享资源锁定，其他事务不能同时获取相同资源的排他锁，或者不能同时获取相同资源的共享锁。
   - 事务操作：在加锁阶段，事务执行读取或修改操作，但不能释放已经获取的锁。
2. **解锁阶段（Shrinking Phase）**：
   - 事务操作完成：在事务执行完所有操作后，进入解锁阶段。
   - 事务释放锁：在解锁阶段，事务开始释放已经持有的锁，将锁定的资源解锁。
   - 锁释放完毕：事务释放所有锁后，它就完成了解锁阶段，最终结束。

**注意事项**：

- 在加锁阶段，事务可以获取锁，但不能释放锁。
- 在解锁阶段，事务可以释放锁，但不能获取新的锁。
- 事务在加锁阶段持有的锁不会释放，直到进入解锁阶段。
- 两阶段锁协议的主要目标是防止死锁的发生，但不能完全杜绝死锁的可能性。因此，仍然需要死锁检测和解除机制来处理潜在的死锁情况。

两阶段锁是数据库管理系统中常用的并发控制协议之一，通过对事务加锁和解锁的合理管理，确保数据库操作的一致性和正确性。

 死锁如何检测
B+树如何实现并发
乐观悲观实现





# Lab3 查询执行============

### 1 什么是火山模型？

火山模型（Volcano Model）是数据库查询优化和执行的一个经典计算模型，用于描述查询执行计划的执行流程。火山模型将查询执行过程看作是数据流图，其中数据流从输入到输出经过多个算子（操作符），类似火山爆发时岩浆从火山口喷发的过程，因此得名“火山模型”。

### 2 为什么要使用火山模型？有什么优点？有什么缺点？缺点从哪些角度进行优化？

- **定义：火山模型，也叫迭代器模型，是查询执行时最常用的模型。**
- **特点：火山模型将整个SQL构建成一颗运算符树，每一个运算符节点实现一个Next()函数，每次调用这个函数将会返回算子产生的一行数据tuple。根节点循环向子节点调用Next()方法，直到数据拉取完毕。**
- **优点：可以单独地实现每个运算符，不需要关注其他运算符的实现逻辑。**
- **缺点：每次计算一个tuple，不利于CPU缓存发挥作用。**
- **优化方式：编译执行模型和向量化执行模型。向量化模型每一个Next()函数返回的都是批量的数据而不只是一个元组。**

使用火山模型的**主要目的是优化查询执行计划**，从而提高数据库查询性能。

火山模型具有以下**优点**：

- 灵活性：火山模型将查询执行过程抽象为数据流图，可以对查询计划进行灵活的优化和重组，以适应不同的查询类型和数据分布。
- 可扩展性：火山模型可以很容易地添加新的操作符，支持更多的查询优化技术和执行策略。
- 易于实现：火山模型的执行过程类似于一个数据流管道，易于实现并在现代计算机上进行高效执行。

火山模型的**缺点**主要包括：

- 重复计算：在火山模型中，每个算子都会重新计算并传递数据，可能导致重复计算的开销。
- 内存占用：火山模型的执行过程需要维护中间结果，可能导致较大的内存占用。
- 不适合大规模并行：火山模型在大规模并行处理（例如分布式数据库）上可能存在一些性能问题。

从**优化角度**，可以考虑以下几个方面：

- 重复计算优化：通过引入缓存、提前计算等技术，减少重复计算的开销。
- 内存管理：优化内存使用，避免内存溢出和频繁的内存分配。
- 并行执行：通过并行化执行，利用多核处理器和分布式计算资源，提高查询性能。

### 3 解释一下左连接和内连接？

左连接（Left Join）和内连接（Inner Join）都是SQL中用于连接两个表的操作。

- 内连接：内连接返回两个表中满足连接条件的交集部分。即只返回两个表中在连接条件上有匹配的行。语法为：SELECT * FROM 表1 INNER JOIN 表2 ON 连接条件。
- 左连接：左连接返回左表中所有的行，以及左表和右表中满足连接条件的交集部分。如果右表没有匹配的行，用NULL填充。语法为：SELECT * FROM 表1 LEFT JOIN 表2 ON 连接条件。

### 4 怎么理解的数据库回表？

数据库回表指的是在查询过程中，数据库引擎需要通过索引查找到对应的记录后，再回到数据表中根据主键或行ID获取完整的行数据。这种回到数据表中获取完整行数据的过程称为回表。

数据库回表通常发生在以下情况：

- 当数据库查询使用非聚集索引时，索引存储的是索引键和对应的主键或行ID，当查询需要获取完整的行数据时，需要回到数据表中根据主键或行ID进行查找。
- 当查询结果需要返回的列不在索引中时，也需要进行回表操作，以获取其他列的数据。

数据库回表会增加查询的开销，因为需要多次IO访问数据库。为了优化回表开销，可以考虑以下方法：

- 覆盖索引：尽量使用覆盖索引，将查询需要的列都包含在索引中，这样可以避免回表操作。
- 聚集索引：对于InnoDB等支持聚集索引的数据库引擎，将主键定义为聚集索引，可以避免回表操作。
- 合理使用索引：合理设计和使用索引，避免不必要的回表操作。





# 面试lab4================

### 解决死锁的wound-wait算法的原理，死锁的条件，问哪些情况会死锁

#### 有向图环检测算法（核心）

包括 DFS 和拓扑排序。在这里我们选用 DFS 来进行环检测。构建 wait for 图时要保证搜索的确定性。始终从 tid 较小的节点开始搜索，在选择邻居时，也要优先搜索 tid 较小的邻居。

#### 构建 wait for 图的过程（算法原理）

遍历 `table_lock_map` 和 `row_lock_map` 中所有的请求队列，对于每一个请求队列，用一个二重循环将所有满足等待关系的一对 tid 加入 wait for 图的边集。满足等待关系是指，对于两个事务 a 和 b，a 是 waiting 请求，b 是 granted 请求，则生成 `a->b` 一条边。

在成功构建 wait for 图后，对 wait for 图实施环检测算法。注意，这个环检测算法不仅需要输出是否存在环，假如存在环，还要输出环上的所有节点。因为之后我们需要在这些成环的节点里挑选合适的事务进行终止。

#### 发现环（死锁）、解决死锁

在发现环后，我们可以得到环上的所有节点。此时我们挑选 **youngest** 的事务将其终止。只用挑选 tid 最大的事务作为 youngest 事务终止即可。

挑选出 youngest 事务后，将此事务的状态设为 Aborted。并且在请求队列中移除此事务，释放其持有的锁，终止其正在阻塞的请求，并调用 `cv_.notify_all()` 通知正在阻塞的相关事务。此外，还需移除 wait for 图中与此事务有关的边。

不是不用维护 wait for 图，每次使用重新构建吗？这是因为图中可能存在多个环，不是打破一个环就可以直接返回了。需要在死锁检测线程醒来的时候打破当前存在的所有环。

之前的阻塞模型需要进行一定的修改：

```cpp
std::unique_lock<std::mutex> lock(queue->latch_);
while (!GrantLock(...)) {
    queue->cv_.wait(lock);
    //在事务被唤醒时，其可能已经被终止掉了。原因可能是死锁检测中将其终止，也可能是外部的一些原因造成终止。因此需要检测是否处于 Aborted 状态，若处于则释放所持资源并返回。
    if (txn->GetState() == Aborted) {
        // release resources
        return false;
    }
}
```



### **locker-manager管理的什么锁，怎么管理的，锁管理器基本结构，还有检测环和abort的逻辑**

锁管理器，利用 2PL 实现并发控制。支持 `REPEATABLE_READ`、`READ_COMMITTED` 和 `READ_UNCOMMITTED` 三种隔离级别，支持 `SHARED`、`EXCLUSIVE`、`INTENTION_SHARED`、`INTENTION_EXCLUSIVE` 和 `SHARED_INTENTION_EXCLUSIVE` 五种锁，支持 table 和 row 两种锁粒度，支持锁升级。

####  基本结构：

- `table_lock_map_`：记录 table 和与其相关锁请求的映射关系。
- `row_lock_map_`：记录 row 和与其相关锁请求的映射关系。

这两个 map 的值均为锁请求队列 `LockRequestQueue`：

- `request_queue_`：实际存放锁请求的队列。
- `cv_` & `latch_`：条件变量和锁，配合使用可以实现经典的等待资源的模型。
- `upgrading_`：正在此资源上尝试锁升级的事务 id。

锁请求以 `LockRequest` 类表示：

- `txn_id_`：发起此请求的事务 id。
- `lock_mode_`：请求锁的类型。
- `oid_`：在 table 粒度锁请求中，代表 table id。在 row 粒度锁请求中，表示 row 属于的 table 的 id。
- `rid_`：仅在 row 粒度锁请求中有效。指 row 对应的 rid。
- `granted_`：是否已经对此请求授予锁？

#### 事务中止abort逻辑：

锁管理器需要**检查事务的隔离级别**，并在锁定/解锁请求上公开（执行）正确的行为。任何失败的锁操作都应导致ABORTED事务状态（隐式中止）并抛出异常。事务管理器将进一步捕获此异常并回滚事务执行的写操作。



# =====================================

## 其他问题：

### 怎么理解高性能，QPS是多少

高性能是系统或软件在单位时间内能够处理的工作量较大，响应速度较快的特性。在计算机领域，高性能可以指计算速度快、并发处理能力强、资源利用率高等方面的优势。

QPS（Queries Per Second）是衡量系统性能的一种常用指标，表示每秒钟能够处理的查询或请求次数。QPS越高，表示系统能够在单位时间内处理更多的请求，也意味着系统具有更高的并发处理能力。

对于不同类型的系统，高性能和合理的QPS标准都是相对的，因为要根据具体的业务需求、系统架构和硬件条件来评估。一些常见的例子包括：

1. Web服务器：对于一个简单的静态网页服务器，QPS可能会很高，例如几千到几万。但对于复杂的动态网页服务器，QPS可能会下降，可能在几百到几千的范围内。
2. 数据库系统：数据库通常处理复杂的查询和事务，QPS通常较低，可能在几十到几百的范围内。
3. 缓存系统：缓存系统的QPS可能非常高，可以达到几十万到数百万。
4. 分布式系统：在分布式系统中，通常需要考虑多个节点的协作，QPS的评估会涉及到整个系统的吞吐量。

### 

### GDB 怎么看值，追问格式x是啥

可以使用 `p` 命令来查看变量的值，使用 `x` 命令来查看内存中指定地址的内容。

1. 使用 `p` 命令查看变量的值：
   - 语法：`p <variable>`
   - 示例：`p my_variable`
   - 这个命令会打印出变量 `my_variable` 的当前值。
2. 使用 `x` 命令查看内存中的内容：
   - 语法：`x/<格式> <address>`
   - 示例：`x/4xw 0x12345678`
   - `x` 命令后面可以跟一个格式和一个地址。格式指定了要如何解释内存中的内容，地址则指定了要查看的内存位置。
   - `/4xw` 中的 `4` 表示查看4个数据，`x` 表示16进制显示，`w` 表示以4字节为单位解释内存内容。你可以根据具体需求使用不同的格式，如 `b` (byte), `h` (halfword, 2字节), `g` (giant, 8字节) 等。

#### cmu15445 学了什么

卡内基梅隆大学的数据库lab。

la1: 建立一个面向磁盘的存储管理器，因为操作系统缓存的透明性，我们要在应用层面实现一个**缓存池**，使用 **Extented_Hash_Table** 做页面映射，**LRU-K** 做缓存驱逐策略。文件 IO 只需要调用相应的接口。

lab2:  为了支持快速数据检索，实现**B+ 树**动态索引结构，支持**索引并发**。lab2 自由度相当高(只有几个函数接口），容易一上来手足无措。官方提供了可视化工具，将B+树可视化帮助debug，看着自己的 B+树运行起来还是相当有成就感的。

lab3: 实现基于**火山模型** 的sql执行器，完成 agg、nested loop join、hash join、insert、index join等算子的逻辑。选做部分设计一些**执行计划优化规则，**自由度也很高，可以自定义任意的优化比如列裁剪、谓词下、常量折叠等等。前两个lab是一个点，从lab3开始，点就连成了面，我们要在自己写的缓存池和B+树索引上真正的去跑一些算子，这个lab一个难点在于要大量阅读相关源代码。完成lab3后，就可以当作一个单机数据库跑一些sql了，包括 select，where，inner/left join，groub by，order by，limit等。

lab4: **并发事务管理**，保证多个事务在数据库并发执行时的隔离性。实现基于**2PL**的事务并发控制，支持Repeatable Read、Read Commited、Read UnCommited三种隔离度。因为 2PL 不可避免产生死锁，还要实现一个 **后台死锁检测、解除线程**。



# Lab4 事务管理========

### 1 2PL原理是什么？2PL问题？如何解决？

在2PL协议下，每个transaction都会经过两个阶段：在第一个阶段里，transaction根据需要不断地获取锁，叫做 ***growing phase (expanding phase)\***；在第二个阶段里，transaction开始释放其持有的锁，根据2PL的规则，这个transaction不能再获得新的锁，所以它所持有的锁逐渐减少，叫做 ***shrinking phase (contracting phase)\***。

2PL（Two-Phase Locking）是一种并发控制协议，用于保证数据库事务的隔离性和一致性。2PL原理可以简述为：

- 事务执行过程中，分为两个阶段：加锁阶段和解锁阶段。

- 加锁阶段：在事务执行期间，所有的数据操作（读取和修改）都需要先获取对应的锁。只能加锁。（不同隔离级别，加锁不一样）

  **可重复读：所有锁都允许、读提交：所有锁都允许、读未提交：允许X、IX锁**

- 解锁阶段：在事务提交之前，释放所有的锁，让其他事务能够继续对数据进行操作。只能解锁。

  **可重复读：所有锁都不允许用；读提交：只允许用IS、S；读未提交：永不允许S、IS、SIX锁**

**2PL存在的问题：**

- 死锁：在并发环境下，不恰当的加锁顺序可能导致死锁，即多个事务互相等待对方持有的锁，从而无法继续执行。
- 阻塞：长时间持有锁可能导致其他事务的阻塞，影响数据库的性能和并发度。
- 低并发：在某些情况下，2PL可能导致并发度较低，从而影响系统的吞吐量和响应时间。

**如何解决这些问题：**

- **死锁检测和处理**：引入死锁检测机制，当发现死锁时，选择一个事务进行回滚，打破死锁循环。
- **锁粒度优化**：优化锁的粒度，尽量减小锁的范围，减少锁的竞争和阻塞。
- 2PL优化：可以采用更灵活的并发控制协议，如**多版本并发控制（MVCC）**、乐观并发控制等。



### 2 如何基于2PL实现的隔离级别？

基于2PL实现的隔离级别主要包括四种隔离级别，从低到高依次是读未提交（Read Uncommitted）、读提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。

- 加锁阶段：

  **可重复读：所有锁都允许、读提交：所有锁都允许、读未提交：允许X、IX锁**

- 解锁阶段：

  **可重复读：所有锁都不允许用；读提交：只允许用IS、S；读未提交：永不允许S、IS、SIX锁**

- **读未提交：**事务可以读取其他事务尚未提交的数据，可能会导致**脏读和不可重复读**。

  ​                 在读取数据时不获取任何锁，允许读取其他事务尚未提交的数据。

  ​                 在更新数据时获取写锁，防止其他事务同时读取和修改数据。

- **读提交：**事务只能读取其他事务已经提交的数据，**避免了脏读**，但仍可能存在不可重复读和幻读问题。

  ​               在读取数据时获取短暂共享锁（S锁），等到读取完成后立即释放锁，允许其他事务读取数据。

  ​               在更新数据时获取短暂排他锁（X锁），在更新完成后立即释放锁，允许其他事务读取数据。

- **可重复读：**事务在整个过程中都只能读取已经提交的数据，**可以避免脏读和不可重复读**，但仍可能出现幻读问题。

  ​              在读取数据时获取共享锁（S锁），并在事务结束前一直保持锁，阻止其他事务更新数据。

  ​              在更新数据时获取排他锁（X锁），并在事务结束前一直保持锁，阻止其他事务读取和更新数据。

- **串行化：**事务串行执行，可以避免脏读、不可重复读和幻读问题，但并发性能较差。

  ​             在读取数据时获取共享锁（S锁），并在事务结束前一直保持锁，阻止其他事务更新数据。

  ​             在更新数据时获取排他锁（X锁），并在事务结束前一直保持锁，阻止其他事务读取和更新数据。
  
  

### 3 什么是严格两阶段锁（strict-2PL）？

严格两阶段锁（strict-2PL）是2PL的一种变种，它要求事务在整个事务执行过程中，只能在开始阶段获取所有需要的锁，并在整个事务执行过程中都不释放任何锁，直到事务提交。只有在事务提交时，才一次性释放所有锁。**要求事务的排它锁必须在事务提交之后释放**

严格两阶段锁避免了2PL中锁释放过程可能导致的死锁问题，但同时可能会增加锁的持有时间，从而影响并发性能。



严格二阶段锁解决了[级联](https://so.csdn.net/so/search?q=级联&spm=1001.2101.3001.7020)回滚的问题；避免了脏读和丢失修改的问题。在提交时才释放排他锁X。级联回滚一般是由脏读引起的，所以解决脏读问题也就解决了级联回滚问题。

脏读是由于读了由其他事务更新得到的数据，而这个数据的更新还没有提交。

严格两阶段锁要求排它锁在事务提交后释放锁。而对于数据的更新要申请排它锁，对数据更新后提交事务再释放排它锁，这时其他事务才能申请锁来进行数据读取，而此时读取的数据就不再是脏数据。

解决丢失修改问题也是同理，在一个事务更新数据并提交后，其他事务才能申请锁进行更新提交，即前一个更新虽然被覆盖，但是并不是丢失。



### 4 什么是强两阶段锁（rigorous-2PL）?

强两阶段锁（rigorous-2PL）是2PL的另一种变种，它在2PL的基础上加强了对锁释放的限制。在强两阶段锁中，事务在释放锁之前，必须等待所有的事务操作完成，并且其他事务对于该事务持有的锁只能进行等待，不能申请新的锁。这样可以避免死锁，但可能增加了事务的等待时间。

shrinking阶段，只能在事务结束后再释放锁，完全杜绝了事务未提交的数据被读到。



强二阶段锁用于可串行化，在提交时才释放所有锁。**要求所有锁都必须在事务提交之后释放**。解决数据项不能重复读的问题。



### 5 隔离级别有哪些？分别存在什么问题？

高隔离级别会增加并发性能问题，而低隔离级别可能会导致数据一致性问题。

SQL标准中定义了四种隔离级别，从低到高依次是读未提交（Read Uncommitted）、读提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。

- 读未提交：事务可以读取其他事务尚未提交的数据，可能会导致脏读和不可重复读。
- 读提交：事务只能读取其他事务已经提交的数据，避免了脏读，但仍可能存在不可重复读和幻读问题。
- 可重复读：事务在整个过程中都只能读取已经提交的数据，可以避免脏读和不可重复读，但仍可能出现幻读问题。
- 串行化：事务串行执行，可以避免脏读、不可重复读和幻读问题，但并发性能较差。



### 6 什么情况下会导致幻读，举个例子？

幻读是数据库中并发事务执行时的一种现象，它通常发生在以下情况下：

1. 并发事务中有插入或删除操作：当一个事务在查询某个范围内的数据时，同时有其他事务在该范围内插入或删除数据，导致查询结果发生了变化，从而产生幻读。
2. 不可重复读：在某个事务中，一个范围内的数据被查询两次，而在这两次查询之间，有其他事务对该范围内的数据进行了修改，导致第二次查询结果与第一次查询结果不一致，产生幻读。
3. 采用Read Committed隔离级别：在Read Committed隔离级别下，事务只能读取其他事务已提交的数据，但是在事务执行期间，其他事务可以插入新的数据，导致事务读取的范围内数据数目发生变化，产生幻读。

举个例子：假设有一个学生成绩表，其中包含学生的姓名和成绩信息。在一个事务中，事务A查询所有成绩大于80分的学生记录，并获取记录数目。然后，事务B在此时向成绩表中插入了一条新的学生记录，使得成绩大于80分的学生数目增加了。接着，事务A再次查询所有成绩大于80分的学生记录，并发现记录数目发生了变化，导致了幻读。

在这个例子中，事务A在两次查询之间发生了幻读，即在同一个事务中，查询的结果范围发生了变化，这是因为事务A在查询之间并没有加锁或使用MVCC机制，导致了事务B的插入操作在事务A的查询中不可见。为了避免幻读问题，可以采用更高的隔离级别，如可重复读（Repeatable Read）或串行化（Serializable），或者使用MVCC等并发控制机制，保证事务间的隔离性，从而避免幻读的发生。

### 7 快照隔离存在什么问题？什么是写偏序问题，举个例子？

- 快照隔离（Snapshot Isolation）是一种数据库事务隔离级别，它允许事务在整个事务期间读取一个一致性的数据快照，而不受其他并发事务的影响。尽管快照隔离在一定程度上解决了脏读和不可重复读的问题，但仍然存在一些问题，其中之一是写偏序问题（Write Skew Problem）。

  写偏序问题是指在快照隔离下，当多个事务并发地修改共享数据时，可能出现依赖于其他事务修改结果的情况。尽管每个事务在读取数据时看到的是一致的数据快照，但在提交时可能发生数据冲突，导致不一致的结果。

  举个例子来说明写偏序问题：

  假设有一个银行账户表，其中包含账户余额信息。现有两个事务A和事务B，事务A执行如下操作：

  1. 读取账户A的余额为1000。
  2. 读取账户B的余额为2000。
  3. 计算并将账户A的余额减少100元。
  4. 提交事务。

  与此同时，事务B执行如下操作：

  1. 读取账户A的余额为1000。
  2. 读取账户B的余额为2000。
  3. 计算并将账户B的余额减少200元。
  4. 提交事务。

  在快照隔离下，事务A和事务B并发地执行读取操作，它们都读取到了账户A和账户B的一致性数据快照。然后，两个事务分别对账户A和账户B进行了修改，并在事务提交时，数据库检测到了冲突。

  假设在事务A提交前，账户A的余额是900，账户B的余额是1800。在事务B提交前，账户A的余额是800，账户B的余额是1800。这样，在事务A和事务B都提交后，账户A的余额变成了800，账户B的余额变成了1800。

  可以看到，事务A和事务B的操作导致了一个不一致的结果，这就是写偏序问题。写偏序问题是快照隔离的一个缺点，它可能导致数据不一致，因此在选择隔离级别时需要根据应用场景进行权衡和选择。如果应用对数据一致性要求较高，可以考虑使用更高级别的隔离级别，如可重复读或串行化。

举个例子：假设有两个事务A和事务B同时对同一个数据进行修改。事务A先开始并提交了更新，然后事务B开始并提交了更新。在快照隔离中，事务B的更新可能会覆盖事务A的更新，从而导致数据的不一致性。

### 8 数据库是怎么实现一致性的？

数据库实现一致性通常依赖于**事务管理和并发控制**机制。一致性是数据库的重要特性之一，它确保事务在执行后，数据库的状态从一个一致的状态转换到另一个一致的状态。

以下是数据库实现一致性的主要方法和机制：

1. 事务管理：数据库使用事务管理来处理一组相关操作，这些操作被视为一个逻辑单元，要么全部执行成功，要么全部失败。事务应该满足ACID（原子性、一致性、隔离性和持久性）属性，其中一致性是指事务执行后，数据库应该从一个合法状态转换到另一个合法状态。
2. 并发控制：数据库需要处理并发事务的同时执行，以确保事务之间的隔离性和数据的一致性。并发控制机制包括锁机制和多版本并发控制（MVCC）等。锁机制通过对数据进行加锁，限制事务对数据的访问，从而保证事务间的隔离性。MVCC是一种基于多版本的并发控制方法，为每个事务创建一个独立的数据版本，从而避免了锁的冲突和阻塞，提高了并发性能。
3. 日志系统：数据库使用事务日志记录事务执行的所有操作，包括修改数据的操作。事务日志在事务提交前持久化到磁盘，以确保在数据库发生故障时，可以通过回滚未提交的事务或者重放已提交的事务来恢复数据库到一致状态。
4. 一致性检查：数据库在执行事务时，会进行一致性检查，以确保事务的执行不会破坏数据的完整性和约束。如果发现事务执行后可能导致数据不一致，数据库会自动回滚事务，保持数据库的一致性。

### 9 项目中所涉及的几种锁介绍一下，怎么理解隔离级别和锁的关系？

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991488422-4349f906-b835-423c-b9ea-9f9345d02cfb.png)

- 加锁阶段：

  **可重复读：所有锁都允许、读提交：所有锁都允许、读未提交：允许X、IX锁**

- 解锁阶段：

  **可重复读：所有锁都不允许用；读提交：只允许用IS、S；读未提交：永不允许S、IS、SIX锁**



在项目中，常见的几种锁包括共享锁（Shared Lock，也称为S锁）、排他锁（Exclusive Lock，也称为X锁）、更新锁（Update Lock，也称为U锁）等。它们在并发控制中起到不同的作用，用于保证事务的隔离性和数据的一致性。

1. 共享锁（Shared Lock，S锁）： 共享锁允许多个事务同时持有锁，并且可以共享读取数据，但不允许事务之间修改数据。多个事务可以同时持有共享锁，因为共享锁不会相互阻塞。
2. 排他锁（Exclusive Lock，X锁）： 排他锁是一种独占锁，一个事务持有排他锁时，其他事务无法同时持有任何锁。排他锁用于防止其他事务读取或修改数据，只有一个事务持有排他锁时，其他事务不能对该数据进行任何操作。
3. 更新锁（Update Lock，U锁）： 更新锁是共享锁和排他锁的结合，它允许事务在读取数据时共享锁，但在更新数据时升级为排他锁。更新锁用于在事务期间防止其他事务获取排他锁，从而保证在更新操作期间数据的一致性。

隔离级别和锁的关系： 隔离级别是数据库中控制并发事务隔离程度的设置，不同的隔离级别对应不同的锁策略和并发控制机制。隔离级别定义了事务间的隔离程度，如读未提交（Read Uncommitted）、读提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）等级别。

不同的隔离级别对应不同的锁机制来实现隔离性。例如，在串行化隔离级别下，事务之间是完全隔离的，数据库会为每个事务创建快照，并使用共享锁和排他锁来实现读一致性和写一致性。而在读提交隔离级别下，事务之间可以读取已提交的数据，但在更新数据时需要获取排他锁来保证写一致性。

### 10 为什么MySQL使用B+树而不使用B树？

MySQL使用B+树而不使用B树作为索引结构，主要有以下几个原因：

- 存储效率：B+树中的非叶子节点只存储索引键，而叶子节点存储索引键和对应的数据。相比之下，B树中的每个节点既存储索引键又存储数据，导致节点的存储空间利用率较低，而B+树在相同的节点大小下可以存储更多的索引项。
- 查找效率：B+树的高度更低，因为所有数据都存储在叶子节点中，并且叶子节点之间形成链表，支持范围查询。这使得B+树在查找效率方面比B树更优秀。
- 顺序访问性能：B+树的叶子节点之间有指针连接，支持顺序访问，对于范围查询和范围扫描效率较高。
- 内存访问：B+树的非叶子节点仅存储索引信息，导致非叶子节点更小，可以容纳更多节点在内存中，提高了缓存命中率。

综上所述，B+树相比B树在存储效率、查找效率和顺序访问性能方面都有优势，因此在数据库中，特别是在索引结构中，通常使用B+树作为首选结构。

### 11 死锁问题怎么解决？

#### 发现环（死锁）、解决死锁

环检测算法发现环后，可以得到环上的所有节点，挑选 tid 最大的事务作为 youngest 事务终止。

挑选出 youngest 事务后，将此事务的状态设为 Aborted。并且在请求队列中移除此事务，释放其持有的锁，终止其正在阻塞的请求，并调用 `cv_.notify_all()` 通知正在阻塞的相关事务。此外，还需移除 wait for 图中与此事务有关的边。

图中可能存在多个环，不是打破一个环就可以直接返回了。需要在死锁检测线程醒来的时候打破当前存在的所有环。



#### 死锁检测

死锁检测，运行在一个 background 后台线程，每间隔一定时间检测当前是否出现死锁，并挑选合适的事务将其 abort 以解开死锁。

2PL不可避免的会产生死锁，所以要及时检测死锁打破依赖。这一节比较简单，bustub 会在创建 lock_manger 时，**在后台创建一个周期性的死锁检测线程**。

每次死锁检测线程唤醒后都要检测死锁，流程如下:

- 遍历所有request_queue，建立一个 wait-for graph有向边 。
- 建图完成后，用DFS算法检测环。
- 如果有环，在此lab中只需要简单的找出环中最年轻的事务，也就是 txn_id 最大的事务。

- 将事务设为abort状态。
  - 找到事务正在等待资源的request_queue。
  - notify_all 将 request_queue上所有线程唤醒。

- 如果事务唤醒后发现状态为abort (这里需要修改一下 上文的等待条件) ，在request_queue中将自己删除，抛异常，返回。

- 在 graph 中删除 所有与 txn_id 相关的边，可能会有多条边，因为会同时在等待多个事务。

- 重复直到没有环。

需要注意的是：事务在被 abort 后，lock_manger 会自动释放相关资源，所以我们不需要添加冗余的逻辑去释放已有的锁。

死锁是指在并发环境中，多个事务相互等待对方所持有的资源，导致所有事务无法继续执行的情况。解决死锁问题的常用方法有：

- 超时机制：设置事务的超时时间，在事务等待一定时间后自动回滚，释放资源，从而避免长时间的死锁阻塞。
- 死锁检测与回滚：周期性地检测系统中是否有死锁发生，如果检测到死锁，则选择其中一个事务进行回滚，从而打破死锁循环。
- 锁顺序：在应用设计中，尽量保持所有事务对资源的访问顺序一致，从而避免产生环形等待条件。
- 锁粒度：适当调整锁的粒度，降低锁竞争的概率，减少死锁的发生。
- 分布式事务：对于分布式环境，可以使用分布式事务框架来管理事务，确保全局一致性和避免死锁问题。

### 12 MySQL怎么保证原子性的？

MySQL保证原子性主要依赖于事务机制的支持。MySQL支持事务，它将一组相关的数据库操作作为一个逻辑单元，要么全部执行成功，要么全部失败。MySQL遵循ACID（原子性、一致性、隔离性和持久性）属性，其中原子性是指事务的所有操作要么全部执行成功，要么全部回滚，不会出现部分执行的情况。

在MySQL中，使用BEGIN、COMMIT和ROLLBACK语句来管理事务。当一个事务开始时，可以使用BEGIN语句来标识事务的开始。然后，在事务执行期间，所有相关的数据库操作会被当作一个原子性操作。最后，当事务结束时，可以使用COMMIT提交事务，将事务中的操作永久保存到数据库中。如果在事务执行过程中发生错误或者需要撤销事务，可以使用ROLLBACK语句回滚事务，将事务中的操作撤销，回到事务开始前的状态。

通过事务的使用，MySQL保证了在事务内的所有操作要么全部成功，要么全部失败，从而实现了原子性。这样可以确保数据库在并发环境中的数据一致性和完整性，同时提供了可靠的事务处理机制。



### 13 MySQL的默认隔离级别是什么？可重复读会有什么问题？既然可重复读有幻读问题那MySQL满足了隔离性吗？如果满足了幻读问题如何解决的？如果没满足如何保证的数据一致性？

可重复读隔离级别下，事务可以读取其他事务已提交的数据，但在事务执行期间，其他事务不能对该数据进行修改。这样可以确保事务在执行期间读取的数据始终保持一致，即使其他事务对数据进行了修改也不会影响当前事务的读取结果。

可重复读隔离级别可以解决脏读和不可重复读的问题，**但仍然存在幻读问题**。幻读是指在一个事务中，某个范围内的记录数目发生了变化，但是查询结果却发现记录数目没有发生变化，产生了幻觉般的读取结果。

**满足隔离性：**尽管可重复读隔离级别下存在幻读问题，但MySQL仍然满足了隔离性。隔离性是数据库的一个特性，它确保每个事务在执行期间与其他事务隔离，互不干扰。在可重复读隔离级别下，事务执行期间读取的数据是一致的，并且不会受到其他事务的修改影响，从而保证了隔离性。

**解决幻读：**

可重复读隔离级别下，MySQL可以通过使用行级锁（如行锁、间隙锁和临键锁）来解决幻读问题。

1. 行锁（Row Lock）：行锁是在事务对某一行数据进行修改时加上的锁。当一个事务对某一行数据进行修改时，其他事务不能同时对该行加行锁，从而保证了数据的一致性。行锁可以防止多个事务同时修改同一行数据，避免了幻读问题。
2. 间隙锁（Gap Lock）：间隙锁是在事务执行范围内对数据行的范围进行锁定，但不锁定实际的数据行。间隙锁用于防止其他事务在事务执行期间插入新的数据，从而导致幻读问题。
3. 临键锁（Next-Key Lock）：临键锁是行锁和间隙锁的组合，它不仅锁定了数据行，还锁定了数据行之间的间隙。临键锁用于防止幻读和不可重复读问题的同时发生。

通过使用这些行级锁，MySQL在可重复读隔离级别下可以避免幻读问题的发生。当一个事务执行范围内的查询时，MySQL会自动为查询的数据行和相关的间隙加上合适的锁，防止其他事务在该范围内插入或删除数据，保证了数据的一致性。


如果数据库的隔离级别没有完全满足隔离性，即可能出现脏读、不可重复读或幻读等问题，仍然可以通过其他方式来保证数据的一致性。

**保证数据一致性**的方法：

1. 业务层面的并发控制：在应用程序的业务逻辑中引入并发控制机制，确保多个事务之间的操作不会产生冲突。例如，可以使用乐观锁或悲观锁来对数据进行控制，确保数据在并发环境下的一致性。
2. 应用级事务处理：在应用程序中使用手动控制事务，而不是依赖数据库的隔离级别。通过在代码中明确指定事务的边界，确保事务的一致性和正确性。
3. 行级锁：即使隔离级别不是最高级别的串行化，也可以使用行级锁来实现更细粒度的并发控制。通过对数据的行进行锁定，确保并发事务之间的数据操作不会产生冲突。
4. 定时任务或后台处理：在某些情况下，可以通过定时任务或后台处理来处理冲突或不一致的数据。例如，定期清理脏数据，或者在低峰期执行数据一致性检查和修复操作。
5. 使用更高的隔离级别：如果数据的一致性要求非常高，并且可以容忍一定的性能损失，可以将隔离级别提升到更高级别，如串行化隔离级别，从而避免一些并发问题。

### 14 MVCC在项目中什么实现的，RR、RC在项目又是什么实现的

MVCC（Multi-Version Concurrency Control）是一种并发控制机制，常用于数据库管理系统中，可以实现在一定程度上的并发事务执行，提高数据库的并发性能。

在项目中，MVCC通常通过版本控制来实现。当一个事务开始时，数据库会为其创建一个事务开始的时间戳（Transaction Start Timestamp），用于标识该事务。在MVCC中，每个数据项都可能存在多个版本，每个版本都有一个时间戳来标识其创建时间。当一个事务对某个数据项进行读取操作时，数据库会根据事务的时间戳选择对应的数据版本，保证读取的是事务开始之前已提交的数据。当一个事务对某个数据项进行写操作时，数据库会创建一个新的数据版本，并将事务的时间戳作为版本的创建时间戳，从而使得该版本对于该事务是可见的，但对于其他事务是不可见的，从而实现了数据的多版本并发控制。

在项目中，通常会根据业务需求选择不同的隔离级别，如读未提交（Read Uncommitted）、读提交（Read Committed）和可重复读（Repeatable Read）。每种隔离级别对应的并发控制机制可能不同，可以采用MVCC、锁机制或其他并发控制策略来实现。

RR（Repeatable Read）和RC（Read Committed）是两种常见的隔离级别，它们在项目中可能采用MVCC来实现：

- RR（Repeatable Read）：在RR隔离级别下，数据库需要为每个事务创建一个快照，并在事务执行期间保持该快照的一致性。在事务执行期间，其他事务的更新操作不会影响该事务读取的数据。MVCC是一种常见的实现方式，通过为每个事务创建快照来保证读取的数据一致性。
- RC（Read Committed）：在RC隔离级别下，数据库只保证每次读取时数据的一致性，而不保证事务内多次读取之间数据的一致性。MVCC也是一种常见的实现方式，通过为每个事务创建快照来保证每次读取的数据一致性。

在实际项目中，根据具体的业务需求和性能要求，可以选择合适的隔离级别和并发控制机制。MVCC作为一种高效的并发控制策略，可以在一定程度上提高数据库的并发性能。

### 补充：

### 1 数据库中删除大量带索引的数据如何更快执行？

先删除索引，删完数据后再重建索引。

### 2 什么情况下索引会失效？

字符串不加单引号，索引失效。

not in索引失效。

以%开头的like模糊查询，索引失效。

范围查询右边的列，索引失效。

索引列上进行运算操作，如子串匹配，索引失效。

### 3 什么情况下要建索引？

频繁查询的字段

需要统计排序的字段

### 4 什么情况下不要建索引？

重复且平均的字段

频繁更新的字段

表记录太少

where条件里用不到的字段

### 5 索引的优缺点

优点：提高查询的效率，降低数据库的IO成本。

缺点：索引是一张表需要占用空间，更新表需要成本。

### 6.意向锁的好处在于：

当表加了IX，意味着表中有行正在修改。

（1）这时对表发起DDL操作，需要请求表的X锁，那么看到表持有IX就直接等待了，而不用逐个检查表内的行是否持有行锁，有效减少了检查开销。

（2）这时有别的读写事务过来，由于表加的是IX而非X，并不会阻止对行的读写请求（先在表上加IX，再去记录上加S/X），事务如果没有涉及已经加了X锁的行，则可以正常执行，增大了系统的并发度。





# Lab 4 锁管理器知乎======

**事务**是数据库执行的最小单元，并发控制为事务并发提供可串行化的能力和隔离性。

**可串行化：**多个事务并发执行时，能找到一个顺序，使多个事务好像在串行执行。

隔离性：不同事务对外界的暴露程度。

## Lock Manager：锁管理器

 利用 2PL 实现并发控制。支持 `REPEATABLE_READ`、`READ_COMMITTED` 和 `READ_UNCOMMITTED` 三种隔离级别，支持 `SHARED`、`EXCLUSIVE`、`INTENTION_SHARED`、`INTENTION_EXCLUSIVE` 和 `SHARED_INTENTION_EXCLUSIVE` 五种锁，支持 table 和 row 两种锁粒度，支持锁升级。

Deadlock Detection：死锁检测，运行在一个 background 线程，每间隔一定时间检测当前是否出现死锁，并挑选合适的事务将其 abort 以解开死锁。

Concurrent Query Execution：修改之前实现的 `SeqScan`、`Insert` 和 `Delete` 算子，加上适当的锁以实现并发的查询。

**基于 2PL 的并发控制方式，自动为并发事务执行加锁解锁，提供可串行化能力并实现可重复读、读已提交、读未提交三种隔离度**

==========

### Lock Manager锁管理器

#### 基本概念

锁管理器的基本思想是，它维护一个**活动事务当前持有的锁**的内部数据结构。然后，事务在访问数据项之前**向锁管理器发出锁请求**。锁管理器将授予调用事务的锁，阻塞该事务，或者中止该事务

lock manager 为**每个资源（表/行）维护一个请求队列，这个队列根据请求的顺序排序。**队列记录了每一个请求的事务、锁级别、是否授予等。lock manager 用**哈希表建立从资源到队列的索引**，我们称这个哈希表为lock table，在p4中分别为 table_lock_map 和 tuple_lock_map。

####  基本结构：

- `table_lock_map_`：记录 table 和与其相关锁请求的映射关系。
- `row_lock_map_`：记录 row 和与其相关锁请求的映射关系。

这两个 map 的值均为锁请求队列 `LockRequestQueue`：

- `request_queue_`：实际存放锁请求的队列。
- `cv_` & `latch_`：条件变量和锁，配合使用可以实现经典的等待资源的模型。
- `upgrading_`：正在此资源上尝试锁升级的事务 id。

锁请求以 `LockRequest` 类表示：

- `txn_id_`：发起此请求的事务 id。
- `lock_mode_`：请求锁的类型。
- `oid_`：在 table 粒度锁请求中，代表 table id。在 row 粒度锁请求中，表示 row 属于的 table 的 id。
- `rid_`：仅在 row 粒度锁请求中有效。指 row 对应的 rid。
- `granted_`：是否已经对此请求授予锁？

#### 作用：

Lock Manager 的**作用是处理事务发送的锁请求**，例如有一个 SeqScan 算子需要扫描某张表，其所在事务就需要对这张表加 S 锁。而加读锁这个动作需要由 Lock Manager 来完成。**事务先对向 Lock Manager 发起加 S 锁请求，Lock Manager 对请求进行处理。**如果发现此时没有其他的锁与这个请求冲突，则授予其 S 锁并返回。**如果存在冲突，例如其他事务持有这张表的 X 锁，则 Lock Manager 会阻塞此请求（即阻塞此事务）**，直到能够授予 S 锁，再授予并返回。

#### 事务中止abort逻辑：

锁管理器需要**检查事务的隔离级别**，并在锁定/解锁请求上公开（执行）正确的行为。任何失败的锁操作都应导致ABORTED事务状态（隐式中止）并抛出异常。事务管理器将进一步捕获此异常并回滚事务执行的写操作。



#### lock_table锁表

##### lock manager处理锁请求流程：

- 当一个新的加锁请求到达时，如果请求队列存在，他在对应资源的请求队列末尾添加一条记录，否则创建一个新的队列。
- 如果资源没有被锁住，那么授予锁。
  - 如果已经有事务获取了锁，检查锁的兼容性，如果兼容并且先前锁请求都被授予，才能获取锁，否则只能等待。**这里保证了锁的请求（前面的锁）不会饥饿。**
- 当解锁请求到达时，lock manger把对应的加锁记录从请求队列中移除。检查后续等待获取请求能否被授予锁。



#### LockTable锁表流程

事务记录了获取锁的集合，所以在授予锁之前，先要进行一些检查，因此，具体的流程如下，以LockTable锁表为例。

1. **根据隔离级别，判断锁的请求是否合理，如果不合理，把事务设为abort，抛出事务异常**。

(1):*REPEATABLE_READ* 可重复读shrinking 解锁阶段不允许加任何锁。

(2): *READ_COMMITTED* 读已提交shrinking 解锁阶段只允许加S、IS锁。

(3):*READ_UNCOMMITTED* 读未提交只允许X、 IX锁。

2. 检查锁是否要升级，以及升级是否合理，如果不合理，abort，throw。如果重复申请相同的锁，直接返回。
   尽管我们可以在 request_queue 中去检查，但是因为事务自己维护了**加锁资源的集合**，我们可以在进入临界区之前**自行检查**，减少 lock_manager 锁的争用。

3. **lock_table_map如果没有对应的请求队列**，那么添加一条请求队列，添加记录，授予锁。

4. 如果有相应的**请求队列（跟踪哪些事务正在等待锁）**。

5. 如果要进行锁升级，

6. 1. 1. 检查锁升级是否冲突，冲突，abort, throw
      2. 删除旧的记录，同时删除事务集合中的记录。
      3. 将一条新的记录插入在队列最前面一条未授予的记录之前

7. 1. 锁不用升级，将新的记录添加在请求队列末尾。
   2. 检查兼容性（前面事务），等待在条件变量上
   3. 通过了兼容性检查，授予锁

8. 更新事务的加锁集合。

```c++
// 等待在条件变量上
while(!checkCompability()){
    que->cv_.wait(lock)
}
// 另一种写法
que->cv_.wait(lock,[&](){
    return checkCompability();
})

```



##### 加锁顺序

在通过前两步检查后，

- 首先**锁住全局的 lock_table_map**，
- 如果能找到对应的请求队列，先锁住队列，然后解锁全局 lock_table_map，在进行下面的授予流程。顺序不能相反，否则事务的顺序会发生混乱。
- 如果不能找到，添加一条新队列，授予，然后解锁全局 lock_table_map
- 正在升级的锁请求应优先于同一资源上的其他等待锁定请求





#### UnLockTable解锁表流程

UnlockTable 请求解锁正好相反，

1. 检查是否持有要解锁的锁
2. 检查是否还**持有相应表的行锁**
3. 加锁全局 lock_table_map
4. 找到对应请求队列，加锁request_queue，解锁 lock_table_map

5. 1. 删除加锁记录
   2. 在**条件变量上 notify_all() 唤醒所有等待的线程（等待锁请求）**（通知的等待锁事务）
   3. 解锁 request queue

6. 判断事务是否需要进入SHRINKING状态。

​       维护事务的状态。事务的状态可能由于解锁操作从GROWING阶段转变为SHRINKING阶段

6. 更新事务锁集合

   跟踪事务所获取的锁，使用_lock_set_这些变量

##### 兼容性检查

**问题：**

notify_all()会唤醒所有等待在这个条件变量上的线程，而且顺序是不一定的。究竟应该怎么去检查唤醒的线程是否应该被授予锁，而不被唤醒顺序影响。

**正确：**

在进行兼容性检查时，**不能简单查看先前记录中的granted字段**判断是否被授予，而是应该**重新计算先前的记录是否应该被granted，检查先前所有未授予的锁是不是和它们前面的兼容**。不能受到notify唤醒顺序的影响。txn2 txn3不论唤醒先后顺序如何，应该都被授予锁。

```cpp
// 等待在条件变量上
while(!checkCompability()){
    que->cv_.wait(lock)
}
// 另一种写法
que->cv_.wait(lock,[&](){
    return checkCompability();
})
```

### 代码注释的提示

### 加锁提示

##### 支持锁模式（隔离级别RR、RC ，事务获取锁）

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686990983361-7733f471-e071-442a-a6b2-e206cdba41e2.png)![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991058909-6c0e43c2-58cd-4f84-8e8f-b3cd8702b3f4.png)![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991442565-a97b37ff-1bca-4d5c-80f4-fa789906a9e6.png)

##### 隔离级别与支持的锁、与两阶段锁、锁升级

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991488422-4349f906-b835-423c-b9ea-9f9345d02cfb.png)

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991601911-1410ac73-e02d-4534-87e5-dc2eb94fb444.png)

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991621761-8466be26-2a6b-4b06-9402-c0cfa9a3c39d.png)

##### 锁兼容矩阵：

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686991708901-eaa051ae-d031-4750-92af-857ec9604189.png)

### 解锁提示

##### 可重复读，读已提交，读未提交隔离级别解锁的事务状态更新

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686992429191-4a373688-e1ad-4023-b57a-290083ae2c80.png)![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686992444608-e7fda0e7-eac5-468e-a2a0-a80cd9864416.png)

##### 行锁和表锁区别

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686993133591-2459de63-2418-46f6-ad5d-ee2b67b42f18.png)

##### 锁表

![img](https://cdn.nlark.com/yuque/0/2023/png/29513910/1686993162507-4f7ff4b3-206a-4f44-90be-a0c4705b25cf.png)



### 死锁检测：

死锁检测，运行在一个 background 后台线程，每间隔一定时间检测当前是否出现死锁，并挑选合适的事务将其 abort 以解开死锁。

2PL不可避免的会产生死锁，所以要及时检测死锁打破依赖。这一节比较简单，bustub 会在创建 lock_manger 时，**在后台创建一个周期性的死锁检测线程**。

每次死锁检测线程唤醒后都要检测死锁，流程如下:

- 遍历所有request_queue，建立一个 wait-for graph有向边 。
- 建图完成后，用DFS算法检测环。
- 如果有环，在此lab中只需要简单的找出环中最年轻的事务，也就是 txn_id 最大的事务。

- 将事务设为abort状态。
  - 找到事务正在等待资源的request_queue。
  - notify_all 将 request_queue上所有线程唤醒。

- 如果事务唤醒后发现状态为abort (这里需要修改一下 上文的等待条件) ，在request_queue中将自己删除，抛异常，返回。

- 在 graph 中删除 所有与 txn_id 相关的边，可能会有多条边，因为会同时在等待多个事务。

- 重复直到没有环。

需要注意的是：事务在被 abort 后，lock_manger 会自动释放相关资源，所以我们不需要添加冗余的逻辑去释放已有的锁。

==========

死锁检测线程在唤醒后，可以从上图建立如下的 wait-for graph。

- 一个事务同时会等待多个事务：txn6 同时在等待 txn1/2
- 一个事务同时会被多个事务等待： txn6 同时被 txn1/4 等待
- txn5 没有事务等待不会出现在图中
- txn1/6 形成了一个环，将 txn6 abort，删除相应边。

最终lockmanger 自动释放 txn6 资源，这时 txn1 就会被唤醒授予锁。



### 并发查询执行：



根据隔离级别选择合适的表锁和行锁，并且判断是否需要手动解锁。

事务abort，commit 时会自动先解锁行锁再解锁表锁。abort 时还会自动对写集合中的操作回退。具体可以看一下 transaction_manager.cpp 文件。

事务abort时，需要回退写操作，table_heap.cpp 已经为我们维护了tuple WriteSet，我们只要在相应文件中维护一下 IndexWriteSet 。

#### 加锁tip:

在 init() 中锁表，在 next() 中锁tuple。

- *REPEATABLE_READ*, 执行器中只加锁不解锁，需要自行判断一下应该加什么锁。 

- - *REPEATABLE_READ实行* 严格的两阶段锁，最后 abort 或 commit 统一释放持有的锁，所以不会出现不可重复读，脏读。
  - 可能出现幻读。

- *READ_COMMITTED*，因为*READ_COMMITTED* 在GROWING状态解S锁不影响事务状态，需要提前解锁（锁未被升级的情况下）。

- - *READ_COMMITTED* 在GROWING状态，会解 S 锁，所以会两次读到不一样的数据。
  - 不会出现脏读，因为前一个修改数据的事务要加X锁，直到提交才会释放，对外界是完全隔离的。

- *READ_UNCOMMITTED*，读不加锁，写加锁。

- - *READ_UNCOMMITTED*对读不加锁，所以会读到未提交的数据。

另外需要注意一下执行过程中会自动发生**锁的升级**，

所以还需要额外的逻辑判断**是否已经持有锁或升级的锁**，这时候就不要再申请了。

因为**事务在申请锁时**可能会发生死锁被lock_manger abort，抛出事务异常，所以**执行器在加锁过程时要try catch**捕获事务异常，再抛出执行异常。





### Concurrent Query Execution：

修改之前实现的 `SeqScan`、`Insert` 和 `Delete` 算子，加上适当的锁以实现并发的查询。

**要理清楚该什么时候加锁/解锁、该加什么锁。**

我们仅需修改 SeqScan、Insert 和 Delete 三个算子。为什么其他的算子不需要修改？因为其他算子获取的 tuple 数据均为中间结果，并不是表中实际的数据。而这三个算子是需要与表中实际数据打交道的。其中 Insert 和 Delete 几乎完全一样，与 SeqScan 分别代表着写和读。

##### SeqScan

如果隔离级别是 READ_UNCOMMITTED 则无需加锁。加锁失败则抛出 ExecutionException 异常。

在 READ_COMMITTED 下，在 Next() 函数中，若表中已经没有数据，则提前释放之前持有的锁。在 REPEATABLE_READ 下，在 Commit/Abort 时统一释放，无需手动释放。

该加什么锁？直观上来说应该直接给表加 S 锁，但实际上会导致 MixedTest 用例失败。实际上需要给表加 IS 锁，再给行加 S 锁。另外，在 Leaderboard Test 里，我们需要实现一个 Predicate pushdown to SeqScan 的优化，即将 Filter 算子结合进 SeqScan 里，这样我们仅需给符合 predicate 的行加上 S 锁，减小加锁数量。

那么在实现了 Predicate pushdown to SeqScan 之后，有没有可以给表直接加 S 锁的情况？有，当 SeqScan 算子中不存在 Predicate 时，即需要全表扫描时，或许可以直接给表加 S 锁，避免给所有行全部加上 S 锁。

##### Insert & Delete

在 Init() 函数中，为表加上 IX 锁，再为行加 X 锁。同样，若获取失败则抛 ExecutionException 异常。另外，这里的获取失败不仅是结果返回 false，还有可能是抛出了 TransactionAbort() 异常，例如UPGRADE_CONFLICT，需要用 try catch 捕获。

锁在 Commit/Abort 时统一释放，无需手动释放。

另外，在 Notes 里提到的

 you will need to maintain the write sets in transactions

似乎 InsertTuple() 函数里已经帮我们做好了，不需要维护 write set。而 index 的 write set 需要我们自己维护。



## 知乎二========

### Lock=====================

以 table lock 为例

#### **一，检查 txn 的状态**

1、若 txn 处于 Abort/Commit 状态，抛逻辑异常，不应该有这种情况出现。

2、若 txn 处于 Shrinking 状态，则需要检查 txn 的隔离级别和当前锁请求类型：

​    若 txn 处于 Shrinking 状态：

- 在 `REPEATABLE_READ` 可重复读下，造成事务终止，并抛出 `LOCK_ON_SHRINKING` 异常。
- 在 `READ_COMMITTED` 读已提交下，若为 IS/S 锁，则正常通过，否则抛 `LOCK_ON_SHRINKING`异常。
- 在 `READ_UNCOMMITTED` 读未提交下，若为 IX/X 锁，抛 `LOCK_ON_SHRINKING`，否则抛 `LOCK_SHARED_ON_READ_UNCOMMITTED`异常。

 支持除了 `SERIALIZABLE` 外的剩下三种隔离级别。

3、若 txn 处于 Growing 状态

   若隔离级别为 `READ_UNCOMMITTED`读未提交 且锁类型为 S/IS/SIX，抛 `LOCK_SHARED_ON_READ_UNCOMMITTED`异常。其余状态正常通过。

第一步保证了锁请求、事务状态、事务隔离级别的兼容。正常通过第一步后，可以开始尝试获取锁。

#### **二，取 table 对应的 lock request queue**

从 `table_lock_map_` 中获取 table 对应的 lock request queue。注意需要对 map 加锁，并且为了提高并发性，在获取到 queue 之后立即释放 map 的锁。若 queue 不存在则创建。

#### **三，检查锁请求是否为一次锁升级**

首先，记得对 queue 加锁。

granted 和 waiting 的锁请求均放在同一个队列里，需要遍历队列查看有没有与当前事务 id相同的请求。如果存在这样的请求，则代表当前事务在此前已经得到了在此资源上的一把锁，接下来可能需要锁升级。注意，这个请求的 `granted_` 一定为 true。

因为假如事务此前的请求还没有被通过，事务会被阻塞在 LockManager 中，不可能再去尝试获取另一把锁。

现在找到了此前已经获取的锁，**开始尝试锁升级**。首先，判断此前授予锁类型是否与当前请求锁类型相同。若相同，则代表是一次重复的请求，直接返回。否则进行下一步检查。

接下来，判断当前资源上是否有另一个事务正在尝试升级（`queue->upgrading_ == INVALID_TXN_ID`）。若有，则终止当前事务，抛出 `UPGRADE_CONFLICT` 异常。因为不允许多个事务在同一资源上同时尝试锁升级。



不允许多事务同时进行锁升级目的：为了降低实现的复杂度，增加一条升级队列可以实现同步的升级。然后，判断升级锁的类型和之前锁是否兼容，不能反向升级。

> IS -> [S, X, IX, SIX] 
>
> S -> [X, SIX] 
>
> IX -> [X, SIX] 
>
> SIX -> [X]

若不兼容，抛 `INCOMPATIBLE_UPGRADE` 异常。

##### 锁升级：

1. 如果要进行锁升级，
2. 检查锁升级是否冲突，冲突，abort, throw
3. 1. 删除旧的记录，同时删除事务集合中的记录。
   2. 将一条新的记录插入在队列最前面一条未授予的记录之前
4. 锁不用升级，将新的记录添加在请求队列末尾。
5. 1. 检查兼容性（前面事务），等待在条件变量上
   2. 通过了兼容性检查，授予锁



#### **四，将锁请求加入请求队列。**

new 一个 LockRequest，加入队列尾部。采用一条队列，把 granted 和 waiting 的请求放在一起

#### **五，尝试获取锁**

条件变量是操作系统中线程同步的一种机制。先给出条件变量经典的使用形式：

```cpp
std::unique_lock<std::mutex> lock(latch);
while (!resource) {
    cv.wait(lock);
}
```

**条件变量与互斥锁**配合使用。首先需要持有锁，并查看是否能够获取资源。这个锁与资源绑定，是用来保护资源的锁。若暂时无法获取资源，则调用条件变量的 wait 函数。调用 wait 函数后，latch 将自动**释放**，并且当前线程被挂起，以节省资源。这就是阻塞的过程。此外，允许有多个线程在 wait 同一个 latch。

当其他线程的活动使得资源状态发生改变时，需要调用条件遍历的 `notify_all()` 函数。即

```cpp
// do something changing the state of resource...
cv.notify_all();
```

`notify_all()` 可以看作一次广播，会唤醒所有正在此条件变量上阻塞的线程。在线程被唤醒后，其仍处于 wait 函数中。在 wait 函数中尝试获取 latch。在成功获取 latch 后，退出 wait 函数，进入循环的判断条件，检查是否能获取资源。若仍不能获取资源，就继续进入 wait 阻塞，释放锁，挂起线程。若能获取资源，则退出循环。这样就实现了阻塞等待资源的模型。条件变量中的条件指的就是满足某个条件，在这里即能够获取资源。

理解条件变量的作用后，就可以写出如下代码：

```cpp
std::unique_lock<std::mutex> lock(queue->latch_);
while (!GrantLock(...)) {
    queue->cv_.wait(lock);
}
```

在 `GrantLock()` 中，Lock Manager 会判断是否可以满足当前锁请求。若可以满足，则返回 true，事务成功获取锁，并退出循环。若不能满足，则返回 false，事务暂时无法获取锁，**在 wait 处阻塞，等待资源状态变化时被唤醒并再次判断是否能够获取锁**。资源状态变化指的是什么？其他事务释放了锁。



##### `GrantLock()` 授予锁函数

在此函数中，我们需要判断当前锁请求是否能被满足。

1.  判断兼容性。遍历请求队列，查看当前锁请求是否与所有的已经 granted 的请求兼容。需要注意的是，在我的实现中 granted 请求不一定都在队列头部，因此需要完全遍历整条队列。锁兼容矩阵可以在 Lecture slides 中查看。若全部兼容，则通过检查。否则直接返回 false。当前请求无法被满足。
    
2.  判断优先级。锁请求会以严格的 FIFO 顺序依次满足。只有当前请求为请求队列中优先级最高的请求时，才允许授予锁。优先级可以这样判断：
    
3. 如果队列中存在锁升级请求，若锁升级请求正为当前请求，则优先级最高。否则代表其他事务正在尝试锁升级，优先级高于当前请求。
4. 若队列中不存在锁升级请求，则遍历队列。如果，当前请求是第一个 waiting 状态的请求，则代表优先级最高。如果当前请求前面还存在其他 waiting 请求，则要判断当前请求是否前面的 waiting 请求兼容。若兼容，则仍可以视为优先级最高。若存在不兼容的请求，则优先级不为最高。

所有兼容的锁请求需要一起被授予。

两项检查通过后，代表当前请求既兼容又有最高优先级，因此可以授予锁。授予锁的方式是将 `granted_` 置为 true。并返回 true。假如这是一次升级请求，则代表升级完成，还要记得将 `upgrading_` 置为 `INVALID_TXN_ID`。

另外，需要进行一些 Bookkeeping 操作。Transaction 中需要维护许多集合，分别记录了 Transaction 当前持有的各种类型的锁。方便在事务提交或终止后全部释放。



Lock 的流程大致如此，row lock 与 table lock 几乎相同，仅多了一个检查步骤。在接收到 row lock 请求后，需要检查是否持有 row 对应的 table lock。必须先持有 table lock 再持有 row lock。

### Unlock===================

 table lock 为例。在释放时需要先检查其下的所有 row lock 是否已经释放。

 table lock 和 row lock 的公共步骤：

#### **一，获取对应的 lock request queue**

#### **二，遍历请求队列，找到 unlock 对应的 granted 请求**

若不存在对应的请求，抛 `ATTEMPTED_UNLOCK_BUT_NO_LOCK_HELD` 异常。

找到对应的请求后，根据事务的隔离级别和锁类型修改其状态。

1. 当隔离级别为 `REPEATABLE_READ` 时，S/X 锁释放会使事务进入 Shrinking 状态。
2. 当为 `READ_COMMITTED` 时，只有 X 锁释放使事务进入 Shrinking 状态。
3. 当为 `READ_UNCOMMITTED` 时，X 锁释放使事务 Shrinking，S 锁不会出现。
4. 之后，在请求队列中 remove unlock 对应的请求，并将请求 delete。

同样，需要进行 Bookkeeping。

在锁成功释放后，调用 `cv_.notify_all()` 唤醒所有阻塞在此 table 上的事务，检查能够获取锁。

Task 1 核心是条件变量阻塞模型



作者：十一
链接：https://zhuanlan.zhihu.com/p/592700870
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。



### Task 2 死锁检测===========

在阻塞过程中有可能会出现多个事务的循环等待，而循环等待会造成死锁。

#### 检测是否出现死锁

采用一个 background Deadlock Detection 线程来定时**检查当前是否出现死锁**。

用 wait for 图来表示事务之间的等待关系。wait for 是一个有向图，`t1->t2` 即代表 t1 事务正在等待 t2 事务释放资源。当 wait for 图中存在环时，即代表出现死锁，需要挑选事务终止以打破死锁。

不需要时刻维护 wait for 图，而是在**死锁检测线程被唤醒**时，根据当前请求队列构建 wait for 图，再通过 wait for 图判断是否存在死锁。当判断完成后，将丢弃当前 wait for 图。**下次线程被唤醒时再重新构建。**

#### 有向图环检测算法（核心）

包括 DFS 和拓扑排序。在这里我们选用 DFS 来进行环检测。构建 wait for 图时要保证搜索的确定性。始终从 tid 较小的节点开始搜索，在选择邻居时，也要优先搜索 tid 较小的邻居。

#### 构建 wait for 图的过程（算法原理）

遍历 `table_lock_map` 和 `row_lock_map` 中所有的请求队列，对于每一个请求队列，用一个二重循环将所有满足等待关系的一对 tid 加入 wait for 图的边集。满足等待关系是指，对于两个事务 a 和 b，a 是 waiting 请求，b 是 granted 请求，则生成 `a->b` 一条边。

在成功构建 wait for 图后，对 wait for 图实施环检测算法。注意，这个环检测算法不仅需要输出是否存在环，假如存在环，还要输出环上的所有节点。因为之后我们需要在这些成环的节点里挑选合适的事务进行终止。

#### 发现环（死锁）、解决死锁

在发现环后，我们可以得到环上的所有节点。此时我们挑选 **youngest** 的事务将其终止。只用挑选 tid 最大的事务作为 youngest 事务终止即可。

挑选出 youngest 事务后，将此事务的状态设为 Aborted。并且在请求队列中移除此事务，释放其持有的锁，终止其正在阻塞的请求，并调用 `cv_.notify_all()` 通知正在阻塞的相关事务。此外，还需移除 wait for 图中与此事务有关的边。

不是不用维护 wait for 图，每次使用重新构建吗？这是因为图中可能存在多个环，不是打破一个环就可以直接返回了。需要在死锁检测线程醒来的时候打破当前存在的所有环。

之前的阻塞模型需要进行一定的修改：

```cpp
std::unique_lock<std::mutex> lock(queue->latch_);
while (!GrantLock(...)) {
    queue->cv_.wait(lock);
    //在事务被唤醒时，其可能已经被终止掉了。原因可能是死锁检测中将其终止，也可能是外部的一些原因造成终止。因此需要检测是否处于 Aborted 状态，若处于则释放所持资源并返回。
    if (txn->GetState() == Aborted) {
        // release resources
        return false;
    }
}
```









# ===================================





B+树比B树好在哪里？哪个层数更多？

B+树乐观锁怎么实现？

火山模型优缺点？

接上条，虚函数开销多大测过吗？多了几次内存访问？

规则优化如何避免 overhead? （就是优化本身比执行开销大）不会

优化器如何避免不必要的路径（裁剪）？

锁管理器怎么加锁解锁？加解锁流程。

MapReduce 相关，直接顶不住，简历已经把这个删了。





